{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "%matplotlib qt\n",
    "\n",
    "#This notebook is a testbed for importing PEAC Center USAPI\n",
    "#raifnall data using pandas and doing some quick analysis\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as cols\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap, shiftgrid\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "from datetime import date\n",
    "import datetime\n",
    "\n",
    "import calendar\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "from scipy import signal, linalg, stats\n",
    "\n",
    "from pycurrents.codas import to_day, to_date\n",
    "from pycurrents.plot.mpltools import dday_to_mpl\n",
    "\n",
    "from pycurrents.system import Bunch\n",
    "from pycurrents.num import eof\n",
    "from pycurrents.num import rangeslice\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.special import comb\n",
    "#import metpy.calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir = '/home/alejandro/Desktop/Data/'\n",
    "#datadir = '/home/peac-ra/Desktop/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NCEP UWND, VWND ahd HGT all go from Jan 1948 to present and are on a 2.5 deg grid\n",
    "# the grid goes from 0.0E to 357.5E and 90N to 90S\n",
    "uwnd_data = Dataset(datadir + 'uwnd.mon.mean.nc')\n",
    "vwnd_data = Dataset(datadir + 'vwnd.mon.mean.nc')\n",
    "hgt_data = Dataset(datadir + 'hgt.mon.mean.nc')\n",
    "\n",
    "# read lats,lons\n",
    "ncep_latitudes = uwnd_data.variables['lat'][:]\n",
    "ncep_longitudes = uwnd_data.variables['lon'][:]\n",
    "\n",
    "#read in ncep levels\n",
    "# these levels are\n",
    "#1000,925,850,700,600,500,400,300,250,200,150,100,70,50,30,20,10\n",
    "ncep_levels = uwnd_data.variables['level'][:]\n",
    "\n",
    "#Choose pressure level now, to save obn memory\n",
    "pressure_selection = ncep_levels[:] == 850\n",
    "\n",
    "\n",
    "#time in hours since 1800-01-01 00:00:0.0\n",
    "\n",
    "ncep_time = uwnd_data.variables['time'][:]\n",
    "\n",
    "# get  wind data.\n",
    "uin = np.squeeze(uwnd_data.variables['uwnd'][:, pressure_selection, :, :])\n",
    "vin = np.squeeze(vwnd_data.variables['vwnd'][:, pressure_selection, :, :])\n",
    "hgtin = np.squeeze(hgt_data.variables['hgt'][:, pressure_selection, :, :])\n",
    "\n",
    "uwnd_data.close()\n",
    "vwnd_data.close()\n",
    "hgt_data.close()\n",
    "\n",
    "# Let's just convert it to days:\n",
    "ncep_dday_1800 = ncep_time / 24\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "#GPCP data goes from Jan 1979 to present and is on a 2.5 deg grid\n",
    "#same grid as NCEP\n",
    "precip_data = Dataset(datadir + 'precip.mon.mean.nc')\n",
    "\n",
    "#time in hours since 1800-01-01 00:00:0.0\n",
    "precip_time = precip_data.variables['time'][:]\n",
    "\n",
    "# read lats,lons\n",
    "precip_latitudes = precip_data.variables['lat'][:]\n",
    "precip_longitudes = precip_data.variables['lon'][:]\n",
    "\n",
    "# get precip data.\n",
    "precip_in = precip_data.variables['precip'][:, :, :]\n",
    "\n",
    "precip_data.close()\n",
    "\n",
    "# GPCP time is in days since jan 1 1800\n",
    "precip_dday_1800 = precip_time\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "#PREC data goes from Jan 1948 to present and is on a 2.5 deg grid\n",
    "#same grid as NCEP\n",
    "PREC_precip_data = Dataset(datadir + 'PREC.precip.mon.anom.nc')\n",
    "\n",
    "#time in hours since 1800-01-01 00:00:0.0\n",
    "PREC_precip_time = PREC_precip_data.variables['time'][:]\n",
    "\n",
    "# read lats,lons\n",
    "PREC_precip_latitudes = PREC_precip_data.variables['lat'][:]\n",
    "PREC_precip_longitudes = PREC_precip_data.variables['lon'][:]\n",
    "\n",
    "# get precip data.\n",
    "PREC_precip_in = PREC_precip_data.variables['precip'][:, :, :]\n",
    "\n",
    "PREC_precip_data.close()\n",
    "\n",
    "# PREC time is in hours since jan 1 1800\n",
    "PREC_precip_dday_1800 = PREC_precip_time /24\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "#ERSST v4 data goes from Jan 1854 to present and is on a 2 deg grid\n",
    "#88.0N - 88.0S, 0.0E - 358.0E.\n",
    "ersst_data = Dataset(datadir + 'sst.mnmean.v4.nc')\n",
    "\n",
    "# read lats,lons\n",
    "ersst_latitudes = ersst_data.variables['lat'][:]\n",
    "ersst_longitudes = ersst_data.variables['lon'][:]\n",
    "\n",
    "#time in hours since 1800-01-01 00:00:0.0\n",
    "ersst_time = ersst_data.variables['time'][:]\n",
    "\n",
    "# get  wind data.\n",
    "ersst_in = ersst_data.variables['sst'][:, :, :]\n",
    "\n",
    "ersst_data.close()\n",
    "\n",
    "# ERSST time is in days since jan 1 1800\n",
    "ersst_dday_1800 = ersst_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extend(dday_1800, dat):\n",
    "    \"\"\"\n",
    "    Return dday, dat extended to fill out the last year.\n",
    "    \n",
    "    This function was modifyed to take in only one data variable.\n",
    "    \"\"\"\n",
    "    n_orig = len(dday_1800)\n",
    "    ymdhms = to_date(1800, dday_1800)\n",
    "    nmissing = 12 - ymdhms[-1, 1]\n",
    "    nmonths = n_orig + nmissing\n",
    "    ymdhms_new = np.zeros((nmonths, 6), dtype=np.uint16)\n",
    "    ymdhms_new[:n_orig, :] = ymdhms\n",
    "    ymdhms_new[n_orig:, 0] = ymdhms[-1, 0]\n",
    "    # Fill in the remaining months:\n",
    "    ymdhms_new[n_orig:, 1] = np.arange(ymdhms[-1, 1] + 1, 13)\n",
    "    ymdhms_new[n_orig:, 2] = 1\n",
    "    dday_new = to_day(1800, ymdhms_new)\n",
    "    \n",
    "    shape_new = (nmonths, dat.shape[1], dat.shape[2])\n",
    "    ## EF: modify to work with masked array or ndarray\n",
    "    if np.ma.isMA(dat):\n",
    "        dat_new = np.ma.masked_all(shape_new, float)\n",
    "    else:    \n",
    "        dat_new = np.nan + np.zeros(shape_new, float)\n",
    "    \n",
    "    dat_new[:n_orig, :, :] = dat\n",
    "    \n",
    "    #This Function has been modifyed to also return the date of the last valid data and its ymdhms index\n",
    "    \n",
    "    last_valid_date = to_date(1800,dday_1800[-1])\n",
    "    last_valid_index = n_orig -1\n",
    "   \n",
    "    return Bunch(dday_1800=dday_new,\n",
    "                 ymdhms=ymdhms_new,\n",
    "                 dat=dat_new,\n",
    "                 last_valid_date=last_valid_date,\n",
    "                 last_valid_index=last_valid_index)\n",
    "\n",
    "def cal_climatology_and_anomaly(data,ymdhms,last_valid_date,last_valid_index,latitudes,longitudes,start_year, end_year):\n",
    "                           \n",
    "    #Select the years for climatology from the new ymdhms_padded\n",
    "    # All the monthly data for the years you want to calculate the climatology\n",
    "    \n",
    "    clim_selection = ((ymdhms[:, 0] >= start_year) & (ymdhms[:, 0] <= end_year))\n",
    "    \n",
    "    # EF: Trim everything right at the start.\n",
    "    data = data[clim_selection]\n",
    "    ymdhms = ymdhms[clim_selection]\n",
    "    \n",
    "    #Calculate the total numbers of years in the climatology\n",
    "    length_in_years = ymdhms[-1, 0] - ymdhms[0, 0] + 1\n",
    "    \n",
    "    # EF: ensure we have masked arrays and no nans\n",
    "    data = np.ma.masked_invalid(data)\n",
    "    \n",
    "    #reshape the matrix so that is has dimensions of [years, months, lat, lon]\n",
    "    reshaped_data = np.reshape(data, (length_in_years, 12, \n",
    "                                      len(latitudes), \n",
    "                                      len(longitudes)))\n",
    "    #Calculate the climatology by using nanmeans, along the yeas axis\n",
    "    #climatology = np.nanmean(reshaped_data, axis=0)\n",
    "    \n",
    "    climatology = reshaped_data.mean(axis=0)\n",
    "    anomaly = reshaped_data - climatology\n",
    "    anomaly = anomaly.reshape(data.shape)\n",
    "    \n",
    "    if end_year < last_valid_date[0]:\n",
    "        last_valid_date_new = ymdhms[-1,:]\n",
    "        last_valid_index_new = -1\n",
    "    else:\n",
    "        last_valid_date_new = last_valid_date\n",
    "        last_valid_index_new = np.where((ymdhms[:,0]== last_valid_date[0]) &\n",
    "                                        (ymdhms[:,1]== last_valid_date[1]) &\n",
    "                                        (ymdhms[:,2]== last_valid_date[2]))[0][0]\n",
    "    \n",
    "    return Bunch(climatology=climatology,\n",
    "                 anomaly=anomaly, \n",
    "                 ymdhms=ymdhms,\n",
    "                 last_valid_date = last_valid_date_new,\n",
    "                 last_valid_index = last_valid_index_new)\n",
    "\n",
    "def seasonal_anomaly(m_anom,last_valid_index):\n",
    "    s_anom = np.ma.zeros(m_anom.shape, float)\n",
    "    orig_mask = np.ma.getmaskarray(m_anom)\n",
    "    \n",
    "    if last_valid_index == -1:\n",
    "        s_anom[1:-1] = (m_anom[:-2] + m_anom[1:-1] + m_anom[2:]) / 3\n",
    "    \n",
    "        s_anom[0] = (m_anom[0] + m_anom[1]) / 2\n",
    "        s_anom[-1] = (m_anom[-2] + m_anom[-1]) / 2\n",
    "    \n",
    "    else:\n",
    "        #s_anom[1:last_valid_index-1,:,:] = (m_anom[:last_valid_index-2,:,:] + \n",
    "        #                                    m_anom[1:last_valid_index-1,:,:] + \n",
    "        #                                    m_anom[2:last_valid_index,:,:]) / 3\n",
    "        s_anom[1:last_valid_index,:,:] = (m_anom[:last_valid_index-1,:,:] + \n",
    "                                          m_anom[1:last_valid_index,:,:] + \n",
    "                                          m_anom[2:last_valid_index+1,:,:]) / 3\n",
    "    \n",
    "        s_anom[0,:,:] = (m_anom[0,:,:] + m_anom[1,:,:]) / 2\n",
    "        s_anom[last_valid_index,:,:] = (m_anom[last_valid_index-1,:,:] + \n",
    "                                        m_anom[last_valid_index,:,:]) / 2\n",
    "        \n",
    "    s_anom_out = np.ma.array(s_anom, mask=orig_mask)\n",
    "    \n",
    "    return Bunch(s_anom = s_anom_out, \n",
    "                 last_valid_index = last_valid_index)\n",
    "\n",
    "def seasonal_anomaly2(m_anom, nmin=2):\n",
    "    newshape = [3] + list(m_anom.shape)\n",
    "    newshape[1] += 2\n",
    "    accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "    accum[:] = np.ma.masked\n",
    "    accum[0, :-2] = m_anom[:]\n",
    "    accum[1, 1:-1] = m_anom[:]\n",
    "    accum[2, 2:] = m_anom[:]\n",
    "    out = accum.mean(axis=0)[1:-1]\n",
    "    out = np.ma.masked_where(accum[:, 1:-1].count(axis=0) < nmin, out, copy=False)\n",
    "    return Bunch(s_anom = out)\n",
    "\n",
    "def running_sum(m_anom, window, nmin = 2):\n",
    "    nmin = (window+1)/2\n",
    "    chop = (window -1)/2\n",
    "\n",
    "    newshape = [window] + list(m_anom.shape)\n",
    "    newshape[1] += window -1\n",
    "    accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "    accum[:] = np.ma.masked\n",
    "    for i in range(window):\n",
    "        end = -window+i+1\n",
    "        if end == 0:\n",
    "            accum[i, i:] = m_anom[:]\n",
    "        else:\n",
    "            accum[i, i:end] = m_anom[:]\n",
    "    out = accum.mean(axis=0)[chop:-chop]\n",
    "    out = np.ma.masked_where(accum[:, chop:-chop].count(axis=0) < nmin, out, copy=False)\n",
    "    return Bunch(s_anom = out)\n",
    "\n",
    "def running_sum_2(m_anom, window, nmin = 2):\n",
    "    \n",
    "    if window % 2 == 0:\n",
    "        nmin = (window)/2\n",
    "        #nmin = 1\n",
    "        chop = (window-1)\n",
    "\n",
    "        newshape = [window] + list(m_anom.shape)\n",
    "        newshape[1] += window -1\n",
    "        accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "        accum[:] = np.ma.masked\n",
    "        for i in range(window):\n",
    "            end = -window+i+1\n",
    "            if end == 0:\n",
    "                accum[i, i:] = m_anom[:]\n",
    "            else:\n",
    "                accum[i, i:end] = m_anom[:]\n",
    "        if window != 2:\n",
    "            start=window/2\n",
    "            stop = (window/2) -1\n",
    "            out = accum.mean(axis=0)[start:-stop]\n",
    "            out = np.ma.masked_where(accum[:,start:-stop].count(axis=0) < nmin, out, copy=False)\n",
    "        else:\n",
    "            out = accum.mean(axis=0)[1:]\n",
    "            out = np.ma.masked_where(accum[:,1:].count(axis=0) < nmin, out, copy=False)\n",
    "        \n",
    "    else:\n",
    "        nmin = (window+1)/2\n",
    "        chop = (window -1)/2\n",
    "\n",
    "        newshape = [window] + list(m_anom.shape)\n",
    "        newshape[1] += window -1\n",
    "        accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "        accum[:] = np.ma.masked\n",
    "        for i in range(window):\n",
    "            end = -window+i+1\n",
    "            if end == 0:\n",
    "                accum[i, i:] = m_anom[:]\n",
    "            else:\n",
    "                accum[i, i:end] = m_anom[:]\n",
    "        out = accum.mean(axis=0)[chop:-chop]\n",
    "        out = np.ma.masked_where(accum[:, chop:-chop].count(axis=0) < nmin, out, copy=False)\n",
    "        \n",
    "    return Bunch(s_anom = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fill out the data for the missing months at the end of the last year of the dataset\n",
    "precip = extend(precip_dday_1800,precip_in)\n",
    "PREC_precip = extend(PREC_precip_dday_1800,PREC_precip_in)\n",
    "\n",
    "uwnd = extend(ncep_dday_1800,uin)\n",
    "vwnd = extend(ncep_dday_1800,vin)\n",
    "hgt = extend(ncep_dday_1800,hgtin)\n",
    "\n",
    "sst = extend(ersst_dday_1800,ersst_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpldays_precip = dday_to_mpl(1800, precip_dday_1800)\n",
    "mpldays_ncep = dday_to_mpl(1800, ncep_dday_1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.5/site-packages/numpy/ma/core.py:3158: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  dout = self.data[indx]\n",
      "/home/alejandro/anaconda3/lib/python3.5/site-packages/numpy/ma/core.py:3213: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  dout._mask = _mask[indx]\n"
     ]
    }
   ],
   "source": [
    "#Here we calculate the anomaly\n",
    "precip_ca = cal_climatology_and_anomaly(precip.dat,precip.ymdhms,\n",
    "                                        precip.last_valid_date, precip.last_valid_index,\n",
    "                                        precip_latitudes,\n",
    "                                        precip_longitudes,1979,2016)\n",
    "\n",
    "PREC_precip_ca = cal_climatology_and_anomaly(PREC_precip.dat,PREC_precip.ymdhms,\n",
    "                                             PREC_precip.last_valid_date, PREC_precip.last_valid_index,\n",
    "                                             PREC_precip_latitudes,\n",
    "                                             PREC_precip_longitudes,1965,2016)\n",
    "\n",
    "uwnd_ca = cal_climatology_and_anomaly(uwnd.dat,uwnd.ymdhms,\n",
    "                                      uwnd.last_valid_date,uwnd.last_valid_index,\n",
    "                                      ncep_latitudes,\n",
    "                                      ncep_longitudes,1965,2016)\n",
    "\n",
    "vwnd_ca = cal_climatology_and_anomaly(vwnd.dat,vwnd.ymdhms,\n",
    "                                      vwnd.last_valid_date,vwnd.last_valid_index,\n",
    "                                      ncep_latitudes,\n",
    "                                      ncep_longitudes,1965,2016)\n",
    "\n",
    "hgt_ca = cal_climatology_and_anomaly(hgt.dat,hgt.ymdhms,\n",
    "                                     hgt.last_valid_date,hgt.last_valid_index,\n",
    "                                     ncep_latitudes,\n",
    "                                     ncep_longitudes,1965,2016)\n",
    "\n",
    "sst_ca = cal_climatology_and_anomaly(sst.dat,sst.ymdhms,\n",
    "                                     sst.last_valid_date,sst.last_valid_index,\n",
    "                                     ersst_latitudes,\n",
    "                                     ersst_longitudes,1965,2016)\n",
    "\n",
    "precip_seasonal_anom = running_sum_2(precip_ca.anomaly,6, nmin=4)\n",
    "PREC_precip_seasonal_anom = running_sum_2(PREC_precip_ca.anomaly,6, nmin=4)\n",
    "\n",
    "uwnd_seasonal_anom = running_sum_2(uwnd_ca.anomaly,3, nmin=2)\n",
    "vwnd_seasonal_anom = running_sum_2(vwnd_ca.anomaly,3, nmin=2)\n",
    "hgt_seasonal_anom = running_sum_2(hgt_ca.anomaly,3, nmin=2)\n",
    "\n",
    "sst_seasonal_anom = running_sum_2(sst_ca.anomaly,3, nmin=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we read and process the time series data from the excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Guam', 'Saipan', 'Koror', 'Yap', 'Chuuk', 'Pohnpei', 'Majuro', 'Kwajalein', 'PagoPago', 'Saipan_old', 'JFM', 'FMA', 'MAM', 'AMJ', 'MJJ', 'JJA', 'JAS', 'ASO', 'SON', 'OND', 'NDJ', 'DJF']\n",
      "['ONI']\n"
     ]
    }
   ],
   "source": [
    "peac_station_rain = pd.ExcelFile(datadir +'PEAC_station_rainfall_database.xlsx')\n",
    "print(peac_station_rain.sheet_names)\n",
    "\n",
    "ONI_file = pd.ExcelFile(datadir +'CPC_ONI.xlsx')\n",
    "print(ONI_file.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_USAPI_data(file_name, island_name):\n",
    "    \n",
    "    #Here we read the data into python from the excel file\n",
    "    raw_data= pd.read_excel(file_name, sheetname = island_name, \n",
    "                            skiprows = 1, parse_cols = \"A:O\")\n",
    "    \n",
    "    #\n",
    "    raw_matrix = raw_data.as_matrix(columns=None)\n",
    "    print(type(raw_matrix), raw_matrix.dtype)\n",
    "    \n",
    "    years =  raw_matrix[:51,1]\n",
    "    station_id = raw_matrix[0,0]\n",
    "\n",
    "    rainfall = raw_matrix[:51,3:]\n",
    "\n",
    "    rainfall[rainfall == '9999'] = np.nan\n",
    "    rainfall[rainfall == \"nan\"] = np.nan\n",
    "\n",
    "    rainfall = rainfall * 0.1\n",
    "    \n",
    "    # We have to convert from an object array to floating point.\n",
    "    rainfall = np.ma.masked_invalid(rainfall.astype(float))\n",
    "    print('rainfall: ', rainfall.dtype, rainfall[5, 5])\n",
    " \n",
    "    #Initiate the ymdhms array as an array of int values filler with zeros\n",
    "    #that has the length og the total number of time steps years*months\n",
    "    ymdhms = np.zeros([51*12,6],dtype = np.int)\n",
    "\n",
    "    #here we will the first column with the year values from the excel sheet. \n",
    "    #repeated each one 12 consecutive times\n",
    "    ymdhms[:,0] = np.repeat(years,12)\n",
    "\n",
    "    #Here we create a list of the month indices\n",
    "    m = np.arange(1,13)\n",
    "    #we tile the list so that the entire list 1..12 repeats as many times as the number of years\n",
    "    mm = np.tile(m,51)\n",
    "\n",
    "    ymdhms[:,1] = mm\n",
    "\n",
    "    #the day column is filled with 15\n",
    "    ymdhms[:,2] = 15\n",
    "    \n",
    "    dday = to_day(1800, ymdhms)\n",
    "    mpldays = dday_to_mpl(1800, dday)\n",
    "    mpldaysformated = mpl.dates.num2date(mpldays)\n",
    "\n",
    "    out = Bunch(island_name = island_name, station_id = station_id, rainfall=rainfall,\n",
    "               ymdhms = ymdhms, mpldaysformated = mpldaysformated)\n",
    "    \n",
    "    return out\n",
    "    #return station_id, rainfall, ymdhms, mpldaysformated\n",
    "    \n",
    "def read_index_data(file_name, index_name):\n",
    "    \n",
    "    #Here we read the data into python from the excel file\n",
    "    raw_data= pd.read_excel(file_name, sheetname = index_name, skiprows = 1, parse_cols = \"A:M\")\n",
    "    \n",
    "    #\n",
    "    raw_matrix = raw_data.as_matrix(columns=None)\n",
    "    print(type(raw_matrix), raw_matrix.dtype)\n",
    "    \n",
    "    years =  raw_matrix[:,0]\n",
    "    index = raw_matrix[:,1:]\n",
    "    \n",
    "\n",
    "    # We have to convert from an object array to floating point.\n",
    "    index = np.ma.masked_invalid(index.astype(float))\n",
    "    print('index: ', index.dtype, index[5, 5])\n",
    " \n",
    "    #Initiate the ymdhms array as an array of int values filler with zeros\n",
    "    #that has the length og the total number of time steps years*months\n",
    "    ymdhms = np.zeros([67*12,6],dtype = np.int)\n",
    "\n",
    "    #here we will the first column with the year values from the excel sheet. \n",
    "    #repeated each one 12 consecutive times\n",
    "    ymdhms[:,0] = np.repeat(years,12)\n",
    "\n",
    "    #Here we create a list of the month indices\n",
    "    m = np.arange(1,13)\n",
    "    #we tile the list so that the entire list 1..12 repeats as many times as the number of years\n",
    "    mm = np.tile(m,67)\n",
    "\n",
    "    ymdhms[:,1] = mm\n",
    "\n",
    "    #the day column is filled with 15\n",
    "    ymdhms[:,2] = 15\n",
    "    \n",
    "    dday = to_day(1800, ymdhms)\n",
    "    mpldays = dday_to_mpl(1800, dday)\n",
    "    mpldaysformated = mpl.dates.num2date(mpldays)\n",
    "\n",
    "    out = Bunch(index_name = index_name, index = index,\n",
    "               ymdhms = ymdhms, mpldaysformated = mpldaysformated)\n",
    "    \n",
    "    return out\n",
    "    #return station_id, rainfall, ymdhms, mpldaysformated\n",
    "\n",
    "def seasonal_anomaly_old(m_anom):\n",
    "    s_anom = np.ma.zeros(m_anom.shape, float)\n",
    "    s_anom[1:-1] = (m_anom[:-2] + m_anom[1:-1] + m_anom[2:]) / 3\n",
    "    s_anom[0] = (m_anom[0] + m_anom[1]) / 2\n",
    "    s_anom[-1] = (m_anom[-2] + m_anom[-1]) / 2\n",
    "    return s_anom\n",
    "\n",
    "def seasonal_sum(m_anom):\n",
    "    s_anom = np.ma.zeros(m_anom.shape, float)\n",
    "    s_anom[1:-1] = (m_anom[:-2] + m_anom[1:-1] + m_anom[2:])\n",
    "    s_anom[0] = (m_anom[0] + m_anom[1])\n",
    "    s_anom[-1] = (m_anom[-2] + m_anom[-1])\n",
    "    return s_anom\n",
    "\n",
    "def running_sum(m_anom, window, nmin = 2):\n",
    "    \n",
    "    if window % 2 == 0:\n",
    "        nmin = (window)/2\n",
    "        #nmin = 1\n",
    "        chop = (window-1)\n",
    "\n",
    "        newshape = [window] + list(m_anom.shape)\n",
    "        newshape[1] += window -1\n",
    "        accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "        accum[:] = np.ma.masked\n",
    "        for i in range(window):\n",
    "            end = -window+i+1\n",
    "            if end == 0:\n",
    "                accum[i, i:] = m_anom[:]\n",
    "            else:\n",
    "                accum[i, i:end] = m_anom[:]\n",
    "        if window != 2:\n",
    "            start=window/2\n",
    "            stop = (window/2) -1\n",
    "            out = accum.mean(axis=0)[start:-stop]\n",
    "            out = np.ma.masked_where(accum[:,start:-stop].count(axis=0) < nmin, out, copy=False)\n",
    "        else:\n",
    "            out = accum.mean(axis=0)[1:]\n",
    "            out = np.ma.masked_where(accum[:,1:].count(axis=0) < nmin, out, copy=False)\n",
    "        \n",
    "    else:\n",
    "        nmin = (window+1)/2\n",
    "        chop = (window -1)/2\n",
    "\n",
    "        newshape = [window] + list(m_anom.shape)\n",
    "        newshape[1] += window -1\n",
    "        accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "        accum[:] = np.ma.masked\n",
    "        for i in range(window):\n",
    "            end = -window+i+1\n",
    "            if end == 0:\n",
    "                accum[i, i:] = m_anom[:]\n",
    "            else:\n",
    "                accum[i, i:end] = m_anom[:]\n",
    "        out = accum.mean(axis=0)[chop:-chop]\n",
    "        out = np.ma.masked_where(accum[:, chop:-chop].count(axis=0) < nmin, out, copy=False)\n",
    "        \n",
    "    return Bunch(s_anom = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 498.1\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 354.3\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 128.6\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 353.9\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 599.9\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 292.9\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 340.5\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 --\n"
     ]
    }
   ],
   "source": [
    "station_name_list = [\"Koror\", \"Yap\", \"Guam\", \"Chuuk\", \"Pohnpei\", \"Kwajalein\", \"Majuro\", \"Saipan\"]\n",
    "#variable_name_list = [koror, yap, guam, chuuk, phonpei, kwajalein, majuro]\n",
    "\n",
    "stations = Bunch()\n",
    "for name in station_name_list:\n",
    "    stations[name] = read_USAPI_data(peac_station_rain, name)\n",
    "    \n",
    "for raindata in stations.values():\n",
    "    #print(raindata.island_name)\n",
    "    #print(np.shape(raindata.rainfall))\n",
    "    raindata.monmean = raindata.rainfall.mean(axis=0)\n",
    "    raindata.monstd = raindata.rainfall.std(axis=0)\n",
    "    raindata.monanom = raindata.rainfall - raindata.monmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.5/site-packages/numpy/ma/core.py:3158: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  dout = self.data[indx]\n",
      "/home/alejandro/anaconda3/lib/python3.5/site-packages/numpy/ma/core.py:3213: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  dout._mask = _mask[indx]\n"
     ]
    }
   ],
   "source": [
    "#print(np.shape(stations.Kwajalein.monanom))\n",
    "kwajalein_seasonal_anom = seasonal_anomaly_old(stations.Kwajalein.monanom.ravel())\n",
    "kwajalein_seasonal_total = seasonal_sum(stations.Kwajalein.rainfall.ravel())\n",
    "kwajalein_dry_season_total = running_sum(stations.Kwajalein.rainfall.ravel(),6)\n",
    "kwajalein_dry_season_anom = running_sum(stations.Kwajalein.monanom.ravel(),6)\n",
    "\n",
    "guam_seasonal_anom = seasonal_anomaly_old(stations.Guam.monanom.ravel())\n",
    "guam_seasonal_total = seasonal_sum(stations.Guam.rainfall.ravel())\n",
    "guam_dry_season_total = running_sum(stations.Guam.rainfall.ravel(),6)\n",
    "guam_dry_season_anom = running_sum(stations.Guam.monanom.ravel(),6)\n",
    "\n",
    "\n",
    "yap_seasonal_anom = seasonal_anomaly_old(stations.Yap.monanom.ravel())\n",
    "yap_seasonal_total = seasonal_sum(stations.Yap.rainfall.ravel())\n",
    "yap_dry_season_total = running_sum(stations.Yap.rainfall.ravel(),6)\n",
    "yap_dry_season_anom = running_sum(stations.Yap.monanom.ravel(),6)\n",
    "\n",
    "majuro_seasonal_anom = seasonal_anomaly_old(stations.Majuro.monanom.ravel())\n",
    "majuro_seasonal_total = seasonal_sum(stations.Majuro.rainfall.ravel())\n",
    "majuro_dry_season_total = running_sum(stations.Majuro.rainfall.ravel(),6)\n",
    "majuro_dry_season_anom = running_sum(stations.Majuro.monanom.ravel(),6)\n",
    "\n",
    "chuuk_seasonal_anom = seasonal_anomaly_old(stations.Chuuk.monanom.ravel())\n",
    "chuuk_seasonal_total = seasonal_sum(stations.Chuuk.rainfall.ravel())\n",
    "chuuk_dry_season_total = running_sum(stations.Chuuk.rainfall.ravel(),6)\n",
    "chuuk_dry_season_anom = running_sum(stations.Chuuk.monanom.ravel(),6)\n",
    "\n",
    "koror_seasonal_anom = seasonal_anomaly_old(stations.Koror.monanom.ravel())\n",
    "koror_seasonal_total = seasonal_sum(stations.Koror.rainfall.ravel())\n",
    "koror_dry_season_total = running_sum(stations.Koror.rainfall.ravel(),6)\n",
    "koror_dry_season_anom = running_sum(stations.Koror.monanom.ravel(),6)\n",
    "\n",
    "pohnpei_seasonal_anom = seasonal_anomaly_old(stations.Pohnpei.monanom.ravel())\n",
    "pohnpei_seasonal_total = seasonal_sum(stations.Pohnpei.rainfall.ravel())\n",
    "pohnpei_dry_season_total = running_sum(stations.Pohnpei.rainfall.ravel(),6)\n",
    "pohnpei_dry_season_anom = running_sum(stations.Pohnpei.monanom.ravel(),6)\n",
    "\n",
    "saipan_seasonal_anom = seasonal_anomaly_old(stations.Saipan.monanom.ravel())\n",
    "saipan_seasonal_total = seasonal_sum(stations.Saipan.rainfall.ravel())\n",
    "saipan_dry_season_total = running_sum(stations.Saipan.rainfall.ravel(),6)\n",
    "saipan_dry_season_anom = running_sum(stations.Saipan.monanom.ravel(),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spi_calculation(data, climatology_series):\n",
    "    \n",
    "    good_data = data[~data.mask]\n",
    "    good_climatology = climatology_series[~climatology_series.mask]\n",
    "    \n",
    "    spi = np.empty_like(good_data)\n",
    "    spi[:] = np.NAN\n",
    "\n",
    "    #fit gamma distribution to climatology series\n",
    "    fit_alpha, fit_loc, fit_beta = stats.gamma.fit(good_climatology)\n",
    "\n",
    "    #find cumulative probabilities of data from fitted distribution\n",
    "    data_cdf = stats.gamma.cdf(good_data, fit_alpha, loc=fit_loc, scale=fit_beta)\n",
    "\n",
    "    # find the percent points from the random normal dist\n",
    "\n",
    "    spi[:] = stats.norm.ppf(data_cdf, loc=0, scale=1)\n",
    "    \n",
    "    return spi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kwajalein_dry_season_total_matrix = kwajalein_dry_season_total.s_anom.reshape(51, 12)\n",
    "kwajalein_dry_season_anom_matrix = kwajalein_dry_season_anom.s_anom.reshape(51, 12)\n",
    "majuro_dry_season_anom_matrix = majuro_dry_season_anom.s_anom.reshape(51, 12)\n",
    "guam_dry_season_anom_matrix = guam_dry_season_anom.s_anom.reshape(51, 12)\n",
    "yap_dry_season_anom_matrix = yap_dry_season_anom.s_anom.reshape(51, 12)\n",
    "koror_dry_season_anom_matrix = koror_dry_season_anom.s_anom.reshape(51, 12)\n",
    "chuuk_dry_season_anom_matrix = chuuk_dry_season_anom.s_anom.reshape(51, 12)\n",
    "pohnpei_dry_season_anom_matrix = pohnpei_dry_season_anom.s_anom.reshape(51, 12)\n",
    "saipan_dry_season_anom_matrix = saipan_dry_season_anom.s_anom.reshape(51, 12)\n",
    "\n",
    "#Put ONI data into a pandas table\n",
    "season_list = ['DJF' , 'JFM' , 'FMA',\n",
    "               'MAM' , 'AMJ' , 'MJJ',\n",
    "               'JJA' , 'JAS' , 'ASO',\n",
    "               'SON' , 'OND' , 'NDJ']\n",
    "\n",
    "seven_month_list = ['Oct-Jan-Apr' , 'Nov-Feb-May' , 'Dec-Mar-Jun',\n",
    "                    'Jan-Apr-Jul' , 'Feb-May-Aug' , 'Mar-Jun-Sep',\n",
    "                    'Apr-Jul-Oct' , 'May-Aug-Nov' , 'Jun-Sep-Dec',\n",
    "                    'Jul-Oct-Jan' , 'Aug-Nov-Feb' , 'Sep-Dec-Mar']   \n",
    "\n",
    "six_month_list = ['Nov-Apr' , 'Dec-May' , 'Jan-Jun',\n",
    "                  'Feb-Jul' , 'Mar-Aug' , 'Apr-Sep',\n",
    "                  'May-Oct' , 'Jun-Nov' , 'Jul-Dec',\n",
    "                  'Aug-Jan' , 'Sep-Feb' , 'Oct-Mar']    \n",
    "\n",
    "kawj_dry_season_total_df = pd.DataFrame(kwajalein_dry_season_total_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "kawj_dry_season_anom_df = pd.DataFrame(kwajalein_dry_season_anom_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "#kawj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kwajalein_dry_season_total_matrix = kwajalein_dry_season_total.s_anom.reshape(51, 12)\n",
    "\n",
    "kwajalein_spi_matrix = np.empty_like(kwajalein_dry_season_total_matrix)\n",
    "kwajalein_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(kwajalein_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(kwajalein_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(kwajalein_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    kwajalein_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "\n",
    "    \n",
    "guam_dry_season_total_matrix = guam_dry_season_total.s_anom.reshape(51, 12)\n",
    "guam_spi_matrix = np.empty_like(guam_dry_season_total_matrix)\n",
    "guam_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(guam_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(guam_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(guam_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    guam_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "    \n",
    "yap_dry_season_total_matrix = yap_dry_season_total.s_anom.reshape(51, 12)\n",
    "yap_spi_matrix = np.empty_like(yap_dry_season_total_matrix)\n",
    "yap_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(yap_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(yap_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(yap_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    yap_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "majuro_dry_season_total_matrix = majuro_dry_season_total.s_anom.reshape(51, 12)\n",
    "majuro_spi_matrix = np.empty_like(majuro_dry_season_total_matrix)\n",
    "majuro_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(majuro_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(majuro_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(majuro_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    majuro_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "    \n",
    "koror_dry_season_total_matrix = koror_dry_season_total.s_anom.reshape(51, 12)\n",
    "koror_spi_matrix = np.empty_like(koror_dry_season_total_matrix)\n",
    "koror_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(koror_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(koror_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(koror_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    koror_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "    \n",
    "chuuk_dry_season_total_matrix = chuuk_dry_season_total.s_anom.reshape(51, 12)\n",
    "chuuk_spi_matrix = np.empty_like(chuuk_dry_season_total_matrix)\n",
    "chuuk_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(chuuk_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(chuuk_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(chuuk_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    chuuk_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "    \n",
    "pohnpei_dry_season_total_matrix = pohnpei_dry_season_total.s_anom.reshape(51, 12)\n",
    "pohnpei_spi_matrix = np.empty_like(pohnpei_dry_season_total_matrix)\n",
    "pohnpei_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(pohnpei_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(pohnpei_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(pohnpei_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    pohnpei_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "saipan_dry_season_total_matrix = saipan_dry_season_total.s_anom.reshape(51, 12)\n",
    "saipan_spi_matrix = np.empty_like(saipan_dry_season_total_matrix)\n",
    "saipan_spi_matrix[:] = np.NAN\n",
    "saipan_spi_matrix_temp = np.empty_like(saipan_dry_season_total_matrix)\n",
    "saipan_spi_matrix_temp[:] = np.NAN\n",
    "\n",
    "for col in range(saipan_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(saipan_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(saipan_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    saipan_spi_matrix_temp[-len(spi):,col] = spi\n",
    "\n",
    "saipan_spi_matrix_temp[saipan_spi_matrix_temp == np.inf] = 7\n",
    "saipan_mask = np.ma.getmask(saipan_dry_season_total_matrix)\n",
    "saipan_spi_matrix = np.ma.masked_invalid(np.ma.array(saipan_spi_matrix_temp,mask = saipan_mask))\n",
    "    \n",
    "#Put ONI data into a pandas table\n",
    "seven_month_list = ['Oct-Jan-Apr' , 'Nov-Feb-May' , 'Dec-Mar-Jun',\n",
    "                    'Jan-Apr-Jul' , 'Feb-May-Aug' , 'Mar-Jun-Sep',\n",
    "                    'Apr-Jul-Oct' , 'May-Aug-Nov' , 'Jun-Sep-Dec',\n",
    "                    'Jul-Oct-Jan' , 'Aug-Nov-Feb' , 'Sep-Dec-Mar']    \n",
    "    \n",
    "six_month_list = ['Nov-Apr' , 'Dec-May' , 'Jan-Jun',\n",
    "                  'Feb-Jul' , 'Mar-Aug' , 'Apr-Sep',\n",
    "                  'May-Oct' , 'Jun-Nov' , 'Jul-Dec',\n",
    "                  'Aug-Jan' , 'Sep-Feb' , 'Oct-Mar']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> float64\n",
      "index:  float64 -0.6\n"
     ]
    }
   ],
   "source": [
    "average_all_station_spi_matrix = np.mean(np.array([kwajalein_spi_matrix, \n",
    "                                                   majuro_spi_matrix, \n",
    "                                                   guam_spi_matrix, \n",
    "                                                   yap_spi_matrix,\n",
    "                                                   chuuk_spi_matrix,\n",
    "                                                   koror_spi_matrix,\n",
    "                                                   pohnpei_spi_matrix]), axis=0)\n",
    "\n",
    "\n",
    "average_all_station_dry_season_anom_matrix = np.mean(np.array([kwajalein_dry_season_anom_matrix, \n",
    "                                                               majuro_dry_season_anom_matrix, \n",
    "                                                               guam_dry_season_anom_matrix, \n",
    "                                                               yap_dry_season_anom_matrix,\n",
    "                                                               chuuk_dry_season_anom_matrix, \n",
    "                                                               koror_dry_season_anom_matrix, \n",
    "                                                               pohnpei_dry_season_anom_matrix,]), axis=0)\n",
    "\n",
    "southern_station_spi_matrix = np.mean(np.array([majuro_spi_matrix, \n",
    "                                                   chuuk_spi_matrix,\n",
    "                                                   koror_spi_matrix,\n",
    "                                                   pohnpei_spi_matrix]), axis=0)\n",
    "\n",
    "\n",
    "southern_station_dry_season_anom_matrix = np.mean(np.array([majuro_dry_season_anom_matrix, \n",
    "                                                               chuuk_dry_season_anom_matrix, \n",
    "                                                               koror_dry_season_anom_matrix, \n",
    "                                                               pohnpei_dry_season_anom_matrix,]), axis=0)\n",
    "\n",
    "average_2_station_spi_matrix = np.mean(np.array([kwajalein_spi_matrix, \n",
    "                                                   guam_spi_matrix]), axis=0)\n",
    "\n",
    "\n",
    "average_2_station_dry_season_anom_matrix = np.mean(np.array([kwajalein_dry_season_anom_matrix, \n",
    "                                                               guam_dry_season_anom_matrix]), axis=0)\n",
    "\n",
    "average_all_station_SPI_df = pd.DataFrame(average_all_station_spi_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "average_all_station_dry_season_anom_df = pd.DataFrame(average_all_station_dry_season_anom_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "\n",
    "average_2_station_SPI_df = pd.DataFrame(average_2_station_spi_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "average_2_station_dry_season_anom_df = pd.DataFrame(average_2_station_dry_season_anom_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "\n",
    "southern_station_SPI_df = pd.DataFrame(southern_station_spi_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "southern_station_dry_season_anom_df = pd.DataFrame(southern_station_dry_season_anom_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "\n",
    "\n",
    "\n",
    "oni = read_index_data(ONI_file, \"ONI\")\n",
    "oni_selection = ((oni.ymdhms[:, 0] >= 1965))\n",
    "oni_time_series = oni.index.ravel()\n",
    "oni_period = oni_time_series[oni_selection]\n",
    "oni_period_matrix = oni_period.reshape(52,12)\n",
    "ONI_df = pd.DataFrame(oni_period_matrix, columns = season_list, index = range(1965,2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DJF</th>\n",
       "      <th>JFM</th>\n",
       "      <th>FMA</th>\n",
       "      <th>MAM</th>\n",
       "      <th>AMJ</th>\n",
       "      <th>MJJ</th>\n",
       "      <th>JJA</th>\n",
       "      <th>JAS</th>\n",
       "      <th>ASO</th>\n",
       "      <th>SON</th>\n",
       "      <th>OND</th>\n",
       "      <th>NDJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>1.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>-1.7</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>-1.5</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>2.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>-1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>-1.6</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>2.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>-1.6</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>2.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      DJF  JFM  FMA  MAM  AMJ  MJJ  JJA  JAS  ASO  SON  OND  NDJ\n",
       "1965 -0.5 -0.3 -0.1  0.1  0.4  0.7  1.0  1.3  1.6  1.7  1.8  1.5\n",
       "1966  1.3  1.0  0.9  0.6  0.3  0.2  0.2  0.1  0.0 -0.1 -0.1 -0.3\n",
       "1967 -0.4 -0.5 -0.5 -0.5 -0.2  0.0  0.0 -0.2 -0.3 -0.4 -0.4 -0.5\n",
       "1968 -0.7 -0.8 -0.7 -0.5 -0.1  0.2  0.5  0.4  0.3  0.4  0.6  0.8\n",
       "1969  0.9  1.0  0.9  0.7  0.6  0.5  0.4  0.5  0.8  0.8  0.8  0.7\n",
       "1970  0.6  0.4  0.4  0.3  0.1 -0.3 -0.6 -0.8 -0.8 -0.8 -0.9 -1.2\n",
       "1971 -1.3 -1.3 -1.1 -0.9 -0.8 -0.7 -0.8 -0.7 -0.8 -0.8 -0.9 -0.8\n",
       "1972 -0.7 -0.4  0.0  0.3  0.6  0.8  1.1  1.3  1.5  1.8  2.0  1.9\n",
       "1973  1.7  1.2  0.6  0.0 -0.4 -0.8 -1.0 -1.2 -1.4 -1.7 -1.9 -1.9\n",
       "1974 -1.7 -1.5 -1.2 -1.0 -0.9 -0.8 -0.6 -0.4 -0.4 -0.6 -0.7 -0.6\n",
       "1975 -0.5 -0.5 -0.6 -0.6 -0.7 -0.8 -1.0 -1.1 -1.3 -1.4 -1.5 -1.6\n",
       "1976 -1.5 -1.1 -0.7 -0.4 -0.3 -0.1  0.1  0.3  0.5  0.7  0.8  0.8\n",
       "1977  0.7  0.6  0.4  0.3  0.3  0.4  0.4  0.4  0.5  0.6  0.8  0.8\n",
       "1978  0.7  0.4  0.1 -0.2 -0.3 -0.3 -0.4 -0.4 -0.4 -0.3 -0.1  0.0\n",
       "1979  0.0  0.1  0.2  0.3  0.3  0.1  0.1  0.2  0.3  0.5  0.5  0.6\n",
       "1980  0.6  0.5  0.3  0.4  0.5  0.5  0.3  0.2  0.0  0.1  0.1  0.0\n",
       "1981 -0.2 -0.4 -0.4 -0.3 -0.2 -0.3 -0.3 -0.3 -0.2 -0.1 -0.1  0.0\n",
       "1982  0.0  0.1  0.2  0.5  0.6  0.7  0.8  1.0  1.5  1.9  2.1  2.1\n",
       "1983  2.1  1.8  1.5  1.2  1.0  0.7  0.3  0.0 -0.3 -0.6 -0.8 -0.8\n",
       "1984 -0.5 -0.3 -0.3 -0.4 -0.4 -0.4 -0.3 -0.2 -0.3 -0.6 -0.9 -1.1\n",
       "1985 -0.9 -0.7 -0.7 -0.7 -0.7 -0.6 -0.4 -0.4 -0.4 -0.3 -0.2 -0.3\n",
       "1986 -0.4 -0.4 -0.3 -0.2 -0.1  0.0  0.2  0.4  0.7  0.9  1.0  1.1\n",
       "1987  1.1  1.2  1.1  1.0  0.9  1.1  1.4  1.6  1.6  1.4  1.2  1.1\n",
       "1988  0.8  0.5  0.1 -0.3 -0.8 -1.2 -1.2 -1.1 -1.2 -1.4 -1.7 -1.8\n",
       "1989 -1.6 -1.4 -1.1 -0.9 -0.6 -0.4 -0.3 -0.3 -0.3 -0.3 -0.2 -0.1\n",
       "1990  0.1  0.2  0.2  0.2  0.2  0.3  0.3  0.3  0.4  0.3  0.4  0.4\n",
       "1991  0.4  0.3  0.2  0.2  0.4  0.6  0.7  0.7  0.7  0.8  1.2  1.4\n",
       "1992  1.6  1.5  1.4  1.2  1.0  0.8  0.5  0.2  0.0 -0.1 -0.1  0.0\n",
       "1993  0.2  0.3  0.5  0.7  0.8  0.6  0.3  0.2  0.2  0.2  0.1  0.1\n",
       "1994  0.1  0.1  0.2  0.3  0.4  0.4  0.4  0.4  0.4  0.6  0.9  1.0\n",
       "1995  0.9  0.7  0.5  0.3  0.2  0.0 -0.2 -0.5 -0.7 -0.9 -1.0 -0.9\n",
       "1996 -0.9 -0.7 -0.6 -0.4 -0.2 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5\n",
       "1997 -0.5 -0.4 -0.2  0.1  0.6  1.0  1.4  1.7  2.0  2.2  2.3  2.3\n",
       "1998  2.1  1.8  1.4  1.0  0.5 -0.1 -0.7 -1.0 -1.2 -1.2 -1.3 -1.4\n",
       "1999 -1.4 -1.2 -1.0 -0.9 -0.9 -1.0 -1.0 -1.0 -1.1 -1.2 -1.4 -1.6\n",
       "2000 -1.6 -1.4 -1.1 -0.9 -0.7 -0.7 -0.6 -0.5 -0.6 -0.7 -0.8 -0.8\n",
       "2001 -0.7 -0.6 -0.5 -0.3 -0.2 -0.1  0.0 -0.1 -0.1 -0.2 -0.3 -0.3\n",
       "2002 -0.2 -0.1  0.1  0.2  0.4  0.7  0.8  0.9  1.0  1.2  1.3  1.1\n",
       "2003  0.9  0.6  0.4  0.0 -0.2 -0.1  0.1  0.2  0.3  0.4  0.4  0.4\n",
       "2004  0.3  0.2  0.1  0.1  0.2  0.3  0.5  0.7  0.7  0.7  0.7  0.7\n",
       "2005  0.6  0.6  0.5  0.5  0.4  0.2  0.1  0.0  0.0 -0.1 -0.4 -0.7\n",
       "2006 -0.7 -0.6 -0.4 -0.2  0.0  0.1  0.2  0.3  0.5  0.8  0.9  1.0\n",
       "2007  0.7  0.3  0.0 -0.1 -0.2 -0.2 -0.3 -0.6 -0.8 -1.1 -1.2 -1.3\n",
       "2008 -1.4 -1.3 -1.1 -0.9 -0.7 -0.5 -0.3 -0.2 -0.2 -0.3 -0.5 -0.7\n",
       "2009 -0.8 -0.7 -0.4 -0.1  0.2  0.4  0.5  0.6  0.7  1.0  1.2  1.3\n",
       "2010  1.3  1.1  0.8  0.5  0.0 -0.4 -0.8 -1.1 -1.3 -1.4 -1.3 -1.4\n",
       "2011 -1.3 -1.1 -0.8 -0.6 -0.3 -0.2 -0.3 -0.5 -0.7 -0.9 -0.9 -0.8\n",
       "2012 -0.7 -0.6 -0.5 -0.4 -0.3 -0.1  0.1  0.3  0.4  0.4  0.2 -0.2\n",
       "2013 -0.4 -0.5 -0.3 -0.2 -0.2 -0.2 -0.2 -0.2 -0.2 -0.2 -0.2 -0.3\n",
       "2014 -0.5 -0.6 -0.4 -0.2  0.0  0.0  0.0  0.0  0.2  0.4  0.6  0.6\n",
       "2015  0.5  0.4  0.5  0.7  0.9  1.0  1.2  1.5  1.8  2.1  2.2  2.3\n",
       "2016  2.2  1.9  1.5  1.1  0.6  0.2  NaN  NaN  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ONI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hgt_2_lat_sl = rangeslice(ncep_latitudes, (10, 20.001))\n",
    "hgt_2_lon_sl = rangeslice(ncep_longitudes, (140, 180.001))\n",
    "\n",
    "hgt_2_avg = np.nanmean(hgt_seasonal_anom.s_anom[:,:,hgt_2_lon_sl], axis=2)\n",
    "hgt_2_avg = np.nanmean(hgt_2_avg[:,hgt_2_lat_sl], axis=1)\n",
    "\n",
    "hgt_2_avg_matrix = hgt_2_avg.reshape(52, 12)\n",
    "\n",
    "hgt_2_avg_matrix_df = pd.DataFrame(hgt_2_avg_matrix, columns = season_list, index = range(1965,2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from metpy.units import units\n",
    "\n",
    "# vorticity_seasonal = np.empty_like(uwnd_seasonal_anom.s_anom)\n",
    "\n",
    "\n",
    "# dx = 2.5*units.deg\n",
    "# dy = 2.5*units.deg\n",
    "\n",
    "# for i in range(len(uwnd_seasonal_anom.s_anom[:,0,0])):\n",
    "#     vorticity_seasonal[i,:,:] = metpy.calc.kinematics.v_vorticity(uwnd_seasonal_anom.s_anom[i,:,:]*units.m/units.sec,\n",
    "#                                                                   vwnd_seasonal_anom.s_anom[i,:,:]*units.m/units.sec,\n",
    "#                                                                   dx,dy)\n",
    "\n",
    "\n",
    "# hgt_2_lat_sl = rangeslice(ncep_latitudes, (0, 30.001))\n",
    "# hgt_2_lon_sl = rangeslice(ncep_longitudes, (140, 180.001))\n",
    "\n",
    "# vort_2_avg = np.nanmean(vorticity_seasonal[:,:,hgt_2_lon_sl], axis=2)\n",
    "# vort_2_avg = np.nanmean(vort_2_avg[:,hgt_2_lat_sl], axis=1)\n",
    "\n",
    "# vort_2_avg_matrix = vort_2_avg.reshape(38, 12)\n",
    "\n",
    "# vort_2_avg_matrix_df = pd.DataFrame(vort_2_avg_matrix, columns = six_month_list, index = range(1979,2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hgt_2_lat_sl = rangeslice(ncep_latitudes, (5, 15.001))\n",
    "hgt_2_lon_sl = rangeslice(ncep_longitudes, (140, 180.001))\n",
    "\n",
    "uwnd_2_avg = np.nanmean(uwnd_seasonal_anom.s_anom[:,:,hgt_2_lon_sl], axis=2)\n",
    "uwnd_2_avg = np.nanmean(uwnd_2_avg[:,hgt_2_lat_sl], axis=1)\n",
    "\n",
    "uwnd_2_avg_matrix = uwnd_2_avg.reshape(52, 12)\n",
    "\n",
    "uwnd_2_avg_matrix_df = pd.DataFrame(uwnd_2_avg_matrix, columns = season_list, index = range(1965,2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hgt_2_lat_sl = rangeslice(ncep_latitudes, (15, 25.001))\n",
    "hgt_2_lon_sl = rangeslice(ncep_longitudes, (140, 180.001))\n",
    "\n",
    "uwnd_3_avg = np.nanmean(uwnd_seasonal_anom.s_anom[:,:,hgt_2_lon_sl], axis=2)\n",
    "uwnd_3_avg = np.nanmean(uwnd_3_avg[:,hgt_2_lat_sl], axis=1)\n",
    "\n",
    "uwnd_3_avg_matrix = uwnd_3_avg.reshape(52, 12)\n",
    "\n",
    "new_wind_index = uwnd_2_avg_matrix - uwnd_3_avg_matrix\n",
    "\n",
    "new_wind_index_df = pd.DataFrame(new_wind_index, columns = season_list, index = range(1965,2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor_train_df = pd.concat([ONI_df.loc[1965:2004]['SON'],ONI_df.loc[1965:2004]['JJA'],\n",
    "                                ONI_df.loc[1965:2004]['SON'] - ONI_df.loc[1965:2004]['JJA'], \n",
    "                                uwnd_2_avg_matrix_df.loc[1965:2004]['SON'], uwnd_2_avg_matrix_df.loc[1965:2004]['JJA']],\n",
    "                                axis = 1, keys = ['SON ONI', 'JJA ONI', 'ONI Tendency','SON uwnd', 'JJA uwnd'])\n",
    "\n",
    "predictor_test_df = pd.concat([ONI_df.loc[2005:2015]['SON'],ONI_df.loc[2005:2015]['JJA'],\n",
    "                               ONI_df.loc[2005:2015]['SON'] - ONI_df.loc[2005:2015]['JJA'],\n",
    "                               uwnd_2_avg_matrix_df.loc[2005:2015]['SON'], uwnd_2_avg_matrix_df.loc[2005:2015]['JJA']],\n",
    "                               axis = 1, keys = ['SON ONI', 'JJA ONI', 'ONI Tendency', 'SON uwnd', 'JJA uwnd'])\n",
    "\n",
    "predictand_train_df = average_2_station_SPI_df.loc[1966:2005]['Dec-May']\n",
    "predictand_test_df = average_2_station_SPI_df.loc[2006:2016]['Dec-May']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "hls = (50,) * 5\n",
    "\n",
    "toy_neural = MLPRegressor(hidden_layer_sizes=hls, alpha = 0.0005)\n",
    "\n",
    "num_tests = 1000\n",
    "\n",
    "toy_nn_predictions = np.empty([len(predictor_test_df), num_tests])\n",
    "\n",
    "for i in range(num_tests):\n",
    "\n",
    "    toy_neural.fit(predictor_train_df,predictand_train_df)\n",
    "    toy_nn_predictions[:,i] = toy_neural.predict(predictor_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(predictand_test_df)\n",
    "# print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy = np.arange(2006,2017)\n",
    "\n",
    "toy_max = np.amax(toy_nn_predictions,axis=1)\n",
    "toy_min = np.amin(toy_nn_predictions,axis=1)\n",
    "toy_avg = np.mean(toy_nn_predictions,axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(dummy, predictand_test_df, color = \"k\")\n",
    "for i in range(num_tests):\n",
    "    plt.plot(dummy, toy_nn_predictions[:,i])\n",
    "\n",
    "plt.title('Observed and NN Predicted SPI (NN 50n, 5L, a0.0005)', fontsize = 12)\n",
    "plt.ylabel('Predicted Dec-May SPI', color='k')\n",
    "plt.ylim(-3.5,3.5)\n",
    "plt.xlim(2005.9,2016.1)\n",
    "plt.xlabel('Year', color='k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('Final_Toy_Observed_n_Predicted_SPI_lines.pdf')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(dummy, predictand_test_df, color = \"k\")\n",
    "plt.plot(dummy, toy_avg)\n",
    "plt.fill_between(dummy, toy_min, toy_max, where=toy_max >= toy_min, facecolor='green', interpolate=True)\n",
    "\n",
    "plt.title('Observed and NN Predicted SPI (NN 50n, 5L, a0.0005)', fontsize = 12)\n",
    "plt.ylabel('Predicted Dec-May SPI', color='k')\n",
    "plt.ylim(-3.5,3.5)\n",
    "plt.xlim(2005.9,2016.1)\n",
    "plt.xlabel('Year', color='k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('Final_Toy_Observed_n_Predicted_SPI_area.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of the Probabilistic forecast and Heidke Skill Score calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Probabilistic forecast from ensemble of deterministic forecasts\n",
    "\n",
    "\n",
    "def deterministic_to_probabilistic(forecasts):\n",
    "    \n",
    "    probabilistic_forecasts = np.empty([np.shape(forecasts)[0],3])\n",
    "    \n",
    "    for i in range(np.shape(forecasts)[0]):\n",
    "        \n",
    "        lower_count = sum(spi <-0.43 for spi in forecasts[i,:])\n",
    "        mid_count = sum(-0.43<= spi <= 0.43 for spi in forecasts[i,:])\n",
    "        upper_count = sum(0.43< spi for spi in forecasts[i,:])\n",
    "        \n",
    "        lower_fraction = lower_count / np.shape(forecasts)[1]\n",
    "        mid_fraction = mid_count / np.shape(forecasts)[1]\n",
    "        upper_fraction = upper_count / np.shape(forecasts)[1]\n",
    "        \n",
    "        probabilistic_forecasts[i,0] = lower_fraction * 100\n",
    "        probabilistic_forecasts[i,1] = mid_fraction * 100\n",
    "        probabilistic_forecasts[i,2] = upper_fraction * 100\n",
    "        \n",
    "        \n",
    "    return probabilistic_forecasts\n",
    "\n",
    "def HSS_calculation(prob_forecast, observed_spi):\n",
    "    \n",
    "    HSS_matrix = np.empty([np.shape(prob_forecast)[0]])\n",
    "    \n",
    "    total_hits = 0\n",
    "    \n",
    "    for i in range(np.shape(prob_forecast)[0]):\n",
    "        \n",
    "        spi_cat = np.zeros(3)\n",
    "        \n",
    "        if observed_spi[i] <-0.43:\n",
    "            spi_cat[0] = 1\n",
    "        if -0.43<= observed_spi[i] <= 0.43:\n",
    "            spi_cat[1] = 1\n",
    "        if observed_spi[i] > 0.43:\n",
    "            spi_cat[2] = 1\n",
    "            \n",
    "        if np.argmax(prob_forecast[i,:]) ==  np.argmax(spi_cat[:]):\n",
    "            hit = 1\n",
    "        else:\n",
    "            hit = 0\n",
    "       \n",
    "        HSS_matrix[i] =  (hit - 0.3) / (1 - 0.3)   \n",
    "        \n",
    "        total_hits = total_hits + hit\n",
    "     \n",
    "    expected_hits = np.shape(prob_forecast)[0]/3\n",
    "    \n",
    "    aggregate_HSS = (total_hits - expected_hits) / (np.shape(prob_forecast)[0] - expected_hits)\n",
    "        \n",
    "    return HSS_matrix, aggregate_HSS\n",
    "        \n",
    "    \n",
    "def RPSS_calculation(prob_forecast, observed_spi):\n",
    "    \n",
    "    RPSS_matrix = np.empty([np.shape(prob_forecast)[0]])\n",
    "    \n",
    "    for i in range(np.shape(prob_forecast)[0]):\n",
    "        \n",
    "        spi_probs = np.zeros(3)\n",
    "        \n",
    "        if observed_spi[i] <-0.43:\n",
    "            spi_probs[0] = 1\n",
    "        if -0.43<= observed_spi[i] <= 0.43:\n",
    "            spi_probs[1] = 1\n",
    "        if observed_spi[i] > 0.43:\n",
    "            spi_probs[2] = 1\n",
    "        \n",
    "        obs_cumm_probs = np.cumsum(spi_probs)\n",
    "        scaled_forecast_probs = prob_forecast[i,:]/100\n",
    "        forecast_cumm_probs = np.cumsum(scaled_forecast_probs)\n",
    "        \n",
    "        cumm_prob_diff = np.subtract(forecast_cumm_probs,obs_cumm_probs)\n",
    "        cumm_prob_diff_square = np.square(cumm_prob_diff)\n",
    "\n",
    "        RPS =  (1 / 2) * np.sum(cumm_prob_diff_square)\n",
    "        \n",
    "        RPSS_matrix[i] = 1 - (RPS / 0.278)\n",
    "        \n",
    "    return RPSS_matrix\n",
    "\n",
    "def observed_category(observed_spi):\n",
    "    \n",
    "    obs_cat = []\n",
    "    \n",
    "    for i in range(np.shape(observed_spi)[0]):\n",
    "        \n",
    "        if observed_spi[i] <-0.43:\n",
    "            spi_cat = 'below'\n",
    "        if -0.43<= observed_spi[i] <= 0.43:\n",
    "            spi_cat = 'average'\n",
    "        if observed_spi[i] > 0.43:\n",
    "            spi_cat = 'above'\n",
    "            \n",
    "        obs_cat.append(spi_cat)\n",
    "        \n",
    "    return obs_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(predictand_test_df.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3181818181818182\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 SPI</th>\n",
       "      <th>2 F Below Prob</th>\n",
       "      <th>3 F Avg Prob</th>\n",
       "      <th>4 F Above Prob</th>\n",
       "      <th>5 Observed Cat</th>\n",
       "      <th>6 HSS</th>\n",
       "      <th>7 RPSS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>-0.539194</td>\n",
       "      <td>63.5</td>\n",
       "      <td>35.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>below</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>-0.279056</td>\n",
       "      <td>18.6</td>\n",
       "      <td>69.1</td>\n",
       "      <td>12.3</td>\n",
       "      <td>average</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>-0.307462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>97.1</td>\n",
       "      <td>average</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.695757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>-0.896638</td>\n",
       "      <td>38.1</td>\n",
       "      <td>37.2</td>\n",
       "      <td>24.7</td>\n",
       "      <td>below</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.201133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>-0.776060</td>\n",
       "      <td>37.2</td>\n",
       "      <td>59.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>below</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.288079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0.666284</td>\n",
       "      <td>12.9</td>\n",
       "      <td>83.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>above</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.711745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>-0.016615</td>\n",
       "      <td>0.9</td>\n",
       "      <td>56.8</td>\n",
       "      <td>42.3</td>\n",
       "      <td>average</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>-0.535341</td>\n",
       "      <td>90.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>below</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>1.064682</td>\n",
       "      <td>88.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>above</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-2.177007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>1.297990</td>\n",
       "      <td>1.5</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>above</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.781025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>-1.562623</td>\n",
       "      <td>99.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>below</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1 SPI  2 F Below Prob  3 F Avg Prob  4 F Above Prob 5 Observed Cat  \\\n",
       "2006 -0.539194            63.5          35.7             0.8          below   \n",
       "2007 -0.279056            18.6          69.1            12.3        average   \n",
       "2008 -0.307462             0.0           2.9            97.1        average   \n",
       "2009 -0.896638            38.1          37.2            24.7          below   \n",
       "2010 -0.776060            37.2          59.0             3.8          below   \n",
       "2011  0.666284            12.9          83.8             3.3          above   \n",
       "2012 -0.016615             0.9          56.8            42.3        average   \n",
       "2013 -0.535341            90.0           9.8             0.2          below   \n",
       "2014  1.064682            88.0          11.6             0.4          above   \n",
       "2015  1.297990             1.5          98.0             0.5          above   \n",
       "2016 -1.562623            99.9           0.1             0.0          below   \n",
       "\n",
       "         6 HSS    7 RPSS  \n",
       "2006  1.000000  0.760272  \n",
       "2007  1.000000  0.910567  \n",
       "2008 -0.428571 -0.695757  \n",
       "2009  1.000000  0.201133  \n",
       "2010 -0.428571  0.288079  \n",
       "2011 -0.428571 -0.711745  \n",
       "2012  1.000000  0.678040  \n",
       "2013  1.000000  0.982007  \n",
       "2014 -0.428571 -2.177007  \n",
       "2015 -0.428571 -0.781025  \n",
       "2016  1.000000  0.999998  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = deterministic_to_probabilistic(toy_nn_predictions)\n",
    "\n",
    "spi_time_series = predictand_test_df.values #This is done to convert the DataFrame to a Numpy array\n",
    "\n",
    "hss, ag_hss = HSS_calculation(a,spi_time_series)\n",
    "rpss = RPSS_calculation(a,spi_time_series)\n",
    "obs_cat = observed_category(spi_time_series)\n",
    "\n",
    "print(ag_hss)\n",
    "\n",
    "# print(np.shape(np.transpose(spi_time_series)))\n",
    "# print(np.shape(hss))\n",
    "# print(np.shape(rpss))\n",
    "# print(np.shape(a))\n",
    "# print(len(obs_cat))\n",
    "\n",
    "forecast_eval_df =  pd.DataFrame({'1 SPI':spi_time_series,\n",
    "                                  '2 F Below Prob': a[:,0],\n",
    "                                  '3 F Avg Prob': a[:,1],\n",
    "                                  '4 F Above Prob': a[:,2], \n",
    "                                  '5 Observed Cat':obs_cat, \n",
    "                                  '6 HSS':hss, \n",
    "                                  '7 RPSS':rpss\n",
    "                                 }, index =range(2006,2017))\n",
    "\n",
    "forecast_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy NN experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSS as a function of the number of layers in the network, for 10, 50 and 100 neurons per layer and layers variying from 1 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "num_neurons = [10, 50, 100]\n",
    "layers = np.arange(1,21)\n",
    "\n",
    "hss_x_neurons = np.empty([len(layers),len(num_neurons)])\n",
    "\n",
    "\n",
    "\n",
    "spi_time_series = predictand_test_df.values #This is done to convert the DataFrame to a Numpy array\n",
    "\n",
    "\n",
    "\n",
    "for li,l in enumerate(layers):\n",
    "    for ni,n in enumerate(num_neurons):\n",
    "\n",
    "        hid_lay_siz = (n,) * l\n",
    "\n",
    "        toy_neural = MLPRegressor(hidden_layer_sizes=hid_lay_siz)\n",
    "\n",
    "        num_tests = 100\n",
    "\n",
    "        toy_nn_predictions = np.empty([len(predictor_test_df), num_tests])\n",
    "\n",
    "        for i in range(num_tests):\n",
    "\n",
    "            toy_neural.fit(predictor_train_df,predictand_train_df)\n",
    "            toy_nn_predictions[:,i] = toy_neural.predict(predictor_test_df)\n",
    "            \n",
    "        a = deterministic_to_probabilistic(toy_nn_predictions)\n",
    "        hss, ag_hss = HSS_calculation(a,spi_time_series)\n",
    "        \n",
    "        hss_x_neurons[li,ni] = ag_hss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4b54fdf518>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(layers, hss_x_neurons[:,0], label = '10 neurons per layer')\n",
    "plt.plot(layers, hss_x_neurons[:,1], label = '50 neurons per layer')\n",
    "plt.plot(layers, hss_x_neurons[:,2], label = '100 neurons per layer')\n",
    "\n",
    "plt.title('Observed and NN Predicted SPI', fontsize = 12)\n",
    "plt.ylabel('Predicted Dec-May SPI', color='k')\n",
    "#plt.ylim(-3.5,3.5)\n",
    "plt.xlim(0,21)\n",
    "plt.xlabel('Number of Layers', color='k')\n",
    "plt.ylabel('HSS', color='k')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "#plt.savefig(\"Toy_NN_HSS_v_Layers_1_20.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSS as a function of alpha for:\n",
    "\n",
    "### a 10neuron 10 layer\n",
    "### 50n 5 layer\n",
    "### 100n 10 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "num_neurons = [10, 50, 100]\n",
    "layers = [10, 5, 10]\n",
    "\n",
    "layers_descriptor = [(num_neurons[0],) * layers[0],\n",
    "                     (num_neurons[1],) * layers[1],\n",
    "                     (num_neurons[2],) * layers[2]]\n",
    "\n",
    "alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "\n",
    "hss_alphas = np.empty([len(alphas),len(num_neurons)])\n",
    "\n",
    "\n",
    "\n",
    "spi_time_series = predictand_test_df.values #This is done to convert the DataFrame to a Numpy array\n",
    "\n",
    "\n",
    "\n",
    "for alpha_i,a in enumerate(alphas):\n",
    "    for layer_i,ly in enumerate(layers_descriptor):\n",
    "\n",
    "        toy_neural = MLPRegressor(hidden_layer_sizes=ly)\n",
    "\n",
    "        num_tests = 100\n",
    "\n",
    "        toy_nn_predictions = np.empty([len(predictor_test_df), num_tests])\n",
    "\n",
    "        for i in range(num_tests):\n",
    "\n",
    "            toy_neural.fit(predictor_train_df,predictand_train_df)\n",
    "            toy_nn_predictions[:,i] = toy_neural.predict(predictor_test_df)\n",
    "            \n",
    "        a = deterministic_to_probabilistic(toy_nn_predictions)\n",
    "        hss, ag_hss = HSS_calculation(a,spi_time_series)\n",
    "        \n",
    "        hss_alphas[alpha_i,layer_i] = ag_hss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe16d0a9940>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.semilogx(alphas, hss_alphas[:,0], label = '10 neurons 10 layers')\n",
    "plt.semilogx(alphas, hss_alphas[:,1], label = '50 neurons 5 layers')\n",
    "plt.semilogx(alphas, hss_alphas[:,2], label = '100 neurons 10 layers')\n",
    "\n",
    "plt.title('HSS as a function of regularization parameter for Toy NN', fontsize = 12)\n",
    "#plt.ylim(-3.5,3.5)\n",
    "plt.xlim(-1,101)\n",
    "plt.xlabel('alpha', color='k')\n",
    "plt.ylabel('HSS', color='k')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "#plt.savefig(\"Toy_NN_HSS_v_alphas.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heavy NN model\n",
    "\n",
    "I will set up a neural network model to forecast using the entire gridded datasets insteade of area average indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "(40, 4708)\n",
      "11\n",
      "(11, 4708)\n"
     ]
    }
   ],
   "source": [
    "sst_area_lats = rangeslice(ersst_latitudes, (-10, 10.001))\n",
    "sst_area_lons = rangeslice(ersst_longitudes, (110, 300.001))\n",
    "\n",
    "sst_test = sst_seasonal_anom.s_anom[:,sst_area_lats,sst_area_lons]\n",
    "\n",
    "sst_selection = (sst_ca.ymdhms[:, 1] == 10)\n",
    "sst_selection = np.nonzero(sst_selection)[0]\n",
    "sst_test_region_son = sst_test[sst_selection]\n",
    "sst_flat_son = np.reshape(sst_test_region_son,(52,np.shape(sst_test_region_son)[1]*np.shape(sst_test_region_son)[2]))\n",
    "\n",
    "sst_son_train = sst_flat_son[:40,:]\n",
    "sst_son_test = sst_flat_son[40:-1,:]\n",
    "\n",
    "sst_selection = (sst_ca.ymdhms[:, 1] == 7)\n",
    "sst_selection = np.nonzero(sst_selection)[0]\n",
    "sst_test_region_jja = sst_test[sst_selection]\n",
    "sst_flat_jja = np.reshape(sst_test_region_jja,(52,np.shape(sst_test_region_jja)[1]*np.shape(sst_test_region_jja)[2]))\n",
    "\n",
    "sst_jja_train = sst_flat_jja[:40,:]\n",
    "sst_jja_test = sst_flat_jja[40:-1,:]\n",
    "\n",
    "sst_tendency_train = np.subtract(sst_son_train,sst_jja_train)\n",
    "sst_tendency_test = np.subtract(sst_son_test,sst_jja_test)\n",
    "\n",
    "uwnd_area_lats = rangeslice(ncep_latitudes, (-5, 20.001))\n",
    "uwnd_area_lons = rangeslice(ncep_longitudes, (110, 300.001))\n",
    "\n",
    "uwnd_test = uwnd_seasonal_anom.s_anom[:,uwnd_area_lats, uwnd_area_lons]\n",
    "\n",
    "uwnd_selection = (uwnd_ca.ymdhms[:, 1] == 10)\n",
    "uwnd_selection = np.nonzero(uwnd_selection)[0]\n",
    "uwnd_test_region_son = uwnd_test[uwnd_selection]\n",
    "uwnd_flat = np.reshape(uwnd_test_region_son,(52,np.shape(uwnd_test_region_son)[1]*np.shape(uwnd_test_region_son)[2]))\n",
    "\n",
    "uwnd_predictor_train = uwnd_flat[:40,:]\n",
    "uwnd_predictor_test = uwnd_flat[40:-1,:]\n",
    "\n",
    "hgt_area_lats = rangeslice(ncep_latitudes, (-10, 10.001))\n",
    "hgt_area_lons = rangeslice(ncep_longitudes, (110, 300.001))\n",
    "\n",
    "hgt_test = hgt_seasonal_anom.s_anom[:,hgt_area_lats, hgt_area_lons]\n",
    "\n",
    "hgt_selection = (hgt_ca.ymdhms[:, 1] == 10)\n",
    "hgt_selection = np.nonzero(hgt_selection)[0]\n",
    "hgt_test_region_son = hgt_test[hgt_selection]\n",
    "hgt_flat = np.reshape(hgt_test_region_son,(52,np.shape(hgt_test_region_son)[1]*np.shape(hgt_test_region_son)[2]))\n",
    "\n",
    "hgt_predictor_train = hgt_flat[:40,:]\n",
    "hgt_predictor_test = hgt_flat[40:-1,:]\n",
    "\n",
    "predictor_train_matrix = np.hstack((sst_son_train, sst_jja_train, sst_tendency_train,\n",
    "                                    uwnd_predictor_train, hgt_predictor_train))\n",
    "predictor_test_matrix = np.hstack((sst_son_test, sst_jja_test, sst_tendency_test, \n",
    "                                   uwnd_predictor_test, hgt_predictor_test))\n",
    "\n",
    "print(len(predictand_train_df))\n",
    "print(np.shape(predictor_train_matrix))\n",
    "\n",
    "print(len(predictand_test_df))\n",
    "print(np.shape(predictor_test_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(predictor_train_matrix)  \n",
    "predictor_train_matrix_scaled = scaler.transform(predictor_train_matrix)  \n",
    "# apply same transformation to test data\n",
    "predictor_test_matrix_scaled = scaler.transform(predictor_test_matrix)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hls = (50,) * 5\n",
    "\n",
    "heavy_neural_scaled = MLPRegressor(hidden_layer_sizes=hls, alpha = 0.0005)\n",
    "\n",
    "# heavy_neural_scaled.fit(predictor_train_matrix_scaled,predictand_train_df)\n",
    "# heavy_test_scaled = heavy_neural_scaled.predict(predictor_test_matrix_scaled)\n",
    "\n",
    "#heavy_neural = MLPRegressor(hidden_layer_sizes=(100, 100, 100))\n",
    "\n",
    "# heavy_neural.fit(predictor_train_matrix,predictand_train_df)\n",
    "# heavy_test = heavy_neural.predict(predictor_test_matrix)\n",
    "\n",
    "num_tests = 100\n",
    "\n",
    "scaled_nn_predictions = np.empty([len(predictor_test_df), num_tests])\n",
    "#no_scaled_nn_predictions = np.empty([len(predictor_test_df), num_tests])\n",
    "\n",
    "for i in range(num_tests):\n",
    "    heavy_neural_scaled.fit(predictor_train_matrix_scaled,predictand_train_df)\n",
    "    scaled_nn_predictions[:,i] = heavy_neural_scaled.predict(predictor_test_matrix_scaled)\n",
    "    \n",
    "#     heavy_neural.fit(predictor_train_matrix,predictand_train_df)\n",
    "#     no_scaled_nn_predictions[:,i] = heavy_neural.predict(predictor_test_matrix)\n",
    "\n",
    "#plt.figure()\n",
    "\n",
    "# dummy = np.arange(2006,2017)\n",
    "# plt.plot(dummy,predictand_test_df, c='k')\n",
    "# plt.plot(dummy, heavy_test, c='b')\n",
    "# plt.plot(dummy, heavy_test_scaled, c='g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04545454545454547\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 SPI</th>\n",
       "      <th>2 F Below Prob</th>\n",
       "      <th>3 F Avg Prob</th>\n",
       "      <th>4 F Above Prob</th>\n",
       "      <th>5 Observed Cat</th>\n",
       "      <th>6 HSS</th>\n",
       "      <th>7 RPSS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>-0.539194</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>38</td>\n",
       "      <td>below</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-1.058273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>-0.279056</td>\n",
       "      <td>36</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>average</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.766187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>-0.307462</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>53</td>\n",
       "      <td>average</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.494065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>-0.896638</td>\n",
       "      <td>19</td>\n",
       "      <td>73</td>\n",
       "      <td>8</td>\n",
       "      <td>below</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.191547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>-0.776060</td>\n",
       "      <td>38</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>below</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.304137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0.666284</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>47</td>\n",
       "      <td>above</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.483273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>-0.016615</td>\n",
       "      <td>6</td>\n",
       "      <td>87</td>\n",
       "      <td>7</td>\n",
       "      <td>average</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>-0.535341</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>21</td>\n",
       "      <td>below</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.634892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>1.064682</td>\n",
       "      <td>7</td>\n",
       "      <td>60</td>\n",
       "      <td>33</td>\n",
       "      <td>above</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.183813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>1.297990</td>\n",
       "      <td>18</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>above</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.227878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>-1.562623</td>\n",
       "      <td>97</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>below</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1 SPI  2 F Below Prob  3 F Avg Prob  4 F Above Prob 5 Observed Cat  \\\n",
       "2006 -0.539194               0            62              38          below   \n",
       "2007 -0.279056              36            62               2        average   \n",
       "2008 -0.307462               2            45              53        average   \n",
       "2009 -0.896638              19            73               8          below   \n",
       "2010 -0.776060              38            57               5          below   \n",
       "2011  0.666284               8            45              47          above   \n",
       "2012 -0.016615               6            87               7        average   \n",
       "2013 -0.535341               7            72              21          below   \n",
       "2014  1.064682               7            60              33          above   \n",
       "2015  1.297990              18            45              37          above   \n",
       "2016 -1.562623              97             3               0          below   \n",
       "\n",
       "         6 HSS    7 RPSS  \n",
       "2006 -0.428571 -1.058273  \n",
       "2007  1.000000  0.766187  \n",
       "2008 -0.428571  0.494065  \n",
       "2009 -0.428571 -0.191547  \n",
       "2010 -0.428571  0.304137  \n",
       "2011  1.000000  0.483273  \n",
       "2012  1.000000  0.984712  \n",
       "2013 -0.428571 -0.634892  \n",
       "2014 -0.428571  0.183813  \n",
       "2015 -0.428571  0.227878  \n",
       "2016  1.000000  0.998381  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = deterministic_to_probabilistic(scaled_nn_predictions)\n",
    "\n",
    "spi_time_series = predictand_test_df.values #This is done to convert the DataFrame to a Numpy array\n",
    "\n",
    "heavy_hss, heavy_ag_hss = HSS_calculation(b,spi_time_series)\n",
    "heavy_rpss = RPSS_calculation(b,spi_time_series)\n",
    "obs_cat = observed_category(spi_time_series)\n",
    "\n",
    "print(heavy_ag_hss)\n",
    "\n",
    "# print(np.shape(np.transpose(spi_time_series)))\n",
    "# print(np.shape(hss))\n",
    "# print(np.shape(rpss))\n",
    "# print(np.shape(a))\n",
    "# print(len(obs_cat))\n",
    "\n",
    "heavy_forecast_eval_df =  pd.DataFrame({'1 SPI':spi_time_series,\n",
    "                                  '2 F Below Prob': b[:,0],\n",
    "                                  '3 F Avg Prob': b[:,1],\n",
    "                                  '4 F Above Prob': b[:,2], \n",
    "                                  '5 Observed Cat':obs_cat, \n",
    "                                  '6 HSS':heavy_hss, \n",
    "                                  '7 RPSS':heavy_rpss\n",
    "                                  }, index =range(2006,2017))\n",
    "\n",
    "heavy_forecast_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forecast_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_max = np.amax(scaled_nn_predictions,axis=1)\n",
    "scaled_min = np.amin(scaled_nn_predictions,axis=1)\n",
    "scaled_avg = np.mean(scaled_nn_predictions,axis=1)\n",
    "\n",
    "no_scaled_max = np.amax(no_scaled_nn_predictions,axis=1)\n",
    "no_scaled_min = np.amin(no_scaled_nn_predictions,axis=1)\n",
    "no_scaled_avg = np.mean(no_scaled_nn_predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(dummy, predictand_test_df, color = \"k\")\n",
    "plt.plot(dummy, scaled_avg)\n",
    "plt.fill_between(dummy, scaled_min, scaled_max, where=scaled_max >= scaled_min, facecolor='green', interpolate=True)\n",
    "\n",
    "plt.title('Observed and NN Predicted SPI', fontsize = 12)\n",
    "plt.ylabel('Predicted Dec-May SPI', color='k')\n",
    "plt.ylim(-5,5)\n",
    "plt.xlim(2005.9,2016.1)\n",
    "plt.xlabel('Year', color='k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('100x3_50iter_scaled_heavy_NN_Observed_n_Predicted_SPI_area_SST_uwnd_hgt.pdf')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(dummy, predictand_test_df, color = \"k\")\n",
    "plt.plot(dummy, no_scaled_avg)\n",
    "plt.fill_between(dummy, no_scaled_min, no_scaled_max, where=no_scaled_max >= no_scaled_min, \n",
    "                 facecolor='green', interpolate=True)\n",
    "\n",
    "plt.title('Observed and NN Predicted SPI', fontsize = 12)\n",
    "plt.ylabel('Predicted Dec-May SPI', color='k')\n",
    "plt.ylim(-5,5)\n",
    "plt.xlim(2005.9,2016.1)\n",
    "plt.xlabel('Year', color='k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('100x3_50iter_no_scaled_heavy_NN_Observed_n_Predicted_SPI_area_SST_uwnd_hgt.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Regressor model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_toy = SGDRegressor()\n",
    "\n",
    "num_tests = 100\n",
    "\n",
    "sgd_toy_nn_predictions = np.empty([len(predictor_test_df), num_tests])\n",
    "\n",
    "for i in range(num_tests):\n",
    "\n",
    "    sgd_toy.fit(predictor_train_df,predictand_train_df)\n",
    "    sgd_toy_nn_predictions[:,i] = sgd_toy.predict(predictor_test_df)\n",
    "    \n",
    "sgd_toy_max = np.amax(sgd_toy_nn_predictions,axis=1)\n",
    "sgd_toy_min = np.amin(sgd_toy_nn_predictions,axis=1)\n",
    "sgd_toy_avg = np.mean(sgd_toy_nn_predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(dummy, predictand_test_df, color = \"k\")\n",
    "plt.plot(dummy, sgd_toy_avg)\n",
    "plt.fill_between(dummy, sgd_toy_min, sgd_toy_max, where=sgd_toy_max >= sgd_toy_min, facecolor='green', interpolate=True)\n",
    "\n",
    "plt.title('Observed and NN Predicted SPI', fontsize = 12)\n",
    "plt.ylabel('Predicted Dec-May SPI', color='k')\n",
    "plt.ylim(-5,5)\n",
    "plt.xlim(2005.9,2016.1)\n",
    "plt.xlabel('Year', color='k')\n",
    "plt.grid(True)\n",
    "\n",
    "#plt.savefig('sgd_Observed_n_Predicted_SPI_area.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA model setup\n",
    "\n",
    "## Here i will setup a CCA model that is analogous whit what we use in PEAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sstdmz = sst_test_region_son.filled(0) * np.cos(np.deg2rad(ersst_latitudes[sst_area_lats]))[np.newaxis, :, np.newaxis]\n",
    "ssteof = eof.EOF(sstdmz[:40,:,:])\n",
    "\n",
    "sst_mask = np.ma.getmask(sst_test_region_son)\n",
    "\n",
    "uwnddmz = uwnd_test_region_son.filled(0) * np.cos(np.deg2rad(ncep_latitudes[uwnd_area_lats]))[np.newaxis, :, np.newaxis]\n",
    "uwndeof = eof.EOF(uwnddmz[:40,:,:])\n",
    "\n",
    "hgtdmz = hgt_test_region_son.filled(0) * np.cos(np.deg2rad(ncep_latitudes[hgt_area_lats]))[np.newaxis, :, np.newaxis]\n",
    "hgteof = eof.EOF(hgtdmz[:40,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sst_ts = ssteof.u[:,:8]\n",
    "uwnd_ts = uwndeof.u[:,:8]\n",
    "hgt_ts = hgteof.u[:,:8]\n",
    "\n",
    "cca_predictor_matrix = np.hstack((sst_ts,uwnd_ts,hgt_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sst_pats = np.ma.array(ssteof.v_reshaped)\n",
    "sst_pats_masked = np.ma.masked_where(sst_mask[:40,:,:], sst_pats) \n",
    "sst_pats /= np.cos(np.deg2rad(ersst_latitudes[sst_area_lats]))[np.newaxis, :, np.newaxis]\n",
    "\n",
    "uwnd_pats = np.array(uwndeof.v_reshaped)\n",
    "uwnd_pats /= np.cos(np.deg2rad(ncep_latitudes[uwnd_area_lats]))[np.newaxis, :, np.newaxis]\n",
    "\n",
    "hgt_pats = np.array(hgteof.v_reshaped)\n",
    "hgt_pats /= np.cos(np.deg2rad(ncep_latitudes[hgt_area_lats]))[np.newaxis, :, np.newaxis]\n",
    "\n",
    "relevant_sst_pats = sst_pats[:8,:,:]\n",
    "relevant_uwnd_pats = uwnd_pats[:8,:,:]\n",
    "relevant_hgt_pats = hgt_pats[:8,:,:]\n",
    "\n",
    "reshaped_sst_pats = np.reshape(relevant_sst_pats, (np.shape(relevant_sst_pats)[0], \n",
    "                                                   np.shape(relevant_sst_pats)[1]*np.shape(relevant_sst_pats)[2]))\n",
    "\n",
    "reshaped_uwnd_pats = np.reshape(relevant_uwnd_pats, (np.shape(relevant_uwnd_pats)[0], \n",
    "                                                   np.shape(relevant_uwnd_pats)[1]*np.shape(relevant_uwnd_pats)[2]))\n",
    "\n",
    "reshaped_hgt_pats = np.reshape(relevant_hgt_pats, (np.shape(relevant_hgt_pats)[0], \n",
    "                                                   np.shape(relevant_hgt_pats)[1]*np.shape(relevant_hgt_pats)[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#project observations onto first 8 EOF pats\n",
    "\n",
    "sst_predictor_data = sstdmz[40:,:,:]\n",
    "uwnd_predictor_data = uwnddmz[40:,:,:]\n",
    "hgt_predictor_data = hgtdmz[40:,:,:]\n",
    "\n",
    "reshaped_sst_predictor_data = np.reshape(sst_predictor_data, (np.shape(sst_predictor_data)[0], \n",
    "                                         np.shape(sst_predictor_data)[1]*np.shape(sst_predictor_data)[2]))\n",
    "\n",
    "reshaped_uwnd_predictor_data = np.reshape(uwnd_predictor_data, (np.shape(uwnd_predictor_data)[0], \n",
    "                                          np.shape(uwnd_predictor_data)[1]*np.shape(uwnd_predictor_data)[2]))\n",
    "            \n",
    "reshaped_hgt_predictor_data = np.reshape(hgt_predictor_data, (np.shape(hgt_predictor_data)[0], \n",
    "                                         np.shape(hgt_predictor_data)[1]*np.shape(hgt_predictor_data)[2]))\n",
    "                                         \n",
    "print(np.shape(reshaped_sst_predictor_data))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "projected_sst = np.empty((np.shape(reshaped_sst_predictor_data)[0],8))\n",
    "projected_uwnd = np.empty((np.shape(reshaped_uwnd_predictor_data)[0],8))\n",
    "projected_hgt = np.empty((np.shape(reshaped_hgt_predictor_data)[0],8))\n",
    "\n",
    "\n",
    "for i in range(np.shape(reshaped_sst_predictor_data)[0]):\n",
    "    for r in range(8):\n",
    "    \n",
    "        projected_sst[i,r] = np.dot(reshaped_sst_predictor_data[i],ssteof.v[r,:])\n",
    " \n",
    "\n",
    "for i in range(np.shape(reshaped_uwnd_predictor_data)[0]):\n",
    "    for r in range(8):\n",
    "    \n",
    "        projected_uwnd[i,r] = np.dot(reshaped_uwnd_predictor_data[i],uwndeof.v[r,:])\n",
    "        \n",
    "        \n",
    "for i in range(np.shape(reshaped_hgt_predictor_data)[0]):\n",
    "    for r in range(8):\n",
    "    \n",
    "        projected_hgt[i,r] = np.dot(reshaped_hgt_predictor_data[i],hgteof.v[r,:])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor_test_matrix = np.hstack((projected_sst,projected_uwnd,projected_hgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "cca_toy = CCA()\n",
    "\n",
    "num_tests = 100\n",
    "\n",
    "cca_toy_nn_predictions = np.empty([len(predictor_test_matrix), num_tests])\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(cca_predictor_matrix)  \n",
    "predictor_train_matrix_scaled = scaler.transform(cca_predictor_matrix)  \n",
    "# apply same transformation to test data\n",
    "predictor_test_matrix_scaled = scaler.transform(predictor_test_matrix)\n",
    "\n",
    "obs_scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "obs_scaler.fit(predictand_train_df)  \n",
    "predictand_train_scaled = obs_scaler.transform(predictand_train_df)  \n",
    "# apply same transformation to test data\n",
    "predictand_test_scaled = obs_scaler.transform(predictand_test_df)\n",
    "\n",
    "for i in range(num_tests):\n",
    "\n",
    "    cca_toy.fit(predictor_train_matrix_scaled,predictand_train_scaled)\n",
    "    #X_t, Y_t = cca_toy.transform(predictor_test_matrix,predictand_test_df)\n",
    "    #print(np.shape(X_t))\n",
    "    \n",
    "    cca_toy_nn_predictions[:,i] = np.squeeze(cca_toy.predict(predictor_test_matrix_scaled))\n",
    "    \n",
    "cca_toy_max = np.amax(cca_toy_nn_predictions,axis=1)\n",
    "cca_toy_min = np.amin(cca_toy_nn_predictions,axis=1)\n",
    "cca_toy_avg = np.mean(cca_toy_nn_predictions,axis=1)\n",
    "\n",
    "print(np.shape(cca_toy_avg))\n",
    "print(np.shape(dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(dummy, predictand_test_scaled, color = \"k\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(dummy, cca_toy_avg[:-1])\n",
    "ax2.fill_between(dummy, cca_toy_min[:-1], cca_toy_max[:-1], where=cca_toy_max[:-1] >= cca_toy_min[:-1], facecolor='green', interpolate=True)\n",
    "\n",
    "plt.title('Observed and CCA Predicted SPI', fontsize = 12)\n",
    "plt.ylabel('Predicted Dec-May SPI', color='k')\n",
    "#plt.ylim(-5,5)\n",
    "plt.xlim(2005.9,2016.1)\n",
    "plt.xlabel('Year', color='k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('cca_Observed_n_Predicted_SPI_area.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===============================================================\n",
    "# STOP HERE\n",
    "# ==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "varlist = [average_2_station_SPI_df.loc[1979:]['Dec-May'],  \n",
    "           new_wind_index_df.loc[:]['Dec-May']]\n",
    "varlist_2 = [ONI_df.loc[1979:]['DJF'], \n",
    "             ONI_df.loc[1979:2016]['DJF']]\n",
    "plot_titles = ['Guam and Kwaj, Avg Dry Season SPI vs ONI',\n",
    "               'Uwnd vs ONI']\n",
    "ylab = ['SPI', 'Uwnd']\n",
    "xlab = ['ONI (C)', 'ONI (c)']\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, sharex=True, \n",
    "                        tight_layout=True,  # trims margins\n",
    "                        figsize=(12, 8))\n",
    "\n",
    "fname = 'Time'\n",
    "#axs[0].set_title(fname)\n",
    "#lines_list = [] we dont really use this\n",
    "for yvar, xvar, tit, ax, y, x in zip(varlist,varlist_2, plot_titles, axs, ylab, xlab):\n",
    "    #cor = ' Corr=' + str('%.4f' % round(np.corrcoef(var,var2)[0][1],4))\n",
    "    ax.set_title(tit, fontsize = 12)\n",
    "    scatter = ax.scatter(xvar, yvar, c='k', label = '1979-2016')\n",
    "    ax.set_ylabel(y, color='k')\n",
    "    ax.set_ylim(-3,3)\n",
    "    ax.set_xlim(-2.5,2.5)\n",
    "    ax.set_xlabel(x, color='k')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    year_list = [ 1984, 2001, 2006, 2009, 2013]\n",
    "    dry_y = np.empty(len(year_list))\n",
    "    dry_x = np.empty(len(year_list))\n",
    "    non_dry_year_list = [1985, 1986, 1996, 1997, 2014]\n",
    "    non_dry_y = np.empty(len(non_dry_year_list))\n",
    "    non_dry_x = np.empty(len(non_dry_year_list))\n",
    "    la_nina_list =  [1989, 1999, 2000, 2008, 2011]\n",
    "    lanina_y = np.empty(len(la_nina_list))\n",
    "    lanina_x = np.empty(len(la_nina_list))\n",
    "    \n",
    "    for year in range(1979,2017):\n",
    "        \n",
    "        if year in year_list:\n",
    "            i = year_list.index(year)\n",
    "            dry_y[i] = yvar[year]\n",
    "            dry_x[i] = xvar[year]\n",
    "            #scatter = ax.scatter(xvar[year], yvar[year], c='r', label = 'Members of Dry Composite')\n",
    "         \n",
    "        elif year in la_nina_list:\n",
    "            i = la_nina_list.index(year)\n",
    "            lanina_y[i] = yvar[year]\n",
    "            lanina_x[i] = xvar[year]\n",
    "            \n",
    "        elif year in non_dry_year_list:\n",
    "            i = non_dry_year_list.index(year)\n",
    "            non_dry_y[i] = yvar[year]\n",
    "            non_dry_x[i] = xvar[year]\n",
    "            \n",
    "    scatter = ax.scatter(dry_x, dry_y, c='r', label = 'Members of Dry Composite')\n",
    "    scatter = ax.scatter(non_dry_x, non_dry_y, c='deepskyblue', label = 'Members of Non Dry Composite')\n",
    "    scatter = ax.scatter(lanina_x, lanina_y, c='b', label = 'Members of La Nina Composite')\n",
    "    \n",
    "    plt.legend(loc='upper center')\n",
    "    \n",
    "    ax_max = np.amax(xvar)\n",
    "    ax_min = np.amin(xvar)\n",
    "    ax_tot_max2 = np.maximum(ax_max, np.absolute(ax_min))\n",
    "    \n",
    "    if y == 'SPI':\n",
    "        ax.set_ylim([-2,2])\n",
    "        \n",
    "    elif y == 'Vort':\n",
    "        ax.set_ylim([-0.1,0.1])\n",
    "    elif y == 'Uwnd':\n",
    "        ax.set_ylim([-5,5])    \n",
    "    else:\n",
    "        ax_max = np.amax(yvar)\n",
    "        ax_min = np.amin(yvar)\n",
    "        ax_tot_max2 = np.maximum(ax_max, np.absolute(ax_min))\n",
    "        ax.set_ylim([-ax_tot_max2-(10*ax_tot_max2/100),ax_tot_max2+(10*ax_tot_max2/100)])\n",
    "        \n",
    "    #if ax ==1:\n",
    "    year_labs = [str(x) for x in list(np.arange(1979,2017))]\n",
    "    \n",
    "    for year, x, y in zip(year_labs, xvar, yvar):\n",
    "        ax.annotate(year,xy=(x,y), xytext = (-20,0),\n",
    "                   textcoords = 'offset points',\n",
    "                   #textcoords = 'figure fraction', \n",
    "                   ha = 'right', va='bottom', fontsize = 10,\n",
    "                   arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "\n",
    "    \n",
    "    # Reduce the number of y-axis ticks:\n",
    "    ax.locator_params(axis='y', nbins=4)\n",
    "\n",
    "#plt.legend(bbox_to_anchor = (0,0),loc = 'upper left',borderaxespad =0.)\n",
    "\n",
    "plt.legend()\n",
    "    \n",
    "fig.savefig('scatterplots_GK_SPI_v_new_Uwnd_index_labs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Composite maps\n",
    "\n",
    "## Here i use the years chosen by hand for Nov-May SPI and cool ONI conditions\n",
    "\n",
    "## The years are 1981, 1984, 1999, 2001, 2009, 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year_list = [1984, 2001, 2006, 2009, 2013]\n",
    "non_dry_year_list = [1985, 1986, 1996, 1997, 2014]\n",
    "la_nina_list =  [1989, 1999, 2000, 2008, 2011]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpcp_season_members_verydry = np.zeros(12)\n",
    "\n",
    "sst_composite_members_verydry = np.empty([50,24, len(ersst_latitudes), len(ersst_longitudes)])\n",
    "sst_composite_members_verydry[:] = np.NAN\n",
    "\n",
    "precip_composite_members_verydry = np.empty([50,24, len(precip_latitudes), len(precip_longitudes)])\n",
    "precip_composite_members_verydry[:] = np.NAN\n",
    "\n",
    "uwnd_composite_members_verydry = np.empty([50,24,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "uwnd_composite_members_verydry[:] = np.NAN\n",
    "vwnd_composite_members_verydry = np.empty([50,24,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "vwnd_composite_members_verydry[:] = np.NAN\n",
    "hgt_composite_members_verydry = np.empty([50,24,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "hgt_composite_members_verydry[:] = np.NAN\n",
    "\n",
    "gpcp_season_members_nondry = np.zeros(12)\n",
    "\n",
    "sst_composite_members_nondry = np.empty([50,24, len(ersst_latitudes), len(ersst_longitudes)])\n",
    "sst_composite_members_nondry[:] = np.NAN\n",
    "\n",
    "precip_composite_members_nondry = np.empty([50,24, len(precip_latitudes), len(precip_longitudes)])\n",
    "precip_composite_members_nondry[:] = np.NAN\n",
    "\n",
    "uwnd_composite_members_nondry = np.empty([50,24,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "uwnd_composite_members_nondry[:] = np.NAN\n",
    "vwnd_composite_members_nondry = np.empty([50,24,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "vwnd_composite_members_nondry[:] = np.NAN\n",
    "hgt_composite_members_nondry = np.empty([50,24,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "hgt_composite_members_nondry[:] = np.NAN\n",
    "\n",
    "gpcp_season_members_lanina = np.zeros(12)\n",
    "\n",
    "sst_composite_members_lanina = np.empty([50,24, len(ersst_latitudes), len(ersst_longitudes)])\n",
    "sst_composite_members_lanina[:] = np.NAN\n",
    "\n",
    "precip_composite_members_lanina = np.empty([50,24, len(precip_latitudes), len(precip_longitudes)])\n",
    "precip_composite_members_lanina[:] = np.NAN\n",
    "\n",
    "uwnd_composite_members_lanina = np.empty([50,24,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "uwnd_composite_members_lanina[:] = np.NAN\n",
    "vwnd_composite_members_lanina = np.empty([50,24,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "vwnd_composite_members_lanina[:] = np.NAN\n",
    "hgt_composite_members_lanina = np.empty([50,24,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "hgt_composite_members_lanina[:] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sst_selection = ((sst_ca.ymdhms[:, 0] == 1966))\n",
    "#print(sst_selection)\n",
    "#sst_selection = np.nonzero(sst_selection)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpcp_season_members_verydry = np.zeros(24)\n",
    "gpcp_season_members_nondry = np.zeros(24)\n",
    "gpcp_season_members_lanina = np.zeros(24)\n",
    "\n",
    "year_range = range(1966,2016)\n",
    "short_year_range = range(1979,2016)\n",
    "for year in short_year_range:\n",
    "    for m in range(1,13):\n",
    "        \n",
    "        if year in year_list:\n",
    "            \n",
    "            #print(year,year_range.index(year),m,cond_verydry_cool[year_range.index(year),m-1])\n",
    "            \n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year-1) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_verydry[year_range.index(year),m-1,:,:] = sst_ca.anomaly[sst_selection]\n",
    "\n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_verydry[year_range.index(year),m-1+12,:,:] = sst_ca.anomaly[sst_selection]\n",
    "\n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year-1) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_composite_members_verydry[year_range.index(year),m-1,:,:] = uwnd_ca.anomaly[selection]\n",
    "            vwnd_composite_members_verydry[year_range.index(year),m-1,:,:] = vwnd_ca.anomaly[selection]\n",
    "            hgt_composite_members_verydry[year_range.index(year),m-1,:,:] = hgt_ca.anomaly[selection]\n",
    "            \n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_composite_members_verydry[year_range.index(year),m-1+12,:,:] = uwnd_ca.anomaly[selection]\n",
    "            vwnd_composite_members_verydry[year_range.index(year),m-1+12,:,:] = vwnd_ca.anomaly[selection]\n",
    "            hgt_composite_members_verydry[year_range.index(year),m-1+12,:,:] = hgt_ca.anomaly[selection]\n",
    "\n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year-1) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_verydry[year_range.index(year),m-1,:,:] = precip_ca.anomaly[precip_selection]\n",
    "            \n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_verydry[year_range.index(year),m-1+12,:,:] = precip_ca.anomaly[precip_selection]\n",
    "\n",
    "            gpcp_season_members_verydry[m-1] = gpcp_season_members_verydry[m-1] +1\n",
    "            gpcp_season_members_verydry[m-1+12] = gpcp_season_members_verydry[m-1+12] +1\n",
    "\n",
    "        if year in non_dry_year_list:   \n",
    "\n",
    "\n",
    "            #print(year,year_range.index(year),m,cond_verydry_cool[year_range.index(year),m-1])\n",
    "\n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year-1) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_nondry[year_range.index(year),m-1,:,:] = sst_ca.anomaly[sst_selection]\n",
    "            \n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_nondry[year_range.index(year),m-1+12,:,:] = sst_ca.anomaly[sst_selection]\n",
    "\n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year-1) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_composite_members_nondry[year_range.index(year),m-1,:,:] = uwnd_ca.anomaly[selection]\n",
    "            vwnd_composite_members_nondry[year_range.index(year),m-1,:,:] = vwnd_ca.anomaly[selection]\n",
    "            hgt_composite_members_nondry[year_range.index(year),m-1,:,:] = hgt_ca.anomaly[selection]\n",
    "            \n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_composite_members_nondry[year_range.index(year),m-1+12,:,:] = uwnd_ca.anomaly[selection]\n",
    "            vwnd_composite_members_nondry[year_range.index(year),m-1+12,:,:] = vwnd_ca.anomaly[selection]\n",
    "            hgt_composite_members_nondry[year_range.index(year),m-1+12,:,:] = hgt_ca.anomaly[selection]\n",
    "\n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year-1) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_nondry[year_range.index(year),m-1,:,:] = precip_ca.anomaly[precip_selection]\n",
    "            \n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_nondry[year_range.index(year),m-1+12,:,:] = precip_ca.anomaly[precip_selection]\n",
    "\n",
    "            gpcp_season_members_nondry[m-1] = gpcp_season_members_nondry[m-1] +1\n",
    "            gpcp_season_members_nondry[m-1+12] = gpcp_season_members_nondry[m-1+12] +1\n",
    "            \n",
    "        if year in la_nina_list:   \n",
    "\n",
    "            #print(year,year_range.index(year),m,cond_verydry_cool[year_range.index(year),m-1])\n",
    "\n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year-1) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_lanina[year_range.index(year),m-1,:,:] = sst_ca.anomaly[sst_selection]\n",
    "            \n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_lanina[year_range.index(year),m-1+12,:,:] = sst_ca.anomaly[sst_selection]\n",
    "\n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year-1) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_composite_members_lanina[year_range.index(year),m-1,:,:] = uwnd_ca.anomaly[selection]\n",
    "            vwnd_composite_members_lanina[year_range.index(year),m-1,:,:] = vwnd_ca.anomaly[selection]\n",
    "            hgt_composite_members_lanina[year_range.index(year),m-1,:,:] = hgt_ca.anomaly[selection]\n",
    "            \n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_composite_members_lanina[year_range.index(year),m-1+12,:,:] = uwnd_ca.anomaly[selection]\n",
    "            vwnd_composite_members_lanina[year_range.index(year),m-1+12,:,:] = vwnd_ca.anomaly[selection]\n",
    "            hgt_composite_members_lanina[year_range.index(year),m-1+12,:,:] = hgt_ca.anomaly[selection]\n",
    "\n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year-1) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_lanina[year_range.index(year),m-1,:,:] = precip_ca.anomaly[precip_selection]\n",
    "            \n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_lanina[year_range.index(year),m-1+12,:,:] = precip_ca.anomaly[precip_selection]\n",
    "\n",
    "            gpcp_season_members_lanina[m-1] = gpcp_season_members_lanina[m-1] +1\n",
    "            gpcp_season_members_lanina[m-1+12] = gpcp_season_members_lanina[m-1+12] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precip_composite_verydry = np.nanmean(precip_composite_members_verydry, axis=0)\n",
    "sst_composite_verydry = np.nanmean(sst_composite_members_verydry, axis=0)\n",
    "uwnd_composite_verydry = np.nanmean(uwnd_composite_members_verydry, axis=0)\n",
    "vwnd_composite_verydry = np.nanmean(vwnd_composite_members_verydry, axis=0)\n",
    "hgt_composite_verydry = np.nanmean(hgt_composite_members_verydry, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precip_composite_nondry = np.nanmean(precip_composite_members_nondry, axis=0)\n",
    "sst_composite_nondry = np.nanmean(sst_composite_members_nondry, axis=0)\n",
    "uwnd_composite_nondry = np.nanmean(uwnd_composite_members_nondry, axis=0)\n",
    "vwnd_composite_nondry = np.nanmean(vwnd_composite_members_nondry, axis=0)\n",
    "hgt_composite_nondry = np.nanmean(hgt_composite_members_nondry, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precip_composite_lanina = np.nanmean(precip_composite_members_lanina, axis=0)\n",
    "sst_composite_lanina = np.nanmean(sst_composite_members_lanina, axis=0)\n",
    "uwnd_composite_lanina = np.nanmean(uwnd_composite_members_lanina, axis=0)\n",
    "vwnd_composite_lanina = np.nanmean(vwnd_composite_members_lanina, axis=0)\n",
    "hgt_composite_lanina = np.nanmean(hgt_composite_members_lanina, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from metpy.units import units\n",
    "\n",
    "vorticity_verydry = np.empty_like(uwnd_composite_verydry)\n",
    "vorticity_nondry = np.empty_like(uwnd_composite_verydry) \n",
    "vorticity_lanina = np.empty_like(uwnd_composite_verydry)\n",
    "\n",
    "dx = 2.5*units.deg\n",
    "dy = 2.5*units.deg\n",
    "\n",
    "for i in range(24):\n",
    "    vorticity_verydry[i,:,:] = metpy.calc.kinematics.v_vorticity(uwnd_composite_verydry[i,:,:]*units.m/units.sec,\n",
    "                                                          vwnd_composite_verydry[i,:,:]*units.m/units.sec,\n",
    "                                                          dx,dy)\n",
    "    vorticity_nondry[i,:,:] = metpy.calc.kinematics.v_vorticity(uwnd_composite_nondry[i,:,:]*units.m/units.sec,\n",
    "                                                         vwnd_composite_nondry[i,:,:]*units.m/units.sec,\n",
    "                                                         dx,dy)\n",
    "    \n",
    "    vorticity_lanina[i,:,:] = metpy.calc.kinematics.v_vorticity(uwnd_composite_lanina[i,:,:]*units.m/units.sec,\n",
    "                                                         vwnd_composite_lanina[i,:,:]*units.m/units.sec,\n",
    "                                                         dx,dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_sl = rangeslice(ersst_latitudes, (-6, 6.001))\n",
    "lon_sl = rangeslice(ersst_longitudes, (100, 280.001))\n",
    "\n",
    "time = range(24)\n",
    "sst_lon = ersst_longitudes[lon_sl]\n",
    "sst_mask = np.ma.getmask(np.squeeze(sst_ca.anomaly[0,:,:]))\n",
    "mask_sst_composite = np.empty_like(sst_composite_verydry)\n",
    "mask_sst_composite[:,:,:] = sst_mask\n",
    "\n",
    "masked_composite = np.ma.masked_where(mask_sst_composite, sst_composite_verydry)\n",
    "sst_avg = np.ma.mean(masked_composite[:,lat_sl,lon_sl], axis=1)\n",
    "\n",
    "masked_composite_nondry = np.ma.masked_where(mask_sst_composite, sst_composite_nondry)\n",
    "sst_avg_nondry = np.ma.mean(masked_composite_nondry[:,lat_sl,lon_sl], axis=1)\n",
    "\n",
    "masked_composite_lanina = np.ma.masked_where(mask_sst_composite, sst_composite_lanina)\n",
    "sst_avg_lanina = np.ma.mean(masked_composite_lanina[:,lat_sl,lon_sl], axis=1)\n",
    "\n",
    "\n",
    "lat_sl = rangeslice(precip_latitudes, (5, 15.001))\n",
    "lon_sl = rangeslice(precip_longitudes, (100, 280.001))\n",
    "precip_avg = np.average(precip_composite_verydry[:,lat_sl,lon_sl], axis=1)\n",
    "precip_avg_nondry = np.average(precip_composite_nondry[:,lat_sl,lon_sl], axis=1)\n",
    "precip_avg_lanina = np.average(precip_composite_lanina[:,lat_sl,lon_sl], axis=1)\n",
    "\n",
    "time = range(24)\n",
    "precip_lon = precip_longitudes[lon_sl]\n",
    "\n",
    "vort_lat_sl = rangeslice(ncep_latitudes, (10, 20.001))\n",
    "lon_sl = rangeslice(ncep_longitudes, (100, 280.001))\n",
    "ncep_lon = ncep_longitudes[lon_sl]\n",
    "vort_avg = np.average(vorticity_verydry[:,vort_lat_sl,lon_sl], axis=1)\n",
    "vort_avg_nondry = np.average(vorticity_nondry[:,vort_lat_sl,lon_sl], axis=1)\n",
    "vort_avg_lanina = np.average(vorticity_lanina[:,vort_lat_sl,lon_sl], axis=1)\n",
    "\n",
    "uwnd_lat_sl = rangeslice(ncep_latitudes, (-5, 5.001))\n",
    "uwnd_avg = np.average(uwnd_composite_verydry[:,uwnd_lat_sl,lon_sl], axis=1)\n",
    "uwnd_avg_nondry = np.average(uwnd_composite_nondry[:,uwnd_lat_sl,lon_sl], axis=1)\n",
    "uwnd_avg_lanina = np.average(uwnd_composite_lanina[:,uwnd_lat_sl,lon_sl], axis=1)\n",
    "\n",
    "hgt_lat_sl = rangeslice(ncep_latitudes, (30, 50.001))\n",
    "hgt_avg = np.average(hgt_composite_verydry[:,hgt_lat_sl,lon_sl], axis=1)\n",
    "hgt_avg_nondry = np.average(hgt_composite_nondry[:,hgt_lat_sl,lon_sl], axis=1)\n",
    "hgt_avg_lanina = np.average(hgt_composite_lanina[:,hgt_lat_sl,lon_sl], axis=1)\n",
    "\n",
    "hgt_2_lat_sl = rangeslice(ncep_latitudes, (10, 20.001))\n",
    "hgt_2_avg = np.average(hgt_composite_verydry[:,hgt_2_lat_sl,lon_sl], axis=1)\n",
    "hgt_2_avg_nondry = np.average(hgt_composite_nondry[:,hgt_2_lat_sl,lon_sl], axis=1)\n",
    "hgt_2_avg_lanina = np.average(hgt_composite_lanina[:,hgt_2_lat_sl,lon_sl], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def holmover_plots(field_1,field_2, lon, time, field_name = 'sst',s_lat = -5, n_lat = 5, save ='no'):\n",
    "\n",
    "    subplotparams = dict(left=0.1, right=0.88,\n",
    "                         bottom=0.1, top=0.9,\n",
    "                         wspace=0.05, hspace=0.1)\n",
    "\n",
    "    fig, axs = plt.subplots(2, #sharex=True,\n",
    "                            figsize=(13, 7.8),\n",
    "                            gridspec_kw=subplotparams)\n",
    "\n",
    "\n",
    "    if field_name == 'precip':\n",
    "        #reverse the coolwarm palatte to make blue rainy and red dry\n",
    "        cmap = plt.get_cmap('BrBG')\n",
    "        bounds = [-2.5,-2, -1.5,-1.25, -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 2.5]\n",
    "            \n",
    "    elif field_name == 'sst':\n",
    "        cmap = cm.coolwarm\n",
    "        bounds = np.arange(-1,1.001,0.1)\n",
    "        bound_0 = [0]\n",
    "        \n",
    "    elif field_name == 'vort':\n",
    "        cmap = cm.jet\n",
    "        bounds = np.arange(-0.05,0.051,0.005)\n",
    "        bound_0 = [0]\n",
    "        \n",
    "    elif field_name == 'uwnd':\n",
    "        cmap = cm.jet\n",
    "        bounds = np.arange(-4,4.1,0.5)\n",
    "        bound_0 = [0]\n",
    "        \n",
    "    elif field_name == 'hgt':\n",
    "        cmap = cm.jet\n",
    "        bounds = np.arange(-20,20.1,2)\n",
    "        bound_0 = [0]\n",
    "    \n",
    "    elif field_name == 'hgt_2':\n",
    "        cmap = cm.jet\n",
    "        bounds = np.arange(-10,10.1,1)\n",
    "        bound_0 = [0]\n",
    "        \n",
    "    time_labs = ['Jan (-1)', 'Feb (-1)', 'Mar (-1)', \n",
    "                 'Apr (-1)', 'May (-1)', 'Jun (-1)', \n",
    "                 'Jul (-1)', 'Aug (-1)', 'Sep (-1)',\n",
    "                 'Oct (-1)', 'Nov (-1)', 'Dec (-1)',\n",
    "                 'Jan (1)', 'Feb (1)', 'Mar (1)', \n",
    "                 'Apr (1)', 'May (1)', 'Jun (1)', \n",
    "                 'Jul (1)', 'Aug (1)', 'Sep (1)',\n",
    "                 'Oct (1)', 'Nov (1)', 'Dec (1)']\n",
    "    \n",
    "    dry_season_start = np.empty(np.shape(lon))\n",
    "                \n",
    "    axs[0].contourf(lon,time,field_1, levels = bounds, cmap=cmap, extend='both')\n",
    "    axs[0].set_yticks(np.arange(0,24,2))\n",
    "    axs[0].set_yticklabels(time_labs[0::2])\n",
    "    #axs[0].contour(lon,time,sst_avg, levels = bound_0, linewidths=0.9, colors='k')\n",
    "    im = axs[1].contourf(lon,time,field_2, levels = bounds, cmap=cmap, extend='both')\n",
    "    #axs[1].yticks(time,time_labs, rotation = 'horizontal')\n",
    "    axs[1].set_yticks(np.arange(0,24,2))\n",
    "    axs[1].set_yticklabels(time_labs[0::2])\n",
    "    axs[1].set_xlabel('longitude')\n",
    "\n",
    "    left = subplotparams['right'] + 0.02\n",
    "    bottom = subplotparams['bottom'] + 0.05\n",
    "    width = 0.015\n",
    "    height = subplotparams['top'] - subplotparams['bottom'] - 0.1\n",
    "\n",
    "    cax = fig.add_axes([left, bottom, width, height])\n",
    "    cb = plt.colorbar(im, cax=cax)\n",
    "\n",
    "\n",
    "    if field_name == 'precip':\n",
    "        cb.set_label('Precip Anomaly mm/day')\n",
    "    elif field_name == 'sst':\n",
    "        cb.set_label('SST Anomaly C')\n",
    "    elif field_name == 'vort':\n",
    "        cb.set_label('Voticity Anomaly 1/s')\n",
    "    elif field_name == 'uwnd':\n",
    "        cb.set_label('Zonal Wind Anomaly m/s')\n",
    "    elif field_name == 'hgt':\n",
    "        cb.set_label('Geopotential Height Anomaly m')   \n",
    "    elif field_name == 'hgt_2':\n",
    "        cb.set_label('Geopotential Height Anomaly m')   \n",
    "        \n",
    "    plt.suptitle(field_name + ' Avg between '+ str(s_lat)+' and ' + str(n_lat)+ ' LAT holmover plot for Dry Comp (top) and Non Dry Comp (Bottom)')\n",
    "        \n",
    "    if save == 'yes':\n",
    "        filename = field_name+'_holmover'\n",
    "        fig.savefig(filename+\".pdf\")\n",
    "        fig.savefig(filename+\".png\")\n",
    "        #fig.savefig(\"VERY_DRY_COMPOSITE_SST_850HGT_WND_seasonal_anomaly_%04d-%02d.pdf\" % (year, mon))\n",
    "        plt.close()# no need for dpi\n",
    "        \n",
    "def holmover_plots_3(field_1,field_2, field_3, lon, time, field_name = 'sst',s_lat = -5, n_lat = 5, save ='no'):\n",
    "\n",
    "    subplotparams = dict(left=0.1, right=0.85,\n",
    "                         bottom=0.1, top=0.9,\n",
    "                         wspace=0.05, hspace=0.12)\n",
    "\n",
    "    fig, axs = plt.subplots(3, #sharex=True,\n",
    "                            figsize=(10, 7.8),\n",
    "                            gridspec_kw=subplotparams)\n",
    "\n",
    "\n",
    "    if field_name == 'precip':\n",
    "        #reverse the coolwarm palatte to make blue rainy and red dry\n",
    "        cmap = plt.get_cmap('BrBG')\n",
    "        bounds = [-2.5,-2, -1.5,-1.25, -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 2.5]\n",
    "            \n",
    "    elif field_name == 'sst':\n",
    "        cmap = cm.coolwarm\n",
    "        bounds = np.arange(-1,1.001,0.1)\n",
    "        bound_0 = [0]\n",
    "        \n",
    "    elif field_name == 'vort':\n",
    "        cmap = cm.jet\n",
    "        bounds = np.arange(-0.05,0.051,0.005)\n",
    "        bound_0 = [0]\n",
    "        \n",
    "    elif field_name == 'uwnd':\n",
    "        cmap = cm.jet\n",
    "        bounds = np.arange(-4,4.1,0.5)\n",
    "        bound_0 = [0]\n",
    "        \n",
    "    elif field_name == 'hgt':\n",
    "        cmap = cm.jet\n",
    "        bounds = np.arange(-20,20.1,2)\n",
    "        bound_0 = [0]\n",
    "    \n",
    "    elif field_name == 'hgt_2':\n",
    "        cmap = cm.jet\n",
    "        bounds = np.arange(-10,10.1,1)\n",
    "        bound_0 = [0]\n",
    "        \n",
    "    time_labs = ['Jan (-1)', 'Feb (-1)', 'Mar (-1)', \n",
    "                 'Apr (-1)', 'May (-1)', 'Jun (-1)', \n",
    "                 'Jul (-1)', 'Aug (-1)', 'Sep (-1)',\n",
    "                 'Oct (-1)', 'Nov (-1)', 'Dec (-1)',\n",
    "                 'Jan (1)', 'Feb (1)', 'Mar (1)', \n",
    "                 'Apr (1)', 'May (1)', 'Jun (1)', \n",
    "                 'Jul (1)', 'Aug (1)', 'Sep (1)',\n",
    "                 'Oct (1)', 'Nov (1)', 'Dec (1)']\n",
    "    \n",
    "    dry_season_start = np.empty(np.shape(lon))\n",
    "    dry_season_start[:] = time[11]\n",
    "    dry_season_end = np.empty(np.shape(lon))\n",
    "    dry_season_end[:] = time[17]\n",
    "                \n",
    "    axs[0].contourf(lon,time,field_1, levels = bounds, cmap=cmap, extend='both')\n",
    "    axs[0].plot(lon,dry_season_start,'k')\n",
    "    axs[0].plot(lon,dry_season_end,'k')\n",
    "    axs[0].set_yticks(np.arange(0,24,2))\n",
    "    axs[0].set_yticklabels(time_labs[0::2])\n",
    "    \n",
    "    axs[1].contourf(lon,time,field_2, levels = bounds, cmap=cmap, extend='both')\n",
    "    axs[1].plot(lon,dry_season_start,'k')\n",
    "    axs[1].plot(lon,dry_season_end,'k')\n",
    "    axs[1].set_yticks(np.arange(0,24,2))\n",
    "    axs[1].set_yticklabels(time_labs[0::2])\n",
    "    #axs[0].contour(lon,time,sst_avg, levels = bound_0, linewidths=0.9, colors='k')\n",
    "    im = axs[2].contourf(lon,time,field_3, levels = bounds, cmap=cmap, extend='both')\n",
    "    axs[2].plot(lon,dry_season_start,'k')\n",
    "    axs[2].plot(lon,dry_season_end,'k')\n",
    "    #axs[1].yticks(time,time_labs, rotation = 'horizontal')\n",
    "    axs[2].set_yticks(np.arange(0,24,2))\n",
    "    axs[2].set_yticklabels(time_labs[0::2])\n",
    "    axs[2].set_xlabel('longitude')\n",
    "\n",
    "    left = subplotparams['right'] + 0.02\n",
    "    bottom = subplotparams['bottom'] + 0.05\n",
    "    width = 0.015\n",
    "    height = subplotparams['top'] - subplotparams['bottom'] - 0.1\n",
    "\n",
    "    cax = fig.add_axes([left, bottom, width, height])\n",
    "    cb = plt.colorbar(im, cax=cax)\n",
    "\n",
    "\n",
    "    if field_name == 'precip':\n",
    "        cb.set_label('Precip Anomaly mm/day')\n",
    "    elif field_name == 'sst':\n",
    "        cb.set_label('SST Anomaly C')\n",
    "    elif field_name == 'vort':\n",
    "        cb.set_label('Voticity Anomaly 1/s')\n",
    "    elif field_name == 'uwnd':\n",
    "        cb.set_label('Zonal Wind Anomaly m/s')\n",
    "    elif field_name == 'hgt':\n",
    "        cb.set_label('Geopotential Height Anomaly m')   \n",
    "    elif field_name == 'hgt_2':\n",
    "        cb.set_label('Geopotential Height Anomaly m')   \n",
    "        \n",
    "    plt.suptitle(field_name + ' Avg between '+ str(s_lat)+' and ' + str(n_lat)+ ' LAT \\n holmover plot for Dry Comp (top), La Nina(mid), and Non Dry Comp (Bottom)')\n",
    "        \n",
    "    if save == 'yes':\n",
    "        filename = field_name+'_holmover'\n",
    "        fig.savefig(filename+\".pdf\")\n",
    "        fig.savefig(filename+\".png\")\n",
    "        #fig.savefig(\"VERY_DRY_COMPOSITE_SST_850HGT_WND_seasonal_anomaly_%04d-%02d.pdf\" % (year, mon))\n",
    "        plt.close()# no need for dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sst = holmover_plots_3(sst_avg, sst_avg_lanina, sst_avg_nondry, sst_lon, time, field_name = 'sst', s_lat = -6, n_lat = 6, save = 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sst = holmover_plots_3(sst_avg, sst_avg_lanina, sst_avg_nondry, sst_lon, time, field_name = 'sst', s_lat = -6, n_lat = 6, save = 'yes')\n",
    "vort = holmover_plots_3(vort_avg, vort_avg_lanina, vort_avg_nondry, ncep_lon, time, field_name = 'vort', s_lat = 10, n_lat = 20, save = 'yes')\n",
    "uwnd = holmover_plots_3(uwnd_avg, uwnd_avg_lanina, uwnd_avg_nondry, ncep_lon, time, field_name = 'uwnd', s_lat = -5, n_lat = 5, save = 'yes')\n",
    "precip = holmover_plots_3(precip_avg, precip_avg_lanina, precip_avg_nondry, precip_lon, time, field_name = 'precip', s_lat = 5, n_lat = 15, save = 'yes')\n",
    "hgt = holmover_plots_3(hgt_avg, hgt_avg_lanina, hgt_avg_nondry, ncep_lon, time, field_name = 'hgt', s_lat = 30, n_lat = 50, save = 'yes')\n",
    "hgt_2 = holmover_plots_3(hgt_2_avg, hgt_2_avg_lanina, hgt_2_avg_nondry, ncep_lon, time, field_name = 'hgt_2', s_lat = 10, n_lat = 20, save = 'yes')\n",
    "#hgt = holmover_plots(hgt_avg, hgt_avg_nondry, ncep_lon, time, field_name = 'hgt', s_lat = 20, n_lat = 60, save = 'yes')\n",
    "#uwnd = holmover_plots(uwnd_avg, uwnd_avg_nondry, ncep_lon, time, field_name = 'uwnd', save = 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hgt = holmover_plots(hgt_avg, hgt_avg_nondry, ncep_lon, time, field_name = 'hgt', s_lat = 30, n_lat = 50, save = 'yes')\n",
    "hgt_2 = holmover_plots(hgt_2_avg, hgt_2_avg_nondry, ncep_lon, time, field_name = 'hgt_2', s_lat = 10, n_lat = 20, save = 'yes')\n",
    "# plt.contourf(ncep_lon,time,hgt_avg)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(time)\n",
    "dry_season_start = np.empty(np.shape(sst_lon))\n",
    "dry_season_start[:] = time[11]\n",
    "print(dry_season_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_sl = rangeslice(precip_latitudes, (-5, 5.001))\n",
    "lon_sl = rangeslice(precip_longitudes, (100, 240.001))\n",
    "precip_avg = np.average(precip_composite_verydry[:,lat_sl,lon_sl], axis=1)\n",
    "precip_avg_nondry = np.average(precip_composite_nondry[:,lat_sl,lon_sl], axis=1)\n",
    "time = range(24)\n",
    "lon = precip_longitudes[lon_sl]\n",
    "\n",
    "levels = np.arange(-5,5.001,0.1)\n",
    "\n",
    "fig, axarr= plt.subplots(2,sharex = True)\n",
    "axarr[0].contourf(lon,time,precip_avg, levels = levels)\n",
    "axarr[1].contourf(lon,time,precip_avg_nondry, levels = levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_sl = rangeslice(ncep_latitudes, (10, 20.001))\n",
    "lon_sl = rangeslice(ncep_longitudes, (100, 240.001))\n",
    "vort_avg = np.average(vorticity_verydry[:,lat_sl,lon_sl], axis=1)\n",
    "vort_avg_nondry = np.average(vorticity_nondry[:,lat_sl,lon_sl], axis=1)\n",
    "time = range(24)\n",
    "lon = precip_longitudes[lon_sl]\n",
    "\n",
    "#levels = np.arange(-0.0002,0.0002,0.000001)\n",
    "\n",
    "fig, axarr= plt.subplots(2,sharex = True)\n",
    "# axarr[0].contourf(lon,time,vort_avg, levels = levels)\n",
    "# axarr[1].contourf(lon,time,vort_avg_nondry, levels = levels)\n",
    "\n",
    "axarr[0].contourf(lon,time,vort_avg)\n",
    "axarr[1].contourf(lon,time,vort_avg_nondry)\n",
    "axarr[1].colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here I define the plotting function and plot the figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotting_function(color_field, contour_field, u_wnd, v_wnd, member_counter,  \n",
    "                      color_field_name, case_name, target_dir, *argv, save = 'yes', field = 'sst'):    \n",
    "    \n",
    "    #here basemap is producing a strange behaviour\n",
    "    m = Basemap(projection='merc', llcrnrlat=-50.1, urcrnrlat=50.01,\n",
    "                        llcrnrlon=80, urcrnrlon=300, lat_ts=0, resolution='c')\n",
    "\n",
    "    with open('mapcache.pk', mode='wb') as f:\n",
    "        pickle.dump(m, f)\n",
    "\n",
    "    ## Tweak the subplot specifications.  \n",
    "\n",
    "    subplotparams = dict(left=0.03, right=0.88,\n",
    "                         bottom=0.015, top=0.94,\n",
    "                         wspace=0.05, hspace=0.001)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(3,3, #sharex=True,\n",
    "                        figsize=(13, 7.8),\n",
    "                        gridspec_kw=subplotparams,\n",
    "                       )\n",
    "\n",
    "    mm = range(9)\n",
    "\n",
    "    for mon, pax in zip(mm, axs.flat):\n",
    "\n",
    "        with open('mapcache.pk', 'rb') as f:\n",
    "            m = pickle.load(f)\n",
    "        m.ax = pax\n",
    "        \n",
    "        if field == 'precip':\n",
    "            #reverse the coolwarm palatte to make blue rainy and red dry\n",
    "            cmap = plt.get_cmap('BrBG')\n",
    "            bounds = [-2.5,-2, -1.5,-1.25, -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 2.5]\n",
    "\n",
    "\n",
    "            x, y = m(*np.meshgrid(PREC_precip_longitudes, PREC_precip_latitudes))\n",
    "            im = m.contourf(x, y, color_field[mon,:,:], levels=bounds, cmap=cmap,\n",
    "                            extend='both')\n",
    "\n",
    "        elif field == 'sst':\n",
    "                #reverse the coolwarm palatte to make blue rainy and red dry\n",
    "                cmap = cm.coolwarm\n",
    "                bounds = [-2, -1.5,-1.25, -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2]\n",
    "\n",
    "                x, y = m(*np.meshgrid(ersst_longitudes, ersst_latitudes))\n",
    "                im = m.contourf(x, y, color_field[mon,:,:], levels=bounds, cmap=cmap,\n",
    "                                extend='both')\n",
    "        #------------------------------------------------\n",
    "\n",
    "        x_hgt, y_hgt = m(*np.meshgrid(ncep_longitudes, ncep_latitudes))\n",
    "        #hgt_bounds = np.arange(-40,40,1)\n",
    "        hgt_bounds = [-80, -70,-60, -50, -40, \n",
    "                      -30, -20, -10,-8, -6, \n",
    "                      -4, -2, -1,0, 1, 2, 4, \n",
    "                      6, 8, 10, 20, 30,\n",
    "                      40, 50, 60, 70, 80]\n",
    "        hgt_bound_0 = [0]\n",
    "        im_hgt = m.contour(x_hgt, y_hgt, contour_field[mon,:,:], levels=hgt_bounds,\n",
    "                           linewidths=0.5, colors='k')\n",
    "        im_hgt = m.contour(x_hgt, y_hgt, contour_field[mon,:,:], levels=hgt_bound_0,\n",
    "                       linewidths=0.9, colors='k')\n",
    "\n",
    "        # transform vectors to projection grid.\n",
    "        uproj, vproj, xx, yy = m.rotate_vector(u_wnd[mon,:,:], \n",
    "                                               v_wnd[mon,:,:], \n",
    "                                               ncep_longitudes, \n",
    "                                               ncep_latitudes,\n",
    "                                               returnxy=True)\n",
    "        # now plot every other vector\n",
    "        Q = m.quiver(xx[::2,::2], yy[::2,::2], \n",
    "                     uproj[::2,::2], vproj[::2,::2],\n",
    "                     scale=20, scale_units='inches')\n",
    "\n",
    "        m.drawcoastlines()\n",
    "        \n",
    "        if box_coords in argv:\n",
    "            x1,y1 = m(box_coords[2], box_coords[1])\n",
    "            x2,y2 = m(box_coords[2], box_coords[0])\n",
    "            x3,y3 = m(box_coords[3], box_coords[0])\n",
    "            x4,y4 = m(box_coords[3], box_coords[1])\n",
    "            \n",
    "            p = Polygon([(x1,y1), (x2,y2), (x3,y3), (x4,y4)], \n",
    "                        edgecolor='b', facecolor = 'none', linewidth = 2)\n",
    "            m.ax.add_patch(p)\n",
    "\n",
    "        \n",
    "        parallels = np.arange(-90, 90, 30)\n",
    "        meridians = np.arange(-180, 180, 60)\n",
    "        if pax in axs.flat[::3]:\n",
    "            m.drawparallels(parallels, labels = [1, 0, 0, 1], fontsize=8)\n",
    "        else:\n",
    "            m.drawparallels(parallels, labels = [0, 0, 0, 0], fontsize=8)\n",
    "\n",
    "        if pax in axs.flat[:6]:\n",
    "            m.drawmeridians(meridians, labels = [0, 0, 0, 0], fontsize=8)\n",
    "        if pax in axs.flat[6:]:\n",
    "            m.drawmeridians(meridians, labels = [1, 0, 0, 1], fontsize=8)\n",
    "\n",
    "        #m.drawparallels(parallels, labels = [1, 0, 0, 1], fontsize=8)\n",
    "        #m.drawmeridians(meridians, labels = [1, 0, 0, 1], fontsize=8)\n",
    "\n",
    "        #pax.set_title(color_field_name+\" Seasonal Anom \" +season_list[mon])\n",
    "        pax.set_title(season_list[mon])\n",
    "\n",
    "    #cax, kw = mpl.colorbar.make_axes([ax for ax in axs.flat])\n",
    "    ##  This method of making an Axes for the cbar interacts very badly with\n",
    "    ##  basemap, so just make one manually.  This also gives more control, so\n",
    "    ##  it looks nicer.\n",
    "    left = subplotparams['right'] + 0.02\n",
    "    bottom = subplotparams['bottom'] + 0.05\n",
    "    width = 0.015\n",
    "    height = subplotparams['top'] - subplotparams['bottom'] - 0.1\n",
    "\n",
    "    cax = fig.add_axes([left, bottom, width, height])\n",
    "    cb = plt.colorbar(im, cax=cax)\n",
    "    \n",
    "    if field == 'precip':\n",
    "        cb.set_label('Precip Anomaly mm/day')\n",
    "    elif field == 'sst':\n",
    "        cb.set_label('SST Anomaly C')\n",
    "    \n",
    "\n",
    "    ## EF: The quiverkey needs to be made using an axes in which a quiver is\n",
    "    ##     drawn so that it has the right transform information.  Otherwise\n",
    "    ##     it won't get the length right.\n",
    "    qkx = left + (1 - left) / 4\n",
    "    qky = bottom + height + 0.025\n",
    "    qk = pax.quiverkey(Q, qkx, qky, 10, '10 m/s', \n",
    "                       coordinates='figure',\n",
    "                       labelpos='N',\n",
    "                       labelsep=0.07, \n",
    "                       fontproperties=dict(size='small'),\n",
    "                      )\n",
    "    \n",
    "    if field == 'precip':\n",
    "        plt.suptitle(case_name+' '+color_field_name+' Precip 850HGT WND seasonal anomaly')\n",
    "    elif field == 'sst':\n",
    "        plt.suptitle(case_name+' '+color_field_name+' SST 850HGT WND seasonal anomaly')\n",
    "    \n",
    "    \n",
    "    if save == 'yes':\n",
    "        filename = target_dir+str(case_name)+\"_\"+color_field_name+field+'_850HGT_WND_seasonal_anomaly'\n",
    "        fig.savefig(filename+\".pdf\")\n",
    "        fig.savefig(filename+\".png\")\n",
    "        #fig.savefig(\"VERY_DRY_COMPOSITE_SST_850HGT_WND_seasonal_anomaly_%04d-%02d.pdf\" % (year, mon))\n",
    "        plt.close()# no need for dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "box_coords =[]\n",
    "season_list = ['Oct Jan Apr' , 'Nov Feb May' , 'Dec Mar Jun',\n",
    "               'Jan Apr Jul' , 'Feb May Aug' , 'Mar Jun Sep',\n",
    "               'Apr Jul Oct' , 'May Aug Nov' , 'Jun Sep Dec',\n",
    "               'Jul Oct Jan' , 'Aug Nov Feb' , 'Sep Dec Mar']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotting_function(precip_composite_verydry, hgt_composite_verydry, \n",
    "                  uwnd_composite_verydry, vwnd_composite_verydry, \n",
    "                  gpcp_season_members_verydry, 'GPCP', 'Very_Dry', './' ,save = 'yes', field = 'precip')\n",
    "\n",
    "plotting_function(sst_composite_verydry, hgt_composite_verydry, \n",
    "                  uwnd_composite_verydry, vwnd_composite_verydry, \n",
    "                  gpcp_season_members_verydry, 'ERSST', 'Very_Dry', './' ,save = 'yes', field = 'sst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotting_function(precip_composite_nondry, hgt_composite_nondry, \n",
    "                  uwnd_composite_nondry, vwnd_composite_nondry, \n",
    "                  gpcp_season_members_nondry, 'GPCP', 'Non_Dry', './' ,save = 'yes', field = 'precip')\n",
    "\n",
    "plotting_function(sst_composite_nondry, hgt_composite_nondry, \n",
    "                  uwnd_composite_nondry, vwnd_composite_nondry, \n",
    "                  gpcp_season_members_nondry, 'ERSST', 'Non_Dry', './' ,save = 'yes', field = 'sst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_dir = \"./composite_member_maps/\"\n",
    "\n",
    "for year in year_list:\n",
    "\n",
    "        sst_selection = ((sst_ca.ymdhms[:, 0] == year))\n",
    "        sst_selection = np.nonzero(sst_selection)[0]\n",
    "        sst_colorfield= sst_seasonal_anom.s_anom[sst_selection]\n",
    "\n",
    "        precip_selection = ((precip_ca.ymdhms[:, 0] == year))\n",
    "        precip_selection = np.nonzero(precip_selection)[0]\n",
    "        precip_color_field= precip_seasonal_anom.s_anom[precip_selection]\n",
    "\n",
    "        selection = ((uwnd_ca.ymdhms[:, 0] == year))\n",
    "        selection = np.nonzero(selection)[0]\n",
    "        uwnd= uwnd_seasonal_anom.s_anom[selection]\n",
    "        vwnd = vwnd_seasonal_anom.s_anom[selection]\n",
    "        hgt_field = hgt_seasonal_anom.s_anom[selection]\n",
    "\n",
    "        plotting_function(precip_color_field, hgt_field, \n",
    "                          uwnd, vwnd, \n",
    "                          gpcp_season_members_verydry, \n",
    "                         'GPCP', str(year), target_dir ,save = 'yes', field = 'precip')\n",
    "\n",
    "        plotting_function(sst_colorfield, hgt_field, \n",
    "                          uwnd, vwnd, \n",
    "                          gpcp_season_members_verydry, \n",
    "                          'ERSST', str(year), target_dir ,save = 'yes', field = 'sst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box_coords =[]\n",
    "\n",
    "year_list = [1978, 1984, 1988, 1996, 1999, 2000, 2001, 2008, 2010, 2013]\n",
    "\n",
    "season_list = ['DJF' , 'JFM' , 'FMA',\n",
    "               'MAM' , 'AMJ' , 'MJJ',\n",
    "               'JJA' , 'JAS' , 'ASO',\n",
    "               'SON' , 'OND' , 'NDJ']\n",
    "\n",
    "target_dir = \"./composite_member_maps/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(sst_seasonal_anom.s_anom.shape)\n",
    "#print(sst_seasonal_anom2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.ma.allclose(sst_seasonal_anom.s_anom, sst_seasonal_anom2, masked_equal = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(precip_seasonal_anom.s_anom[-1])\n",
    "\n",
    "#print('--------------------------------')\n",
    "\n",
    "#print(uwnd_seasonal_anom.s_anom[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comp_plotting_function(color_field, contour_field, u_wnd, v_wnd, \n",
    "                           dry_member_counter, wet_member_counter, year_list, \n",
    "                           color_field_name, case_name, target_dir, box_coords =[], save = 'yes', field = 'sst'):    \n",
    "    \n",
    "    #here basemap is producing a strange behaviour\n",
    "    m = Basemap(projection='merc', llcrnrlat=-50.1, urcrnrlat=50.01,\n",
    "                        llcrnrlon=80, urcrnrlon=300, lat_ts=0, resolution='c')\n",
    "\n",
    "    with open('mapcache.pk', mode='wb') as f:\n",
    "        pickle.dump(m, f)\n",
    "\n",
    "    ## Tweak the subplot specifications.  \n",
    "\n",
    "    subplotparams = dict(left=0.03, right=0.88,\n",
    "                         bottom=0.015, top=0.94,\n",
    "                         wspace=0.05, hspace=0.001)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(3,3, #sharex=True,\n",
    "                        figsize=(13, 7.8),\n",
    "                        gridspec_kw=subplotparams,\n",
    "                       )\n",
    "\n",
    "    \n",
    "    mm = 1\n",
    "\n",
    "    for year, pax in zip(year_list, axs.flat):\n",
    "\n",
    "        with open('mapcache.pk', 'rb') as f:\n",
    "            m = pickle.load(f)\n",
    "        m.ax = pax\n",
    "        \n",
    "        \n",
    "        sst_selection = ((sst_ca.ymdhms[:, 0] == year))\n",
    "        sst_selection = np.nonzero(sst_selection)[0]\n",
    "        sst_colorfield= sst_seasonal_anom.s_anom[sst_selection]\n",
    "\n",
    "        PREC_precip_selection = ((PREC_precip_ca.ymdhms[:, 0] == year))\n",
    "        PREC_precip_selection = np.nonzero(PREC_precip_selection)[0]\n",
    "        PREC_precip_color_field = PREC_precip_seasonal_anom.s_anom[PREC_precip_selection]\n",
    "\n",
    "        selection = ((uwnd_ca.ymdhms[:, 0] == year))\n",
    "        selection = np.nonzero(selection)[0]\n",
    "        uwnd= uwnd_seasonal_anom.s_anom[selection]\n",
    "        vwnd = vwnd_seasonal_anom.s_anom[selection]\n",
    "        hgt_field = hgt_seasonal_anom.s_anom[selection]\n",
    "        \n",
    "        if field == 'precip':\n",
    "            #reverse the coolwarm palatte to make blue rainy and red dry\n",
    "            cmap = plt.get_cmap('BrBG')\n",
    "            bounds = [-2.5,-2, -1.5,-1.25, -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 2.5]\n",
    "\n",
    "\n",
    "            x, y = m(*np.meshgrid(PREC_precip_longitudes, PREC_precip_latitudes))\n",
    "            im = m.contourf(x, y, color_field[mon,:,:], levels=bounds, cmap=cmap,\n",
    "                            extend='both')\n",
    "\n",
    "        elif field == 'sst':\n",
    "                #reverse the coolwarm palatte to make blue rainy and red dry\n",
    "                cmap = cm.coolwarm\n",
    "                bounds = [-2, -1.5,-1.25, -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2]\n",
    "\n",
    "                x, y = m(*np.meshgrid(ersst_longitudes, ersst_latitudes))\n",
    "                im = m.contourf(x, y, color_field[mon,:,:], levels=bounds, cmap=cmap,\n",
    "                                extend='both')\n",
    "        #------------------------------------------------\n",
    "\n",
    "        x_hgt, y_hgt = m(*np.meshgrid(ncep_longitudes, ncep_latitudes))\n",
    "        #hgt_bounds = np.arange(-40,40,1)\n",
    "        hgt_bounds = [-80, -70,-60, -50, -40, \n",
    "                      -30, -20, -10,-8, -6, \n",
    "                      -4, -2, -1,0, 1, 2, 4, \n",
    "                      6, 8, 10, 20, 30,\n",
    "                      40, 50, 60, 70, 80]\n",
    "        hgt_bound_0 = [0]\n",
    "        im_hgt = m.contour(x_hgt, y_hgt, contour_field[mon,:,:], levels=hgt_bounds,\n",
    "                           linewidths=0.5, colors='k')\n",
    "        im_hgt = m.contour(x_hgt, y_hgt, contour_field[mon,:,:], levels=hgt_bound_0,\n",
    "                       linewidths=0.9, colors='k')\n",
    "\n",
    "        # transform vectors to projection grid.\n",
    "        uproj, vproj, xx, yy = m.rotate_vector(u_wnd[mon,:,:], \n",
    "                                               v_wnd[mon,:,:], \n",
    "                                               ncep_longitudes, \n",
    "                                               ncep_latitudes,\n",
    "                                               returnxy=True)\n",
    "        # now plot every other vector\n",
    "        Q = m.quiver(xx[::2,::2], yy[::2,::2], \n",
    "                     uproj[::2,::2], vproj[::2,::2],\n",
    "                     scale=20, scale_units='inches')\n",
    "\n",
    "        m.drawcoastlines()\n",
    "        \n",
    "        if box_coords in argv:\n",
    "            x1,y1 = m(box_coords[2], box_coords[1])\n",
    "            x2,y2 = m(box_coords[2], box_coords[0])\n",
    "            x3,y3 = m(box_coords[3], box_coords[0])\n",
    "            x4,y4 = m(box_coords[3], box_coords[1])\n",
    "            \n",
    "            p = Polygon([(x1,y1), (x2,y2), (x3,y3), (x4,y4)], \n",
    "                        edgecolor='b', facecolor = 'none', linewidth = 2)\n",
    "            m.ax.add_patch(p)\n",
    "\n",
    "        \n",
    "        parallels = np.arange(-90, 90, 30)\n",
    "        meridians = np.arange(-180, 180, 60)\n",
    "        if pax in axs.flat[::3]:\n",
    "            m.drawparallels(parallels, labels = [1, 0, 0, 1], fontsize=8)\n",
    "        else:\n",
    "            m.drawparallels(parallels, labels = [0, 0, 0, 0], fontsize=8)\n",
    "\n",
    "        if pax in axs.flat[:6]:\n",
    "            m.drawmeridians(meridians, labels = [0, 0, 0, 0], fontsize=8)\n",
    "        if pax in axs.flat[6:]:\n",
    "            m.drawmeridians(meridians, labels = [1, 0, 0, 1], fontsize=8)\n",
    "\n",
    "        #m.drawparallels(parallels, labels = [1, 0, 0, 1], fontsize=8)\n",
    "        #m.drawmeridians(meridians, labels = [1, 0, 0, 1], fontsize=8)\n",
    "\n",
    "        #pax.set_title(color_field_name+\" Seasonal Anom \" +season_list[mon])\n",
    "        pax.set_title(season_list[mon])\n",
    "\n",
    "    #cax, kw = mpl.colorbar.make_axes([ax for ax in axs.flat])\n",
    "    ##  This method of making an Axes for the cbar interacts very badly with\n",
    "    ##  basemap, so just make one manually.  This also gives more control, so\n",
    "    ##  it looks nicer.\n",
    "    left = subplotparams['right'] + 0.02\n",
    "    bottom = subplotparams['bottom'] + 0.05\n",
    "    width = 0.015\n",
    "    height = subplotparams['top'] - subplotparams['bottom'] - 0.1\n",
    "\n",
    "    cax = fig.add_axes([left, bottom, width, height])\n",
    "    cb = plt.colorbar(im, cax=cax)\n",
    "    \n",
    "    if field == 'precip':\n",
    "        cb.set_label('Precip Anomaly mm/day')\n",
    "    elif field == 'sst':\n",
    "        cb.set_label('SST Anomaly C')\n",
    "    \n",
    "\n",
    "    ## EF: The quiverkey needs to be made using an axes in which a quiver is\n",
    "    ##     drawn so that it has the right transform information.  Otherwise\n",
    "    ##     it won't get the length right.\n",
    "    qkx = left + (1 - left) / 4\n",
    "    qky = bottom + height + 0.025\n",
    "    qk = pax.quiverkey(Q, qkx, qky, 10, '10 m/s', \n",
    "                       coordinates='figure',\n",
    "                       labelpos='N',\n",
    "                       labelsep=0.07, \n",
    "                       fontproperties=dict(size='small'),\n",
    "                      )\n",
    "    \n",
    "    if field == 'precip':\n",
    "        plt.suptitle(case_name+' '+color_field_name+' Precip 850HGT WND seasonal anomaly')\n",
    "    elif field == 'sst':\n",
    "        plt.suptitle(case_name+' '+color_field_name+' Precip 850HGT WND seasonal anomaly')\n",
    "    \n",
    "    \n",
    "    if save == 'yes':\n",
    "\n",
    "        fig.savefig(target_dir+str(case_name)+\"_\"+color_field_name+\"_PRECIP_850HGT_WND_seasonal_anomaly.pdf\")\n",
    "        fig.savefig(target_dir+str(case_name)+\"_\"+color_field_name+\"_PRECIP_850HGT_WND_seasonal_anomaly.png\")\n",
    "        #fig.savefig(\"VERY_DRY_COMPOSITE_SST_850HGT_WND_seasonal_anomaly_%04d-%02d.pdf\" % (year, mon))\n",
    "        plt.close()# no need for dpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainfall anomaly pattern correlation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_sl = rangeslice(precip_latitudes, (-5, 15.001))\n",
    "\n",
    "#lon goes from 0 to 360\n",
    "lon_sl = rangeslice(precip_longitudes, (120, 240.001))\n",
    "\n",
    "start_year = 1979\n",
    "end_year = 2015\n",
    "\n",
    "#Calculate the total number of pattern correlations possible\n",
    "total_year_number = end_year - start_year +1\n",
    "\n",
    "total_dims = comb(total_year_number,2, exact = True)\n",
    "\n",
    "\n",
    "c_precip = np.empty((total_dims,12))\n",
    "\n",
    "x=0\n",
    "for y in range(start_year, end_year):\n",
    "    #print(y)\n",
    "    for i in range(1,end_year - y+1):\n",
    "        #print(y+i)\n",
    "        for m in range(12):\n",
    "            #\n",
    "            #t1 = np.nonzero(dat.dday_1800 == to_day(1800, y, m+1))[0][0]\n",
    "            t1 = ((precip_ca.ymdhms[:, 0] == y) & (precip_ca.ymdhms[:, 1] == m+1))\n",
    "            #t2 = np.nonzero(dat.dday_1800 == to_day(1800, y+i, m+1))[0][0]\n",
    "            t2 = ((precip_ca.ymdhms[:, 0] == y+i) & (precip_ca.ymdhms[:, 1] == m+1))\n",
    "                \n",
    "            #\n",
    "            uav_1 = precip_seasonal_anom.s_anom[t1, lat_sl, lon_sl]\n",
    "            uav_2 = precip_seasonal_anom.s_anom[t2, lat_sl, lon_sl]\n",
    "            \n",
    "            y1 = uav_1.compressed()\n",
    "            y2 = uav_2.compressed()\n",
    "            try:\n",
    "                c_precip[x,m] = np.corrcoef(y1, y2)[0, 1]\n",
    "            except ValueError:\n",
    "                c_precip[x,m] = np.nan\n",
    "            \n",
    "            if c_precip[x,m] > 1:\n",
    "                print(y)\n",
    "                print(y+i)\n",
    "        \n",
    "        x=x+1\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "PREC_start_year = 1966\n",
    "PREC_end_year = 2015\n",
    "\n",
    "#Calculate the total number of pattern correlations possible\n",
    "PREC_total_year_number = PREC_end_year - PREC_start_year +1\n",
    "\n",
    "PREC_total_dims = comb(PREC_total_year_number,2, exact = True)\n",
    "\n",
    "\n",
    "c_PREC_precip = np.empty((PREC_total_dims,12))\n",
    "\n",
    "x=0\n",
    "for y in range(PREC_start_year, PREC_end_year):\n",
    "    #print(y)\n",
    "    for i in range(1,PREC_end_year - y+1):\n",
    "        #print(y+i)\n",
    "        for m in range(12):\n",
    "            #\n",
    "            #t1 = np.nonzero(dat.dday_1800 == to_day(1800, y, m+1))[0][0]\n",
    "            t1 = ((PREC_precip_ca.ymdhms[:, 0] == y) & (PREC_precip_ca.ymdhms[:, 1] == m+1))\n",
    "            #t2 = np.nonzero(dat.dday_1800 == to_day(1800, y+i, m+1))[0][0]\n",
    "            t2 = ((PREC_precip_ca.ymdhms[:, 0] == y+i) & (PREC_precip_ca.ymdhms[:, 1] == m+1))\n",
    "                \n",
    "            #\n",
    "            uav_1 = PREC_precip_seasonal_anom.s_anom[t1, lat_sl, lon_sl]\n",
    "            uav_2 = PREC_precip_seasonal_anom.s_anom[t2, lat_sl, lon_sl]\n",
    "            \n",
    "            y1 = uav_1.compressed()\n",
    "            y2 = uav_2.compressed()\n",
    "            try:\n",
    "                c_PREC_precip[x,m] = np.corrcoef(y1, y2)[0, 1]\n",
    "            except ValueError:\n",
    "                c_PREC_precip[x,m] = np.nan\n",
    "            \n",
    "            if c_PREC_precip[x,m] > 1:\n",
    "                print(y)\n",
    "                print(y+i)\n",
    "        \n",
    "        x=x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pc90 = np.nanpercentile(c_precip, 90, axis=0)\n",
    "pc95 = np.nanpercentile(c_precip, 95, axis=0)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(pc90[:])\n",
    "print(pc95[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PREC_pc90 = np.nanpercentile(c_PREC_precip, 90, axis=0)\n",
    "PREC_pc95 = np.nanpercentile(c_PREC_precip, 95, axis=0)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(PREC_pc90[:])\n",
    "print(PREC_pc95[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the pattern correlation of each year to 2013\n",
    "total_year_number = end_year - start_year +1\n",
    "\n",
    "c_precip_2013 = np.empty((total_year_number,12))\n",
    "\n",
    "x=0\n",
    "for y in range(start_year, end_year+1):\n",
    "    #print(y+i)\n",
    "    for m in range(12):\n",
    "        #\n",
    "        #t1 = np.nonzero(dat.dday_1800 == to_day(1800, y, m+1))[0][0]\n",
    "        t1 = ((precip_ca.ymdhms[:, 0] == y) & (precip_ca.ymdhms[:, 1] == m+1))\n",
    "        #t2 = np.nonzero(dat.dday_1800 == to_day(1800, y+i, m+1))[0][0]\n",
    "        t2 = ((precip_ca.ymdhms[:, 0] == 2013) & (precip_ca.ymdhms[:, 1] == m+1))\n",
    "\n",
    "        #\n",
    "        uav_1 = precip_seasonal_anom.s_anom[t1, lat_sl, lon_sl]\n",
    "        uav_2 = precip_seasonal_anom.s_anom[t2, lat_sl, lon_sl]\n",
    "\n",
    "\n",
    "        y1 = uav_1.compressed()\n",
    "        y2 = uav_2.compressed()\n",
    "        try:\n",
    "            c_precip_2013[x,m] = np.corrcoef(y1, y2)[0, 1]\n",
    "        except ValueError:\n",
    "            c_precip_2013[x,m] = np.nan\n",
    "\n",
    "        if c_precip_2013[x,m] > 1:\n",
    "            print(y)\n",
    "            print(y+i)\n",
    "\n",
    "    x=x+1\n",
    "    \n",
    "# Calculate the pattern correlation of each year to 2013\n",
    "PREC_total_year_number = PREC_end_year - PREC_start_year +1\n",
    "\n",
    "c_PREC_precip_2013 = np.empty((PREC_total_year_number,12))\n",
    "\n",
    "x=0\n",
    "for y in range(PREC_start_year, PREC_end_year+1):\n",
    "    #print(y+i)\n",
    "    for m in range(12):\n",
    "        #\n",
    "        #t1 = np.nonzero(dat.dday_1800 == to_day(1800, y, m+1))[0][0]\n",
    "        t1 = ((PREC_precip_ca.ymdhms[:, 0] == y) & (PREC_precip_ca.ymdhms[:, 1] == m+1))\n",
    "        #t2 = np.nonzero(dat.dday_1800 == to_day(1800, y+i, m+1))[0][0]\n",
    "        t2 = ((PREC_precip_ca.ymdhms[:, 0] == 2013) & (PREC_precip_ca.ymdhms[:, 1] == m+1))\n",
    "\n",
    "        #\n",
    "        uav_1 = PREC_precip_seasonal_anom.s_anom[t1, lat_sl, lon_sl]\n",
    "        uav_2 = PREC_precip_seasonal_anom.s_anom[t2, lat_sl, lon_sl]\n",
    "\n",
    "\n",
    "        y1 = uav_1.compressed()\n",
    "        y2 = uav_2.compressed()\n",
    "        try:\n",
    "            c_PREC_precip_2013[x,m] = np.corrcoef(y1, y2)[0, 1]\n",
    "        except ValueError:\n",
    "            c_PREC_precip_2013[x,m] = np.nan\n",
    "\n",
    "        if c_PREC_precip_2013[x,m] > 1:\n",
    "            print(y)\n",
    "            print(y+i)\n",
    "\n",
    "    x=x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(c_precip_2013)\n",
    "#print(uav_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(c_PREC_precip_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpcp_pat_cor_90_test_lev = c_precip_2013 > pc90\n",
    "gpcp_pat_cor_95_test_lev = c_precip_2013 > pc95\n",
    "\n",
    "PREC_pat_cor_90_test_lev = c_PREC_precip_2013 > PREC_pc90\n",
    "PREC_pat_cor_95_test_lev = c_PREC_precip_2013 > PREC_pc95\n",
    "\n",
    "\n",
    "pat_cor_90_test_lev = np.concatenate((PREC_pat_cor_90_test_lev[:13,:], gpcp_pat_cor_90_test_lev), axis = 0)\n",
    "pat_cor_95_test_lev = np.concatenate((PREC_pat_cor_95_test_lev[:13,:], gpcp_pat_cor_95_test_lev), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(pat_cor_90_test_lev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we read and process the time series data from the excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peac_station_rain = pd.ExcelFile(datadir +'PEAC_station_rainfall_database.xlsx')\n",
    "print(peac_station_rain.sheet_names)\n",
    "\n",
    "ONI_file = pd.ExcelFile(datadir +'CPC_ONI.xlsx')\n",
    "print(ONI_file.sheet_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_USAPI_data(file_name, island_name):\n",
    "    \n",
    "    #Here we read the data into python from the excel file\n",
    "    raw_data= pd.read_excel(file_name, sheetname = island_name, \n",
    "                            skiprows = 1, parse_cols = \"A:O\")\n",
    "    \n",
    "    #\n",
    "    raw_matrix = raw_data.as_matrix(columns=None)\n",
    "    print(type(raw_matrix), raw_matrix.dtype)\n",
    "    \n",
    "    years =  raw_matrix[:51,1]\n",
    "    station_id = raw_matrix[0,0]\n",
    "    rainfall = raw_matrix[:51,3:] * 0.1\n",
    "    \n",
    "    rainfall[rainfall == \"'9999\"] = np.nan\n",
    "    rainfall[rainfall == \"nan\"] = np.nan\n",
    "    # We have to convert from an object array to floating point.\n",
    "    rainfall = np.ma.masked_invalid(rainfall.astype(float))\n",
    "    print('rainfall: ', rainfall.dtype, rainfall[5, 5])\n",
    " \n",
    "    #Initiate the ymdhms array as an array of int values filler with zeros\n",
    "    #that has the length og the total number of time steps years*months\n",
    "    ymdhms = np.zeros([51*12,6],dtype = np.int)\n",
    "\n",
    "    #here we will the first column with the year values from the excel sheet. \n",
    "    #repeated each one 12 consecutive times\n",
    "    ymdhms[:,0] = np.repeat(years,12)\n",
    "\n",
    "    #Here we create a list of the month indices\n",
    "    m = np.arange(1,13)\n",
    "    #we tile the list so that the entire list 1..12 repeats as many times as the number of years\n",
    "    mm = np.tile(m,51)\n",
    "\n",
    "    ymdhms[:,1] = mm\n",
    "\n",
    "    #the day column is filled with 15\n",
    "    ymdhms[:,2] = 15\n",
    "    \n",
    "    dday = to_day(1800, ymdhms)\n",
    "    mpldays = dday_to_mpl(1800, dday)\n",
    "    mpldaysformated = mpl.dates.num2date(mpldays)\n",
    "\n",
    "    out = Bunch(island_name = island_name, station_id = station_id, rainfall=rainfall,\n",
    "               ymdhms = ymdhms, mpldaysformated = mpldaysformated)\n",
    "    \n",
    "    return out\n",
    "    #return station_id, rainfall, ymdhms, mpldaysformated\n",
    "    \n",
    "def read_index_data(file_name, index_name):\n",
    "    \n",
    "    #Here we read the data into python from the excel file\n",
    "    raw_data= pd.read_excel(file_name, sheetname = index_name, skiprows = 1, parse_cols = \"A:M\")\n",
    "    \n",
    "    #\n",
    "    raw_matrix = raw_data.as_matrix(columns=None)\n",
    "    print(type(raw_matrix), raw_matrix.dtype)\n",
    "    \n",
    "    years =  raw_matrix[:,0]\n",
    "    index = raw_matrix[:,1:]\n",
    "    \n",
    "\n",
    "    # We have to convert from an object array to floating point.\n",
    "    index = np.ma.masked_invalid(index.astype(float))\n",
    "    print('index: ', index.dtype, index[5, 5])\n",
    " \n",
    "    #Initiate the ymdhms array as an array of int values filler with zeros\n",
    "    #that has the length og the total number of time steps years*months\n",
    "    ymdhms = np.zeros([67*12,6],dtype = np.int)\n",
    "\n",
    "    #here we will the first column with the year values from the excel sheet. \n",
    "    #repeated each one 12 consecutive times\n",
    "    ymdhms[:,0] = np.repeat(years,12)\n",
    "\n",
    "    #Here we create a list of the month indices\n",
    "    m = np.arange(1,13)\n",
    "    #we tile the list so that the entire list 1..12 repeats as many times as the number of years\n",
    "    mm = np.tile(m,67)\n",
    "\n",
    "    ymdhms[:,1] = mm\n",
    "\n",
    "    #the day column is filled with 15\n",
    "    ymdhms[:,2] = 15\n",
    "    \n",
    "    dday = to_day(1800, ymdhms)\n",
    "    mpldays = dday_to_mpl(1800, dday)\n",
    "    mpldaysformated = mpl.dates.num2date(mpldays)\n",
    "\n",
    "    out = Bunch(index_name = index_name, index = index,\n",
    "               ymdhms = ymdhms, mpldaysformated = mpldaysformated)\n",
    "    \n",
    "    return out\n",
    "    #return station_id, rainfall, ymdhms, mpldaysformated\n",
    "\n",
    "def seasonal_anomaly_old(m_anom):\n",
    "    s_anom = np.ma.zeros(m_anom.shape, float)\n",
    "    s_anom[1:-1] = (m_anom[:-2] + m_anom[1:-1] + m_anom[2:]) / 3\n",
    "    s_anom[0] = (m_anom[0] + m_anom[1]) / 2\n",
    "    s_anom[-1] = (m_anom[-2] + m_anom[-1]) / 2\n",
    "    return s_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "station_name_list = [\"Koror\", \"Yap\", \"Guam\", \"Chuuk\", \"Pohnpei\", \"Kwajalein\", \"Majuro\"]\n",
    "#variable_name_list = [koror, yap, guam, chuuk, phonpei, kwajalein, majuro]\n",
    "\n",
    "stations = Bunch()\n",
    "for name in station_name_list:\n",
    "    stations[name] = read_USAPI_data(peac_station_rain, name)\n",
    "    \n",
    "#print(stations.Kwajalein.ymdhms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for raindata in stations.values():\n",
    "    print(raindata.island_name)\n",
    "    print(np.shape(raindata.rainfall))\n",
    "    raindata.monmean = raindata.rainfall.mean(axis=0)\n",
    "    raindata.monstd = raindata.rainfall.std(axis=0)\n",
    "    raindata.monanom = raindata.rainfall - raindata.monmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(stations.Kwajalein.monanom))\n",
    "kwajalein_seasonal_anom = seasonal_anomaly_old(stations.Kwajalein.monanom.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oni = read_index_data(ONI_file, \"ONI\")\n",
    "\n",
    "print(np.shape(oni.ymdhms))\n",
    "print(np.shape(oni.mpldaysformated))\n",
    "print(np.shape(oni.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oni_selection = ((oni.ymdhms[:, 0] >= 1966))\n",
    "\n",
    "oni_time_series = oni.index.ravel()\n",
    "\n",
    "oni_period = oni_time_series[oni_selection]\n",
    "\n",
    "print(type(oni.ymdhms))\n",
    "print(type(oni.mpldaysformated))\n",
    "print(type(oni_selection))\n",
    "print(np.shape(oni_period))\n",
    "\n",
    "mpldaysformated_period = oni.mpldaysformated[-612:]\n",
    "print(len(mpldaysformated_period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(mpldaysformated_period, oni_period, 'r-')\n",
    "ax1.set_xlabel('year')\n",
    "# Make the y-axis label and tick labels match the line color.\n",
    "ax1.set_ylabel('ONI', color='r')\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(mpldaysformated_period, kwajalein_seasonal_anom, 'b-')\n",
    "ax2.set_ylabel('Kwajalein Seasonal Rainfall Anom mm/month', color='b')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stdev of the kwajalein rainfall\n",
    "Now we must calculate the stdev of the seasonal anomaly. The most straight forward way to calculate this may be unraveling the monthly anomaly into a series, using the seasonal_anomaly function and reshaping the series back into a matrix, then calculate the stdev of the columns of that matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here we restructure the seaqsonal anomaly time series into matices for easier logical selection\n",
    "kwajalein_seasonal_anom_matrix = kwajalein_seasonal_anom.reshape(51, 12)\n",
    "oni_period_matrix = oni_period.reshape(51,12)\n",
    "\n",
    "#Here we calculate the stdev of eac column of the matrix this is the stdev of seasonal rainfall anomaly for each season\n",
    "kwajalein_season_std = kwajalein_seasonal_anom_matrix.std(axis=0)\n",
    "#we repeat the row 51 times to have a matrix the same size as the data matrix and the oni matrix\n",
    "kwajalein_season_std_matrix = np.tile(kwajalein_season_std,(51,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplication of conditions for composite membership\n",
    "\n",
    "## first for the rainfall and ONI conditions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cond_1stdev = kwajalein_seasonal_anom_matrix <= -1* kwajalein_season_std_matrix\n",
    "cond_oni = oni_period_matrix <= -0.1\n",
    "cond_verydry_cool = np.logical_and(cond_1stdev, cond_oni)\n",
    "\n",
    "cond_05stdev = kwajalein_seasonal_anom_matrix <= -0.5* kwajalein_season_std_matrix\n",
    "cond_oni = oni_period_matrix <= -0.1\n",
    "cond_dry_cool = np.logical_and(cond_05stdev, cond_oni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nos for the pattern correlation conditions too\n",
    "\n",
    "### we use only the 90% test level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cond_verydry_cool_90patcor = np.logical_and.reduce((cond_1stdev[:-1,:], cond_oni[:-1,:], pat_cor_90_test_lev))\n",
    "#cond_dry_cool_90patcor = np.logical_and.reduce((cond_05stdev[:-1,:], cond_oni[:-1,:], pat_cor_90_test_lev))\n",
    "\n",
    "cond_verydry_cool_90patcor = np.logical_and(cond_verydry_cool[:-1,:], pat_cor_90_test_lev)\n",
    "cond_dry_cool_90patcor = np.logical_and(cond_dry_cool[:-1,:], pat_cor_90_test_lev)\n",
    "\n",
    "#cond_verydry_cool_90patcor = np.logical_and(cond_1stdev[:-1,:], cond_oni[:-1,:], pat_cor_90_test_lev)\n",
    "#cond_dry_cool_90patcor = np.logical_and(cond_05stdev[:-1,:], cond_oni[:-1,:], pat_cor_90_test_lev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Print the years selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#selection_matrix = cond_dry_cool\n",
    "\n",
    "#for i in range(len(selection_matrix.ravel())):\n",
    "#    if selection_matrix.ravel()[i]:\n",
    "#        print(stations.Kwajalein.ymdhms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selection_matrix = cond_verydry_cool\n",
    "\n",
    "for i in range(len(selection_matrix.ravel())):\n",
    "    if selection_matrix.ravel()[i]:\n",
    "        print(stations.Kwajalein.ymdhms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selection_matrix = cond_verydry_cool_90patcor\n",
    "\n",
    "for i in range(len(selection_matrix.ravel())):\n",
    "    if selection_matrix.ravel()[i]:\n",
    "        print(stations.Kwajalein.ymdhms[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below we set up the matrices that will contain the composite members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "season_members_verydry = np.zeros(12)\n",
    "gpcp_season_members_verydry = np.zeros(12)\n",
    "\n",
    "sst_composite_members_verydry = np.empty([50,12, len(ersst_latitudes), len(ersst_longitudes)])\n",
    "sst_composite_members_verydry[:] = np.NAN\n",
    "\n",
    "precip_composite_members_verydry = np.empty([50,12, len(precip_latitudes), len(precip_longitudes)])\n",
    "precip_composite_members_verydry[:] = np.NAN\n",
    "\n",
    "PREC_precip_composite_members_verydry = np.empty([50,12, len(PREC_precip_latitudes), len(PREC_precip_longitudes)])\n",
    "PREC_precip_composite_members_verydry[:] = np.NAN\n",
    "\n",
    "uwnd_composite_members_verydry = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "uwnd_composite_members_verydry[:] = np.NAN\n",
    "vwnd_composite_members_verydry = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "vwnd_composite_members_verydry[:] = np.NAN\n",
    "hgt_composite_members_verydry = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "hgt_composite_members_verydry[:] = np.NAN\n",
    "\n",
    "season_members_dry = np.zeros(12)\n",
    "gpcp_season_members_dry = np.zeros(12)\n",
    "\n",
    "precip_composite_members_dry = np.empty([50,12, len(precip_latitudes), len(precip_longitudes)])\n",
    "precip_composite_members_dry[:] = np.NAN\n",
    "\n",
    "sst_composite_members_dry = np.empty([50,12, len(ersst_latitudes), len(ersst_longitudes)])\n",
    "sst_composite_members_dry[:] = np.NAN\n",
    "\n",
    "PREC_precip_composite_members_dry = np.empty([51,12, len(PREC_precip_latitudes), len(PREC_precip_longitudes)])\n",
    "PREC_precip_composite_members_dry[:] = np.NAN\n",
    "\n",
    "uwnd_composite_members_dry = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "uwnd_composite_members_dry[:] = np.NAN\n",
    "vwnd_composite_members_dry = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "vwnd_composite_members_dry[:] = np.NAN\n",
    "hgt_composite_members_dry = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "hgt_composite_members_dry[:] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "season_members_verydry_90patcor = np.zeros(12)\n",
    "gpcp_season_members_verydry_90patcor = np.zeros(12)\n",
    "\n",
    "precip_composite_members_verydry_90patcor = np.empty([50,12, len(precip_latitudes), len(precip_longitudes)])\n",
    "precip_composite_members_verydry_90patcor[:] = np.NAN\n",
    "\n",
    "sst_composite_members_verydry_90patcor = np.empty([50,12, len(ersst_latitudes), len(ersst_longitudes)])\n",
    "sst_composite_members_verydry_90patcor[:] = np.NAN\n",
    "\n",
    "PREC_precip_composite_members_verydry_90patcor = np.empty([50,12, len(PREC_precip_latitudes), len(PREC_precip_longitudes)])\n",
    "PREC_precip_composite_members_verydry_90patcor[:] = np.NAN\n",
    "\n",
    "uwnd_composite_members_verydry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "uwnd_composite_members_verydry_90patcor[:] = np.NAN\n",
    "vwnd_composite_members_verydry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "vwnd_composite_members_verydry_90patcor[:] = np.NAN\n",
    "hgt_composite_members_verydry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "hgt_composite_members_verydry_90patcor[:] = np.NAN\n",
    "\n",
    "season_members_dry_90patcor = np.zeros(12)\n",
    "gpcp_season_members_dry_90patcor = np.zeros(12)\n",
    "\n",
    "precip_composite_members_dry_90patcor = np.empty([50,12, len(precip_latitudes), len(precip_longitudes)])\n",
    "precip_composite_members_dry_90patcor[:] = np.NAN\n",
    "\n",
    "sst_composite_members_dry_90patcor = np.empty([50,12, len(ersst_latitudes), len(ersst_longitudes)])\n",
    "sst_composite_members_dry_90patcor[:] = np.NAN\n",
    "\n",
    "PREC_precip_composite_members_dry_90patcor = np.empty([50,12, len(PREC_precip_latitudes), len(PREC_precip_longitudes)])\n",
    "PREC_precip_composite_members_dry_90patcor[:] = np.NAN\n",
    "\n",
    "uwnd_composite_members_dry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "uwnd_composite_members_dry_90patcor[:] = np.NAN\n",
    "vwnd_composite_members_dry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "vwnd_composite_members_dry_90patcor[:] = np.NAN\n",
    "hgt_composite_members_dry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "hgt_composite_members_dry_90patcor[:] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#season_members_verydry_90patcor = np.zeros(12)\n",
    "\n",
    "#sst_composite_members_verydry_90patcor = np.empty([50,12, len(ersst_latitudes), len(ersst_longitudes)])\n",
    "#sst_composite_members_verydry_90patcor[:] = np.NAN\n",
    "#uwnd_composite_members_verydry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "#uwnd_composite_members_verydry_90patcor[:] = np.NAN\n",
    "#vwnd_composite_members_verydry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "#vwnd_composite_members_verydry_90patcor[:] = np.NAN\n",
    "#hgt_composite_members_verydry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "#hgt_composite_members_verydry_90patcor[:] = np.NAN\n",
    "\n",
    "#season_members_dry_90patcor = np.zeros(12)\n",
    "\n",
    "#sst_composite_members_dry_90patcor = np.empty([50,12, len(ersst_latitudes), len(ersst_longitudes)])\n",
    "#sst_composite_members_dry_90patcor[:] = np.NAN\n",
    "#uwnd_composite_members_dry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "#uwnd_composite_members_dry_90patcor[:] = np.NAN\n",
    "#vwnd_composite_members_dry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "#vwnd_composite_members_dry_90patcor[:] = np.NAN\n",
    "#hgt_composite_members_dry_90patcor = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "#hgt_composite_members_dry_90patcor[:] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "season_members_verydry = np.zeros(12)\n",
    "gpcp_season_members_verydry = np.zeros(12)\n",
    "\n",
    "season_members_verydry_90patcor = np.zeros(12)\n",
    "gpcp_season_members_verydry_90patcor = np.zeros(12)\n",
    "\n",
    "\n",
    "year_range = range(1966,2016)\n",
    "short_year_range = range(1979,2016)\n",
    "for year in short_year_range:\n",
    "    for m in range(1,13):\n",
    "        \n",
    "        if cond_verydry_cool[year_range.index(year),m-1]:   \n",
    "            \n",
    "            \n",
    "            #print(year,year_range.index(year),m,cond_verydry_cool[year_range.index(year),m-1])\n",
    "\n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_verydry[year_range.index(year),m-1,:,:] = sst_seasonal_anom.s_anom[sst_selection]\n",
    "            \n",
    "            PREC_precip_selection = ((PREC_precip_ca.ymdhms[:, 0] == year) & (PREC_precip_ca.ymdhms[:, 1] == m))\n",
    "            PREC_precip_selection = np.nonzero(PREC_precip_selection)[0][0]\n",
    "            PREC_precip_composite_members_verydry[year_range.index(year),m-1,:,:] = PREC_precip_seasonal_anom.s_anom[PREC_precip_selection]\n",
    "            \n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_composite_members_verydry[year_range.index(year),m-1,:,:] = uwnd_seasonal_anom.s_anom[selection]\n",
    "            vwnd_composite_members_verydry[year_range.index(year),m-1,:,:] = vwnd_seasonal_anom.s_anom[selection]\n",
    "            hgt_composite_members_verydry[year_range.index(year),m-1,:,:] = hgt_seasonal_anom.s_anom[selection]\n",
    "            \n",
    "            season_members_verydry[m-1] = season_members_verydry[m-1] +1\n",
    "            \n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_verydry[year_range.index(year),m-1,:,:] = precip_seasonal_anom.s_anom[precip_selection]\n",
    "\n",
    "            gpcp_season_members_verydry[m-1] = gpcp_season_members_verydry[m-1] +1\n",
    "                \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "for year in short_year_range:\n",
    "\n",
    "    for m in range(1,13):\n",
    "        \n",
    "        if cond_verydry_cool_90patcor[year_range.index(year),m-1]:   \n",
    "            \n",
    "            #print(year,year_range.index(year),m,cond_verydry_cool[year_range.index(year),m-1])\n",
    "\n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_verydry_90patcor[year_range.index(year),m-1,:,:] = sst_seasonal_anom.s_anom[sst_selection]\n",
    "            \n",
    "            PREC_precip_selection = ((PREC_precip_ca.ymdhms[:, 0] == year) & (PREC_precip_ca.ymdhms[:, 1] == m))\n",
    "            PREC_precip_selection = np.nonzero(PREC_precip_selection)[0][0]\n",
    "            PREC_precip_composite_members_verydry_90patcor[year_range.index(year),m-1,:,:] = PREC_precip_seasonal_anom.s_anom[PREC_precip_selection]\n",
    "            \n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_composite_members_verydry_90patcor[year_range.index(year),m-1,:,:] = uwnd_seasonal_anom.s_anom[selection]\n",
    "            vwnd_composite_members_verydry_90patcor[year_range.index(year),m-1,:,:] = vwnd_seasonal_anom.s_anom[selection]\n",
    "            hgt_composite_members_verydry_90patcor[year_range.index(year),m-1,:,:] = hgt_seasonal_anom.s_anom[selection]\n",
    "            \n",
    "            season_members_verydry_90patcor[m-1] = season_members_verydry_90patcor[m-1] +1\n",
    "            \n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_verydry_90patcor[year_range.index(year),m-1,:,:] = precip_seasonal_anom.s_anom[precip_selection]\n",
    "\n",
    "            gpcp_season_members_verydry_90patcor[m-1] = gpcp_season_members_verydry_90patcor[m-1] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(np.shape(PREC_precip_seasonal_anom.s_anom))\n",
    "#PREC_precip_selection = ((PREC_precip_ca.ymdhms[:, 0] == 1999) & (PREC_precip_ca.ymdhms[:, 1] == m))\n",
    "#PREC_precip_selection = np.nonzero(PREC_precip_selection)[0][0]\n",
    "#print(PREC_precip_seasonal_anom.s_anom[PREC_precip_selection])\n",
    "\n",
    "#year_range = range(1966,2016)\n",
    "#for year in year_range:\n",
    "#    print(year_range.index(year), year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "season_members_dry = np.zeros(12)\n",
    "gpcp_season_members_dry = np.zeros(12)\n",
    "\n",
    "season_members_dry_90patcor = np.zeros(12)\n",
    "gpcp_season_members_dry_90patcor = np.zeros(12)\n",
    "\n",
    "for year in short_year_range:\n",
    "    for m in range(1,13):\n",
    "        \n",
    "        if cond_dry_cool[year_range.index(year),m-1]:   \n",
    "            \n",
    "            #print(m-1,year)\n",
    "            #print(year,year_range.index(year),m,cond_verydry_cool[year_range.index(year),m-1])\n",
    "\n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_dry[year_range.index(year),m-1,:,:] = sst_seasonal_anom.s_anom[sst_selection]\n",
    "            \n",
    "            PREC_precip_selection = ((PREC_precip_ca.ymdhms[:, 0] == year) & (PREC_precip_ca.ymdhms[:, 1] == m))\n",
    "            PREC_precip_selection = np.nonzero(PREC_precip_selection)[0][0]\n",
    "            PREC_precip_composite_members_dry[year_range.index(year),m-1,:,:] = PREC_precip_seasonal_anom.s_anom[PREC_precip_selection]\n",
    "            \n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_composite_members_dry[year_range.index(year),m-1,:,:] = uwnd_seasonal_anom.s_anom[selection]\n",
    "            vwnd_composite_members_dry[year_range.index(year),m-1,:,:] = vwnd_seasonal_anom.s_anom[selection]\n",
    "            hgt_composite_members_dry[year_range.index(year),m-1,:,:] = hgt_seasonal_anom.s_anom[selection]\n",
    "            \n",
    "            season_members_dry[m-1] = season_members_dry[m-1] +1\n",
    "            \n",
    "\n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_dry[year_range.index(year),m-1,:,:] = precip_seasonal_anom.s_anom[precip_selection]\n",
    "\n",
    "            gpcp_season_members_dry[m-1] = gpcp_season_members_dry[m-1] +1\n",
    "                \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "for year in short_year_range:\n",
    "\n",
    "    for m in range(1,13):\n",
    "        \n",
    "        if cond_dry_cool_90patcor[year_range.index(year),m-1]:   \n",
    "            \n",
    "            #print(year,year_range.index(year),m,cond_verydry_cool[year_range.index(year),m-1])\n",
    "\n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_dry_90patcor[year_range.index(year),m-1,:,:] = sst_seasonal_anom.s_anom[sst_selection]\n",
    "            \n",
    "            PREC_precip_selection = ((PREC_precip_ca.ymdhms[:, 0] == year) & (PREC_precip_ca.ymdhms[:, 1] == m))\n",
    "            PREC_precip_selection = np.nonzero(PREC_precip_selection)[0][0]\n",
    "            PREC_precip_composite_members_dry_90patcor[year_range.index(year),m-1,:,:] = PREC_precip_seasonal_anom.s_anom[PREC_precip_selection]\n",
    "            \n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_composite_members_dry_90patcor[year_range.index(year),m-1,:,:] = uwnd_seasonal_anom.s_anom[selection]\n",
    "            vwnd_composite_members_dry_90patcor[year_range.index(year),m-1,:,:] = vwnd_seasonal_anom.s_anom[selection]\n",
    "            hgt_composite_members_dry_90patcor[year_range.index(year),m-1,:,:] = hgt_seasonal_anom.s_anom[selection]\n",
    "            \n",
    "            season_members_dry_90patcor[m-1] = season_members_dry_90patcor[m-1] +1\n",
    "            \n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_dry_90patcor[year_range.index(year),m-1,:,:] = precip_seasonal_anom.s_anom[precip_selection]\n",
    "\n",
    "            gpcp_season_members_dry_90patcor[m-1] = gpcp_season_members_dry_90patcor[m-1] +1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precip_composite_verydry = np.nanmean(precip_composite_members_verydry, axis=0)\n",
    "sst_composite_verydry = np.nanmean(sst_composite_members_verydry, axis=0)\n",
    "PREC_precip_composite_verydry = np.nanmean(PREC_precip_composite_members_verydry, axis=0)\n",
    "uwnd_composite_verydry = np.nanmean(uwnd_composite_members_verydry, axis=0)\n",
    "vwnd_composite_verydry = np.nanmean(vwnd_composite_members_verydry, axis=0)\n",
    "hgt_composite_verydry = np.nanmean(hgt_composite_members_verydry, axis=0)\n",
    "\n",
    "precip_composite_dry = np.nanmean(precip_composite_members_dry, axis=0)\n",
    "sst_composite_dry = np.nanmean(sst_composite_members_dry, axis=0)\n",
    "PREC_precip_composite_dry  = np.nanmean(PREC_precip_composite_members_dry, axis=0)\n",
    "uwnd_composite_dry = np.nanmean(uwnd_composite_members_dry, axis=0)\n",
    "vwnd_composite_dry = np.nanmean(vwnd_composite_members_dry, axis=0)\n",
    "hgt_composite_dry = np.nanmean(hgt_composite_members_dry, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precip_composite_verydry_90patcor = np.nanmean(precip_composite_members_verydry_90patcor, axis=0)\n",
    "sst_composite_verydry_90patcor = np.nanmean(sst_composite_members_verydry_90patcor, axis=0)\n",
    "PREC_precip_composite_verydry_90patcor = np.nanmean(PREC_precip_composite_members_verydry_90patcor, axis=0)\n",
    "uwnd_composite_verydry_90patcor = np.nanmean(uwnd_composite_members_verydry_90patcor, axis=0)\n",
    "vwnd_composite_verydry_90patcor = np.nanmean(vwnd_composite_members_verydry_90patcor, axis=0)\n",
    "hgt_composite_verydry_90patcor = np.nanmean(hgt_composite_members_verydry_90patcor, axis=0)\n",
    "\n",
    "precip_composite_dry_90patcor = np.nanmean(precip_composite_members_dry_90patcor, axis=0)\n",
    "sst_composite_dry_90patcor = np.nanmean(sst_composite_members_dry_90patcor, axis=0)\n",
    "PREC_precip_composite_dry_90patcor = np.nanmean(PREC_precip_composite_members_dry_90patcor, axis=0)\n",
    "uwnd_composite_dry_90patcor = np.nanmean(uwnd_composite_members_dry_90patcor, axis=0)\n",
    "vwnd_composite_dry_90patcor = np.nanmean(vwnd_composite_members_dry_90patcor, axis=0)\n",
    "hgt_composite_dry_90patcor = np.nanmean(hgt_composite_members_dry_90patcor, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "season_list = ['DJF' , 'JFM' , 'FMA',\n",
    "               'MAM' , 'AMJ' , 'MJJ',\n",
    "               'JJA' , 'JAS' , 'ASO',\n",
    "               'SON' , 'OND' , 'NDJ']\n",
    "\n",
    "month_list = ['Jan' , 'Feb' , 'Mar',\n",
    "              'Apr' , 'May' , 'Jun',\n",
    "              'Jul' , 'Aug' , 'Sep',\n",
    "              'Oct' , 'Nov' , 'Dec']\n",
    "\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 113\n",
    "plt.rcParams['axes.titlesize'] = 'small'\n",
    "plt.rcParams['ytick.labelsize'] = 'small'  # for colorbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here I define the plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotting_function(color_field, contour_field, u_wnd, v_wnd, member_counter, \n",
    "                      color_field_name, case_name, *argv, save = 'yes'):    \n",
    "    \n",
    "    #here basemap is producing a strange behaviour\n",
    "    m = Basemap(projection='merc', llcrnrlat=-50.1, urcrnrlat=50.01,\n",
    "                        llcrnrlon=80, urcrnrlon=300, lat_ts=0, resolution='c')\n",
    "\n",
    "    with open('mapcache.pk', mode='wb') as f:\n",
    "        pickle.dump(m, f)\n",
    "\n",
    "    ## Tweak the subplot specifications.  \n",
    "\n",
    "    subplotparams = dict(left=0.03, right=0.88,\n",
    "                         bottom=0.015, top=0.94,\n",
    "                         wspace=0.05, hspace=0.001)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(3,3, #sharex=True,\n",
    "                        figsize=(13, 7.8),\n",
    "                        gridspec_kw=subplotparams,\n",
    "                       )\n",
    "\n",
    "    mm = range(2,11)\n",
    "\n",
    "    for mon, pax in zip(mm, axs.flat):\n",
    "\n",
    "        with open('mapcache.pk', 'rb') as f:\n",
    "            m = pickle.load(f)\n",
    "        m.ax = pax\n",
    "\n",
    "        #reverse the coolwarm palatte to make blue rainy and red dry\n",
    "        cmap = plt.get_cmap('BrBG')\n",
    "        bounds = [-2.5,-2, -1.5,-1.25, -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 2.5]\n",
    "\n",
    "\n",
    "        x, y = m(*np.meshgrid(PREC_precip_longitudes, PREC_precip_latitudes))\n",
    "        im = m.contourf(x, y, color_field[mon,:,:], levels=bounds, cmap=cmap,\n",
    "                        extend='both')\n",
    "\n",
    "        x_hgt, y_hgt = m(*np.meshgrid(ncep_longitudes, ncep_latitudes))\n",
    "        #hgt_bounds = np.arange(-40,40,1)\n",
    "        hgt_bounds = [-80, -70,-60, -50, -40, \n",
    "                      -30, -20, -10,-8, -6, \n",
    "                      -4, -2, -1,0, 1, 2, 4, \n",
    "                      6, 8, 10, 20, 30,\n",
    "                      40, 50, 60, 70, 80]\n",
    "        hgt_bound_0 = [0]\n",
    "        im_hgt = m.contour(x_hgt, y_hgt, contour_field[mon,:,:], levels=hgt_bounds,\n",
    "                           linewidths=0.5, colors='k')\n",
    "        im_hgt = m.contour(x_hgt, y_hgt, contour_field[mon,:,:], levels=hgt_bound_0,\n",
    "                       linewidths=0.9, colors='k')\n",
    "\n",
    "\n",
    "        # transform vectors to projection grid.\n",
    "        uproj, vproj, xx, yy = m.rotate_vector(u_wnd[mon,:,:], \n",
    "                                               v_wnd[mon,:,:], \n",
    "                                               ncep_longitudes, \n",
    "                                               ncep_latitudes,\n",
    "                                               returnxy=True)\n",
    "        # now plot every other vector\n",
    "        Q = m.quiver(xx[::2,::2], yy[::2,::2], \n",
    "                     uproj[::2,::2], vproj[::2,::2],\n",
    "                     scale=20, scale_units='inches')\n",
    "\n",
    "        m.drawcoastlines()\n",
    "        \n",
    "        if box_coords in argv:\n",
    "            x1,y1 = m(box_coords[2], box_coords[1])\n",
    "            x2,y2 = m(box_coords[2], box_coords[0])\n",
    "            x3,y3 = m(box_coords[3], box_coords[0])\n",
    "            x4,y4 = m(box_coords[3], box_coords[1])\n",
    "            \n",
    "            p = Polygon([(x1,y1), (x2,y2), (x3,y3), (x4,y4)], \n",
    "                        edgecolor='b', facecolor = 'none', linewidth = 2)\n",
    "            m.ax.add_patch(p)\n",
    "        \n",
    "        parallels = np.arange(-90, 90, 30)\n",
    "        meridians = np.arange(-180, 180, 60)\n",
    "        if pax in axs.flat[::3]:\n",
    "            m.drawparallels(parallels, labels = [1, 0, 0, 1], fontsize=8)\n",
    "        else:\n",
    "            m.drawparallels(parallels, labels = [0, 0, 0, 0], fontsize=8)\n",
    "\n",
    "        if pax in axs.flat[:6]:\n",
    "            m.drawmeridians(meridians, labels = [0, 0, 0, 0], fontsize=8)\n",
    "        if pax in axs.flat[6:]:\n",
    "            m.drawmeridians(meridians, labels = [1, 0, 0, 1], fontsize=8)\n",
    "\n",
    "        #m.drawparallels(parallels, labels = [1, 0, 0, 1], fontsize=8)\n",
    "        #m.drawmeridians(meridians, labels = [1, 0, 0, 1], fontsize=8)\n",
    "\n",
    "        pax.set_title(color_field_name+\" \"+season_list[mon]+ \" Anom Comp. Memb \" + str(member_counter[mon]))\n",
    "\n",
    "    #cax, kw = mpl.colorbar.make_axes([ax for ax in axs.flat])\n",
    "    ##  This method of making an Axes for the cbar interacts very badly with\n",
    "    ##  basemap, so just make one manually.  This also gives more control, so\n",
    "    ##  it looks nicer.\n",
    "    left = subplotparams['right'] + 0.02\n",
    "    bottom = subplotparams['bottom'] + 0.05\n",
    "    width = 0.015\n",
    "    height = subplotparams['top'] - subplotparams['bottom'] - 0.1\n",
    "\n",
    "    cax = fig.add_axes([left, bottom, width, height])\n",
    "    cb = plt.colorbar(im, cax=cax)\n",
    "    cb.set_label('Precip Anomaly mm/day')\n",
    "\n",
    "    ## EF: The quiverkey needs to be made using an axes in which a quiver is\n",
    "    ##     drawn so that it has the right transform information.  Otherwise\n",
    "    ##     it won't get the length right.\n",
    "    qkx = left + (1 - left) / 4\n",
    "    qky = bottom + height + 0.025\n",
    "    qk = pax.quiverkey(Q, qkx, qky, 10, '10 m/s', \n",
    "                       coordinates='figure',\n",
    "                       labelpos='N',\n",
    "                       labelsep=0.07, \n",
    "                       fontproperties=dict(size='small'),\n",
    "                      )\n",
    "    \n",
    "    plt.suptitle(case_name+\" \"+color_field_name+\" PRECIP 850HGT WND seasonal anomaly composite. 1979-2015 clim\")\n",
    "    \n",
    "    if save == 'yes':\n",
    "        fig.savefig(\"./clim_1979-2015/\"+case_name+\"_\"+color_field_name+\"_PRECIP_850HGT_WND_seasonal_comp_79_15_clim.pdf\")\n",
    "        fig.savefig(\"./clim_1979-2015/\"+case_name+\"_\"+color_field_name+\"_PRECIP_850HGT_WND_seasonal_comp_79_15_clim.png\")\n",
    "    #fig.savefig(\"VERY_DRY_COMPOSITE_SST_850HGT_WND_seasonal_anomaly_%04d-%02d.pdf\" % (year, mon))\n",
    "        plt.close()# no need for dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sst_plotting_function(color_field, contour_field, u_wnd, v_wnd, member_counter, \n",
    "                          color_field_name, case_name, *argv, save = 'yes'):    \n",
    "    \n",
    "    #here basemap is producing a strange behaviour\n",
    "    m = Basemap(projection='merc', llcrnrlat=-50.1, urcrnrlat=50.01,\n",
    "                        llcrnrlon=80, urcrnrlon=300, lat_ts=0, resolution='c')\n",
    "\n",
    "    with open('mapcache.pk', mode='wb') as f:\n",
    "        pickle.dump(m, f)\n",
    "\n",
    "    ## Tweak the subplot specifications.  \n",
    "\n",
    "    subplotparams = dict(left=0.03, right=0.88,\n",
    "                         bottom=0.015, top=0.94,\n",
    "                         wspace=0.05, hspace=0.001)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(3,3, #sharex=True,\n",
    "                        figsize=(13, 7.8),\n",
    "                        gridspec_kw=subplotparams,\n",
    "                       )\n",
    "\n",
    "    mm = range(2,11)\n",
    "\n",
    "    for mon, pax in zip(mm, axs.flat):\n",
    "\n",
    "        with open('mapcache.pk', 'rb') as f:\n",
    "            m = pickle.load(f)\n",
    "        m.ax = pax\n",
    "\n",
    "        #reverse the coolwarm palatte to make blue rainy and red dry\n",
    "        cmap = cm.coolwarm\n",
    "        bounds = [-2, -1.5,-1.25, -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2]\n",
    "\n",
    "\n",
    "        x, y = m(*np.meshgrid(ersst_longitudes, ersst_latitudes))\n",
    "        im = m.contourf(x, y, color_field[mon,:,:], levels=bounds, cmap=cmap,\n",
    "                        extend='both')\n",
    "\n",
    "        x_hgt, y_hgt = m(*np.meshgrid(ncep_longitudes, ncep_latitudes))\n",
    "        #hgt_bounds = np.arange(-40,40,1)\n",
    "        hgt_bounds = [-80, -70,-60, -50, -40, \n",
    "                      -30, -20, -10,-8, -6, \n",
    "                      -4, -2, -1,0, 1, 2, 4, \n",
    "                      6, 8, 10, 20, 30,\n",
    "                      40, 50, 60, 70, 80]\n",
    "        hgt_bound_0 = [0]\n",
    "        im_hgt = m.contour(x_hgt, y_hgt, contour_field[mon,:,:], levels=hgt_bounds,\n",
    "                           linewidths=0.5, colors='k')\n",
    "        im_hgt = m.contour(x_hgt, y_hgt, contour_field[mon,:,:], levels=hgt_bound_0,\n",
    "                       linewidths=0.9, colors='k')\n",
    "\n",
    "\n",
    "        # transform vectors to projection grid.\n",
    "        uproj, vproj, xx, yy = m.rotate_vector(u_wnd[mon,:,:], \n",
    "                                               v_wnd[mon,:,:], \n",
    "                                               ncep_longitudes, \n",
    "                                               ncep_latitudes,\n",
    "                                               returnxy=True)\n",
    "        # now plot every other vector\n",
    "        Q = m.quiver(xx[::2,::2], yy[::2,::2], \n",
    "                     uproj[::2,::2], vproj[::2,::2],\n",
    "                     scale=20, scale_units='inches')\n",
    "\n",
    "        m.drawcoastlines()\n",
    "        \n",
    "        #draw the box\n",
    "        #lons = range(180,200,2.5)\n",
    "        #lats = \n",
    "        #m.drawgreatcircle(180,20,180,0,linewidth = 2, color = 'b')\n",
    "        \n",
    "        if box_coords in argv:\n",
    "            x1,y1 = m(box_coords[2], box_coords[1])\n",
    "            x2,y2 = m(box_coords[2], box_coords[0])\n",
    "            x3,y3 = m(box_coords[3], box_coords[0])\n",
    "            x4,y4 = m(box_coords[3], box_coords[1])\n",
    "            \n",
    "            p = Polygon([(x1,y1), (x2,y2), (x3,y3), (x4,y4)], \n",
    "                        edgecolor='b', facecolor = 'none', linewidth = 2)\n",
    "            m.ax.add_patch(p)\n",
    "            #plt.gca().add_patch(p)\n",
    "\n",
    "        \n",
    "        parallels = np.arange(-90, 90, 30)\n",
    "        meridians = np.arange(-180, 180, 60)\n",
    "        if pax in axs.flat[::3]:\n",
    "            m.drawparallels(parallels, labels = [1, 0, 0, 1], fontsize=8)\n",
    "        else:\n",
    "            m.drawparallels(parallels, labels = [0, 0, 0, 0], fontsize=8)\n",
    "\n",
    "        if pax in axs.flat[:6]:\n",
    "            m.drawmeridians(meridians, labels = [0, 0, 0, 0], fontsize=8)\n",
    "        if pax in axs.flat[6:]:\n",
    "            m.drawmeridians(meridians, labels = [1, 0, 0, 1], fontsize=8)\n",
    "\n",
    "        #m.drawparallels(parallels, labels = [1, 0, 0, 1], fontsize=8)\n",
    "        #m.drawmeridians(meridians, labels = [1, 0, 0, 1], fontsize=8)\n",
    "\n",
    "        pax.set_title(color_field_name+\" \"+season_list[mon]+ \" Anom Comp. Memb \" + str(member_counter[mon]))\n",
    "\n",
    "    #cax, kw = mpl.colorbar.make_axes([ax for ax in axs.flat])\n",
    "    ##  This method of making an Axes for the cbar interacts very badly with\n",
    "    ##  basemap, so just make one manually.  This also gives more control, so\n",
    "    ##  it looks nicer.\n",
    "    left = subplotparams['right'] + 0.02\n",
    "    bottom = subplotparams['bottom'] + 0.05\n",
    "    width = 0.015\n",
    "    height = subplotparams['top'] - subplotparams['bottom'] - 0.1\n",
    "\n",
    "    cax = fig.add_axes([left, bottom, width, height])\n",
    "    cb = plt.colorbar(im, cax=cax)\n",
    "    cb.set_label('SST Anomaly Deg C')\n",
    "\n",
    "    ## EF: The quiverkey needs to be made using an axes in which a quiver is\n",
    "    ##     drawn so that it has the right transform information.  Otherwise\n",
    "    ##     it won't get the length right.\n",
    "    qkx = left + (1 - left) / 4\n",
    "    qky = bottom + height + 0.025\n",
    "    qk = pax.quiverkey(Q, qkx, qky, 10, '10 m/s', \n",
    "                       coordinates='figure',\n",
    "                       labelpos='N',\n",
    "                       labelsep=0.07, \n",
    "                       fontproperties=dict(size='small'),\n",
    "                      )\n",
    "    \n",
    "    plt.suptitle(case_name+\" \"+color_field_name+\" SST 850HGT WND seasonal anomaly composite. 1979-2015 clim\")\n",
    "    \n",
    "    if save == 'yes':\n",
    "        fig.savefig(\"./clim_1979-2015/\"+case_name+\"_\"+color_field_name+\"_SST_850HGT_WND_seasonal_comp_79_15_clim.pdf\")\n",
    "        fig.savefig(\"./clim_1979-2015/\"+case_name+\"_\"+color_field_name+\"_SST_850HGT_WND_seasonal_comp_79_15_clim.png\")\n",
    "        #fig.savefig(\"VERY_DRY_COMPOSITE_SST_850HGT_WND_seasonal_anomaly_%04d-%02d.pdf\" % (year, mon))\n",
    "        plt.close()# no need for dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting_function(PREC_precip_composite_verydry, hgt_composite_verydry, \n",
    "#                  uwnd_composite_verydry, vwnd_composite_verydry, \n",
    "#                  season_members_verydry, 'PREC', \"Very_Dry\")\n",
    "\n",
    "box_coords =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we plot the composites for the verydry cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verydry case composite with PREC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotting_function(PREC_precip_composite_verydry_90patcor, hgt_composite_verydry_90patcor, \n",
    "                  uwnd_composite_verydry_90patcor, vwnd_composite_verydry_90patcor, \n",
    "                  season_members_verydry_90patcor, 'PREC', \"Very_Dry_90patcor\",  save = 'yes')\n",
    "\n",
    "plotting_function(PREC_precip_composite_dry, hgt_composite_dry, \n",
    "                  uwnd_composite_dry, vwnd_composite_dry, \n",
    "                  season_members_dry, 'PREC', \"Dry\",  save = 'yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPCP verydry 90 patcor composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotting_function(precip_composite_verydry, hgt_composite_verydry, \n",
    "                  uwnd_composite_verydry, vwnd_composite_verydry, \n",
    "                  gpcp_season_members_verydry, 'GPCP', \"Very_Dry\",  save = 'yes')\n",
    "\n",
    "\n",
    "plotting_function(precip_composite_verydry_90patcor, hgt_composite_verydry_90patcor, \n",
    "                  uwnd_composite_verydry_90patcor, vwnd_composite_verydry_90patcor, \n",
    "                  gpcp_season_members_verydry_90patcor, 'GPCP', \"Very_Dry_90patcor\",  save = 'yes')\n",
    "\n",
    "plotting_function(precip_composite_dry, hgt_composite_dry, \n",
    "                  uwnd_composite_dry, vwnd_composite_dry, \n",
    "                  gpcp_season_members_dry, 'GPCP', \"Dry\",  save = 'yes')\n",
    "\n",
    "\n",
    "plotting_function(precip_composite_dry_90patcor, hgt_composite_dry_90patcor, \n",
    "                  uwnd_composite_dry_90patcor, vwnd_composite_dry_90patcor, \n",
    "                  gpcp_season_members_dry_90patcor, 'GPCP', \"Dry_90patcor\",  save = 'yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sst_plotting_function(sst_composite_verydry, hgt_composite_verydry, \n",
    "                      uwnd_composite_verydry, vwnd_composite_verydry, \n",
    "                      season_members_verydry, 'ERSST', \"Very_Dry\",  save = 'yes')\n",
    "\n",
    "\n",
    "sst_plotting_function(sst_composite_verydry_90patcor, hgt_composite_verydry_90patcor, \n",
    "                      uwnd_composite_verydry_90patcor, vwnd_composite_verydry_90patcor, \n",
    "                      season_members_verydry_90patcor, 'ERSST', \"Very_Dry_90patcor\",  save = 'yes')\n",
    "\n",
    "sst_plotting_function(sst_composite_dry, hgt_composite_dry, \n",
    "                      uwnd_composite_dry, vwnd_composite_dry, \n",
    "                      season_members_dry, 'ERSST', \"Dry\",  save = 'yes')\n",
    "\n",
    "\n",
    "sst_plotting_function(sst_composite_dry_90patcor, hgt_composite_dry_90patcor, \n",
    "                      uwnd_composite_dry_90patcor, vwnd_composite_dry_90patcor, \n",
    "                      season_members_dry_90patcor, 'ERSST', \"Dry_90patcor\",  save = 'yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the non dry composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cond_non_dry = kwajalein_seasonal_anom_matrix >= 0* kwajalein_season_std_matrix\n",
    "cond_oni = oni_period_matrix <= -0.1\n",
    "cond_non_dry_cool = np.logical_and(cond_non_dry, cond_oni)\n",
    "\n",
    "precip_composite_members_non_dry = np.empty([50,12, len(precip_latitudes), len(precip_longitudes)])\n",
    "precip_composite_members_non_dry[:] = np.NAN\n",
    "\n",
    "sst_composite_members_non_dry = np.empty([50,12, len(ersst_latitudes), len(ersst_longitudes)])\n",
    "sst_composite_members_non_dry[:] = np.NAN\n",
    "\n",
    "PREC_precip_composite_members_non_dry = np.empty([50,12, len(PREC_precip_latitudes), len(PREC_precip_longitudes)])\n",
    "PREC_precip_composite_members_non_dry[:] = np.NAN\n",
    "\n",
    "uwnd_non_dry_composite_members = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "uwnd_non_dry_composite_members[:] = np.NAN\n",
    "vwnd_non_dry_composite_members = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "vwnd_non_dry_composite_members[:] = np.NAN\n",
    "hgt_non_dry_composite_members = np.empty([50,12,len(ncep_latitudes), len(ncep_longitudes)])\n",
    "hgt_non_dry_composite_members[:] = np.NAN\n",
    "\n",
    "gpcp_season_members_non_dry = np.zeros(12)\n",
    "season_members_non_dry = np.zeros(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year_range = range(1966,2016)\n",
    "short_year_range = range(1979,2016)\n",
    "for year in short_year_range:\n",
    "    for m in range(1,13):\n",
    "        \n",
    "        if cond_non_dry_cool[year_range.index(year),m-1]:\n",
    "            \n",
    "            sst_selection = ((sst_ca.ymdhms[:, 0] == year) & (sst_ca.ymdhms[:, 1] == m))\n",
    "            sst_selection = np.nonzero(sst_selection)[0][0]\n",
    "            sst_composite_members_non_dry[year_range.index(year),m-1,:,:] = sst_seasonal_anom.s_anom[sst_selection]\n",
    "            \n",
    "            PREC_precip_selection = ((PREC_precip_ca.ymdhms[:, 0] == year) & (PREC_precip_ca.ymdhms[:, 1] == m))\n",
    "            PREC_precip_selection = np.nonzero(PREC_precip_selection)[0][0]\n",
    "            PREC_precip_composite_members_non_dry[year_range.index(year),m-1,:,:] = PREC_precip_seasonal_anom.s_anom[PREC_precip_selection]\n",
    "            \n",
    "            selection = ((uwnd_ca.ymdhms[:, 0] == year) & (uwnd_ca.ymdhms[:, 1] == m))\n",
    "            selection = np.nonzero(selection)[0][0]\n",
    "            uwnd_non_dry_composite_members[year_range.index(year),m-1,:,:] = uwnd_seasonal_anom.s_anom[selection]\n",
    "            vwnd_non_dry_composite_members[year_range.index(year),m-1,:,:] = vwnd_seasonal_anom.s_anom[selection]\n",
    "            hgt_non_dry_composite_members[year_range.index(year),m-1,:,:] = hgt_seasonal_anom.s_anom[selection]\n",
    "            \n",
    "            season_members_non_dry[m-1] = season_members_non_dry[m-1] +1\n",
    "            \n",
    "\n",
    "            precip_selection = ((precip_ca.ymdhms[:, 0] == year) & (precip_ca.ymdhms[:, 1] == m))\n",
    "            precip_selection = np.nonzero(precip_selection)[0][0]\n",
    "            precip_composite_members_non_dry[year_range.index(year),m-1,:,:] = precip_seasonal_anom.s_anom[precip_selection]\n",
    "\n",
    "            gpcp_season_members_non_dry[m-1] = gpcp_season_members_non_dry[m-1] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precip_non_dry_composite = np.nanmean(precip_composite_members_non_dry, axis=0)\n",
    "\n",
    "sst_non_dry_composite = np.nanmean(sst_composite_members_non_dry, axis=0)\n",
    "\n",
    "PREC_precip_non_dry_composite = np.nanmean(PREC_precip_composite_members_non_dry, axis=0)\n",
    "\n",
    "uwnd_non_dry_composite = np.nanmean(uwnd_non_dry_composite_members, axis=0)\n",
    "vwnd_non_dry_composite = np.nanmean(vwnd_non_dry_composite_members, axis=0)\n",
    "hgt_non_dry_composite = np.nanmean(hgt_non_dry_composite_members, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotting_function(PREC_precip_non_dry_composite, hgt_non_dry_composite, \n",
    "                  uwnd_non_dry_composite, vwnd_non_dry_composite, \n",
    "                  season_members_non_dry, 'PREC', \"Non_Dry\",  save = 'yes')\n",
    "\n",
    "plotting_function(precip_non_dry_composite, hgt_non_dry_composite, \n",
    "                  uwnd_non_dry_composite, vwnd_non_dry_composite, \n",
    "                  gpcp_season_members_non_dry, 'GPCP', \"Non_Dry\",  save = 'yes')\n",
    "\n",
    "sst_plotting_function(sst_non_dry_composite, hgt_non_dry_composite, \n",
    "                      uwnd_non_dry_composite, vwnd_non_dry_composite, \n",
    "                      season_members_non_dry, 'ERSST', \"Non_Dry\",  save = 'yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Here-------------------------^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here basemap is producing a strange behaviour\n",
    "m = Basemap(projection='merc', llcrnrlat=-50.1, urcrnrlat=50.01,\n",
    "                    llcrnrlon=80, urcrnrlon=300, lat_ts=0, resolution='c')\n",
    "\n",
    "with open('mapcache.pk', mode='wb') as f:\n",
    "    pickle.dump(m, f)\n",
    "\n",
    "## Tweak the subplot specifications.  \n",
    "\n",
    "subplotparams = dict(left=0.03, right=0.88,\n",
    "                     bottom=0.03, top=0.96,\n",
    "                     wspace=0.05, hspace=0.1)\n",
    "\n",
    "    \n",
    "fig, axs = plt.subplots(3,3, #sharex=True,\n",
    "                    figsize=(13, 7.8),\n",
    "                    gridspec_kw=subplotparams,\n",
    "                   )\n",
    "\n",
    "mm = range(2,11)\n",
    "\n",
    "for mon, pax in zip(mm, axs.flat):\n",
    "\n",
    "    with open('mapcache.pk', 'rb') as f:\n",
    "        m = pickle.load(f)\n",
    "    m.ax = pax\n",
    "\n",
    "    #reverse the coolwarm palatte to make blue rainy and red dry\n",
    "    cmap = cm.coolwarm\n",
    "    bounds = [-2, -1.5,-1.25, -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2]\n",
    "    #norm = cols.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    #im = m.imshow(sst_transform,cmap=cm.coolwarm, interpolation = \"none\",norm=norm)\n",
    "\n",
    "    ##  EF: using an image gains nothing here, loses resolution, and adds complexity.\n",
    "    ##      Contouring is nicer.  If you wanted something image-like, it would\n",
    "    ##      be better to use pcolormesh--but there is no point when you are using\n",
    "    ##      discretized colors on a relatively fine lon/lat grid like this.\n",
    "\n",
    "    x, y = m(*np.meshgrid(ersst_longitudes, ersst_latitudes))\n",
    "    im = m.contourf(x, y, sst_non_dry_composite[mon,:,:], levels=bounds, cmap=cmap,\n",
    "                    extend='both')\n",
    "\n",
    "    x_hgt, y_hgt = m(*np.meshgrid(ncep_longitudes, ncep_latitudes))\n",
    "    #hgt_bounds = np.arange(-40,40,1)\n",
    "    hgt_bounds = [-80, -70,-60, -50, -40, \n",
    "                  -30, -20, -10,-8, -6, \n",
    "                  -4, -2, -1,0, 1, 2, 4, \n",
    "                  6, 8, 10, 20, 30,\n",
    "                  40, 50, 60, 70, 80]\n",
    "    im_hgt = m.contour(x_hgt, y_hgt, hgt_non_dry_composite[mon,:,:], levels=hgt_bounds,\n",
    "                       linewidths=0.5, colors='k')\n",
    "\n",
    "\n",
    "    # transform vectors to projection grid.\n",
    "    uproj, vproj, xx, yy = m.rotate_vector(uwnd_non_dry_composite[mon,:,:], \n",
    "                                           vwnd_non_dry_composite[mon,:,:], \n",
    "                                           ncep_longitudes, \n",
    "                                           ncep_latitudes,\n",
    "                                           returnxy=True)\n",
    "    # now plot every other vector\n",
    "    Q = m.quiver(xx[::2,::2], yy[::2,::2], \n",
    "                 uproj[::2,::2], vproj[::2,::2],\n",
    "                 scale=20, scale_units='inches')\n",
    "\n",
    "    m.drawcoastlines()\n",
    "    parallels = np.arange(-90, 90, 30)\n",
    "    meridians = np.arange(-180, 180, 60)\n",
    "    if pax in axs.flat[::3]:\n",
    "        m.drawparallels(parallels, labels = [1, 0, 0, 1], fontsize=8)\n",
    "    else:\n",
    "        m.drawparallels(parallels, labels = [0, 0, 0, 0], fontsize=8)\n",
    "\n",
    "    if pax in axs.flat[:6]:\n",
    "        m.drawmeridians(meridians, labels = [0, 0, 0, 0], fontsize=8)\n",
    "    if pax in axs.flat[6:]:\n",
    "        m.drawmeridians(meridians, labels = [1, 0, 0, 1], fontsize=8)\n",
    "\n",
    "    #m.drawparallels(parallels, labels = [1, 0, 0, 1], fontsize=8)\n",
    "    #m.drawmeridians(meridians, labels = [1, 0, 0, 1], fontsize=8)\n",
    "\n",
    "    pax.set_title(\"Seasonal Anomaly Composite \" +season_list[mon] + \" members \" + str(season_members_non_dry[mon]))\n",
    "\n",
    "#cax, kw = mpl.colorbar.make_axes([ax for ax in axs.flat])\n",
    "##  This method of making an Axes for the cbar interacts very badly with\n",
    "##  basemap, so just make one manually.  This also gives more control, so\n",
    "##  it looks nicer.\n",
    "left = subplotparams['right'] + 0.02\n",
    "bottom = subplotparams['bottom'] + 0.05\n",
    "width = 0.015\n",
    "height = subplotparams['top'] - subplotparams['bottom'] - 0.1\n",
    "\n",
    "cax = fig.add_axes([left, bottom, width, height])\n",
    "cb = plt.colorbar(im, cax=cax)\n",
    "cb.set_label('SST C')\n",
    "\n",
    "## EF: The quiverkey needs to be made using an axes in which a quiver is\n",
    "##     drawn so that it has the right transform information.  Otherwise\n",
    "##     it won't get the length right.\n",
    "qkx = left + (1 - left) / 4\n",
    "qky = bottom + height + 0.025\n",
    "qk = pax.quiverkey(Q, qkx, qky, 10, '10 m/s', \n",
    "                   coordinates='figure',\n",
    "                   labelpos='N',\n",
    "                   labelsep=0.07, \n",
    "                   fontproperties=dict(size='small'),\n",
    "                  )\n",
    "\n",
    "fig.savefig(\"NON_DRY_COMPOSITE_SST_850HGT_WNDseasonal_anomaly.pdf\")\n",
    "#plt.close()# no need for dpi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
