{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "%matplotlib qt\n",
    "\n",
    "#This notebook is a testbed for importing PEAC Center USAPI\n",
    "#raifnall data using pandas and doing some quick analysis\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as cols\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap, shiftgrid\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "from datetime import date\n",
    "import datetime\n",
    "\n",
    "import calendar\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "from scipy import signal, linalg, stats\n",
    "\n",
    "from pycurrents.codas import to_day, to_date\n",
    "from pycurrents.plot.mpltools import dday_to_mpl\n",
    "\n",
    "from pycurrents.system import Bunch\n",
    "from pycurrents.num import eof\n",
    "from pycurrents.num import rangeslice\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.special import comb\n",
    "#import metpy.calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir = '/home/erin/machine_learning_635/Final_Project/netcdf_data_files/'\n",
    "exceldatadir = '/home/erin/machine_learning_635/Final_Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NCEP UWND, VWND ahd HGT all go from Jan 1948 to present and are on a 2.5 deg grid\n",
    "# the grid goes from 0.0E to 357.5E and 90N to 90S\n",
    "uwnd_data = Dataset(datadir + 'uwnd.mon.mean.nc')\n",
    "vwnd_data = Dataset(datadir + 'vwnd.mon.mean.nc')\n",
    "hgt_data = Dataset(datadir + 'hgt.mon.mean.nc')\n",
    "\n",
    "# read lats,lons\n",
    "ncep_latitudes = uwnd_data.variables['lat'][:]\n",
    "ncep_longitudes = uwnd_data.variables['lon'][:]\n",
    "\n",
    "#read in ncep levels\n",
    "# these levels are\n",
    "#1000,925,850,700,600,500,400,300,250,200,150,100,70,50,30,20,10\n",
    "ncep_levels = uwnd_data.variables['level'][:]\n",
    "\n",
    "#Choose pressure level now, to save obn memory\n",
    "pressure_selection = ncep_levels[:] == 850\n",
    "\n",
    "\n",
    "#time in hours since 1800-01-01 00:00:0.0\n",
    "\n",
    "ncep_time = uwnd_data.variables['time'][:]\n",
    "\n",
    "# get  wind data.\n",
    "uin = np.squeeze(uwnd_data.variables['uwnd'][:, pressure_selection, :, :])\n",
    "vin = np.squeeze(vwnd_data.variables['vwnd'][:, pressure_selection, :, :])\n",
    "hgtin = np.squeeze(hgt_data.variables['hgt'][:, pressure_selection, :, :])\n",
    "\n",
    "uwnd_data.close()\n",
    "vwnd_data.close()\n",
    "hgt_data.close()\n",
    "\n",
    "# Let's just convert it to days:\n",
    "ncep_dday_1800 = ncep_time / 24\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "#GPCP data goes from Jan 1979 to present and is on a 2.5 deg grid\n",
    "#same grid as NCEP\n",
    "precip_data = Dataset(datadir + 'precip.mon.mean.nc')\n",
    "\n",
    "#time in hours since 1800-01-01 00:00:0.0\n",
    "precip_time = precip_data.variables['time'][:]\n",
    "\n",
    "# read lats,lons\n",
    "precip_latitudes = precip_data.variables['lat'][:]\n",
    "precip_longitudes = precip_data.variables['lon'][:]\n",
    "\n",
    "# get precip data.\n",
    "precip_in = precip_data.variables['precip'][:, :, :]\n",
    "\n",
    "precip_data.close()\n",
    "\n",
    "# GPCP time is in days since jan 1 1800\n",
    "precip_dday_1800 = precip_time\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "#ERSST v4 data goes from Jan 1854 to present and is on a 2 deg grid\n",
    "#88.0N - 88.0S, 0.0E - 358.0E.\n",
    "ersst_data = Dataset(datadir + 'sst.mnmean.v4.nc')\n",
    "\n",
    "# read lats,lons\n",
    "ersst_latitudes = ersst_data.variables['lat'][:]\n",
    "ersst_longitudes = ersst_data.variables['lon'][:]\n",
    "\n",
    "#time in hours since 1800-01-01 00:00:0.0\n",
    "ersst_time = ersst_data.variables['time'][:]\n",
    "\n",
    "# get  wind data.\n",
    "ersst_in = ersst_data.variables['sst'][:, :, :]\n",
    "\n",
    "ersst_data.close()\n",
    "\n",
    "# ERSST time is in days since jan 1 1800\n",
    "ersst_dday_1800 = ersst_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extend(dday_1800, dat):\n",
    "    \"\"\"\n",
    "    Return dday, dat extended to fill out the last year.\n",
    "    \n",
    "    This function was modifyed to take in only one data variable.\n",
    "    \"\"\"\n",
    "    n_orig = len(dday_1800)\n",
    "    ymdhms = to_date(1800, dday_1800)\n",
    "    nmissing = 12 - ymdhms[-1, 1]\n",
    "    nmonths = n_orig + nmissing\n",
    "    ymdhms_new = np.zeros((nmonths, 6), dtype=np.uint16)\n",
    "    ymdhms_new[:n_orig, :] = ymdhms\n",
    "    ymdhms_new[n_orig:, 0] = ymdhms[-1, 0]\n",
    "    # Fill in the remaining months:\n",
    "    ymdhms_new[n_orig:, 1] = np.arange(ymdhms[-1, 1] + 1, 13)\n",
    "    ymdhms_new[n_orig:, 2] = 1\n",
    "    dday_new = to_day(1800, ymdhms_new)\n",
    "    \n",
    "    shape_new = (nmonths, dat.shape[1], dat.shape[2])\n",
    "    ## EF: modify to work with masked array or ndarray\n",
    "    if np.ma.isMA(dat):\n",
    "        dat_new = np.ma.masked_all(shape_new, float)\n",
    "    else:    \n",
    "        dat_new = np.nan + np.zeros(shape_new, float)\n",
    "    \n",
    "    dat_new[:n_orig, :, :] = dat\n",
    "    \n",
    "    #This Function has been modifyed to also return the date of the last valid data and its ymdhms index\n",
    "    \n",
    "    last_valid_date = to_date(1800,dday_1800[-1])\n",
    "    last_valid_index = n_orig -1\n",
    "   \n",
    "    return Bunch(dday_1800=dday_new,\n",
    "                 ymdhms=ymdhms_new,\n",
    "                 dat=dat_new,\n",
    "                 last_valid_date=last_valid_date,\n",
    "                 last_valid_index=last_valid_index)\n",
    "\n",
    "def cal_climatology_and_anomaly(data,ymdhms,last_valid_date,last_valid_index,latitudes,longitudes,start_year, end_year):\n",
    "                           \n",
    "    #Select the years for climatology from the new ymdhms_padded\n",
    "    # All the monthly data for the years you want to calculate the climatology\n",
    "    \n",
    "    clim_selection = ((ymdhms[:, 0] >= start_year) & (ymdhms[:, 0] <= end_year))\n",
    "    \n",
    "    # EF: Trim everything right at the start.\n",
    "    data = data[clim_selection]\n",
    "    ymdhms = ymdhms[clim_selection]\n",
    "    \n",
    "    #Calculate the total numbers of years in the climatology\n",
    "    length_in_years = ymdhms[-1, 0] - ymdhms[0, 0] + 1\n",
    "    \n",
    "    # EF: ensure we have masked arrays and no nans\n",
    "    data = np.ma.masked_invalid(data)\n",
    "    \n",
    "    #reshape the matrix so that is has dimensions of [years, months, lat, lon]\n",
    "    reshaped_data = np.reshape(data, (length_in_years, 12, \n",
    "                                      len(latitudes), \n",
    "                                      len(longitudes)))\n",
    "    #Calculate the climatology by using nanmeans, along the yeas axis\n",
    "    #climatology = np.nanmean(reshaped_data, axis=0)\n",
    "    \n",
    "    climatology = reshaped_data.mean(axis=0)\n",
    "    anomaly = reshaped_data - climatology\n",
    "    anomaly = anomaly.reshape(data.shape)\n",
    "    \n",
    "    if end_year < last_valid_date[0]:\n",
    "        last_valid_date_new = ymdhms[-1,:]\n",
    "        last_valid_index_new = -1\n",
    "    else:\n",
    "        last_valid_date_new = last_valid_date\n",
    "        last_valid_index_new = np.where((ymdhms[:,0]== last_valid_date[0]) &\n",
    "                                        (ymdhms[:,1]== last_valid_date[1]) &\n",
    "                                        (ymdhms[:,2]== last_valid_date[2]))[0][0]\n",
    "    \n",
    "    return Bunch(climatology=climatology,\n",
    "                 anomaly=anomaly, \n",
    "                 ymdhms=ymdhms,\n",
    "                 last_valid_date = last_valid_date_new,\n",
    "                 last_valid_index = last_valid_index_new)\n",
    "\n",
    "def seasonal_anomaly(m_anom,last_valid_index):\n",
    "    s_anom = np.ma.zeros(m_anom.shape, float)\n",
    "    orig_mask = np.ma.getmaskarray(m_anom)\n",
    "    \n",
    "    if last_valid_index == -1:\n",
    "        s_anom[1:-1] = (m_anom[:-2] + m_anom[1:-1] + m_anom[2:]) / 3\n",
    "    \n",
    "        s_anom[0] = (m_anom[0] + m_anom[1]) / 2\n",
    "        s_anom[-1] = (m_anom[-2] + m_anom[-1]) / 2\n",
    "    \n",
    "    else:\n",
    "        #s_anom[1:last_valid_index-1,:,:] = (m_anom[:last_valid_index-2,:,:] + \n",
    "        #                                    m_anom[1:last_valid_index-1,:,:] + \n",
    "        #                                    m_anom[2:last_valid_index,:,:]) / 3\n",
    "        s_anom[1:last_valid_index,:,:] = (m_anom[:last_valid_index-1,:,:] + \n",
    "                                          m_anom[1:last_valid_index,:,:] + \n",
    "                                          m_anom[2:last_valid_index+1,:,:]) / 3\n",
    "    \n",
    "        s_anom[0,:,:] = (m_anom[0,:,:] + m_anom[1,:,:]) / 2\n",
    "        s_anom[last_valid_index,:,:] = (m_anom[last_valid_index-1,:,:] + \n",
    "                                        m_anom[last_valid_index,:,:]) / 2\n",
    "        \n",
    "    s_anom_out = np.ma.array(s_anom, mask=orig_mask)\n",
    "    \n",
    "    return Bunch(s_anom = s_anom_out, \n",
    "                 last_valid_index = last_valid_index)\n",
    "\n",
    "def seasonal_anomaly2(m_anom, nmin=2):\n",
    "    newshape = [3] + list(m_anom.shape)\n",
    "    newshape[1] += 2\n",
    "    accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "    accum[:] = np.ma.masked\n",
    "    accum[0, :-2] = m_anom[:]\n",
    "    accum[1, 1:-1] = m_anom[:]\n",
    "    accum[2, 2:] = m_anom[:]\n",
    "    out = accum.mean(axis=0)[1:-1]\n",
    "    out = np.ma.masked_where(accum[:, 1:-1].count(axis=0) < nmin, out, copy=False)\n",
    "    return Bunch(s_anom = out)\n",
    "\n",
    "def running_sum(m_anom, window, nmin = 2):\n",
    "    nmin = (window+1)/2\n",
    "    chop = (window -1)/2\n",
    "\n",
    "    newshape = [window] + list(m_anom.shape)\n",
    "    newshape[1] += window -1\n",
    "    accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "    accum[:] = np.ma.masked\n",
    "    for i in range(window):\n",
    "        end = -window+i+1\n",
    "        if end == 0:\n",
    "            accum[i, i:] = m_anom[:]\n",
    "        else:\n",
    "            accum[i, i:end] = m_anom[:]\n",
    "    out = accum.mean(axis=0)[chop:-chop]\n",
    "    out = np.ma.masked_where(accum[:, chop:-chop].count(axis=0) < nmin, out, copy=False)\n",
    "    return Bunch(s_anom = out)\n",
    "\n",
    "def running_sum_2(m_anom, window, nmin = 2):\n",
    "    \n",
    "    if window % 2 == 0:\n",
    "        nmin = (window)/2\n",
    "        #nmin = 1\n",
    "        chop = (window-1)\n",
    "\n",
    "        newshape = [window] + list(m_anom.shape)\n",
    "        newshape[1] += window -1\n",
    "        accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "        accum[:] = np.ma.masked\n",
    "        for i in range(window):\n",
    "            end = -window+i+1\n",
    "            if end == 0:\n",
    "                accum[i, i:] = m_anom[:]\n",
    "            else:\n",
    "                accum[i, i:end] = m_anom[:]\n",
    "        if window != 2:\n",
    "            start=window/2\n",
    "            stop = (window/2) -1\n",
    "            out = accum.mean(axis=0)[start:-stop]\n",
    "            out = np.ma.masked_where(accum[:,start:-stop].count(axis=0) < nmin, out, copy=False)\n",
    "        else:\n",
    "            out = accum.mean(axis=0)[1:]\n",
    "            out = np.ma.masked_where(accum[:,1:].count(axis=0) < nmin, out, copy=False)\n",
    "        \n",
    "    else:\n",
    "        nmin = (window+1)/2\n",
    "        chop = (window -1)/2\n",
    "\n",
    "        newshape = [window] + list(m_anom.shape)\n",
    "        newshape[1] += window -1\n",
    "        accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "        accum[:] = np.ma.masked\n",
    "        for i in range(window):\n",
    "            end = -window+i+1\n",
    "            if end == 0:\n",
    "                accum[i, i:] = m_anom[:]\n",
    "            else:\n",
    "                accum[i, i:end] = m_anom[:]\n",
    "        out = accum.mean(axis=0)[chop:-chop]\n",
    "        out = np.ma.masked_where(accum[:, chop:-chop].count(axis=0) < nmin, out, copy=False)\n",
    "        \n",
    "    return Bunch(s_anom = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fill out the data for the missing months at the end of the last year of the dataset\n",
    "precip = extend(precip_dday_1800,precip_in)\n",
    "\n",
    "uwnd = extend(ncep_dday_1800,uin)\n",
    "vwnd = extend(ncep_dday_1800,vin)\n",
    "hgt = extend(ncep_dday_1800,hgtin)\n",
    "\n",
    "sst = extend(ersst_dday_1800,ersst_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpldays_precip = dday_to_mpl(1800, precip_dday_1800)\n",
    "mpldays_ncep = dday_to_mpl(1800, ncep_dday_1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erin/anaconda3/lib/python3.5/site-packages/numpy/ma/core.py:3117: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  dout = self.data[indx]\n",
      "/home/erin/anaconda3/lib/python3.5/site-packages/numpy/ma/core.py:3172: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  dout._mask = _mask[indx]\n"
     ]
    }
   ],
   "source": [
    "#Here we calculate the anomaly\n",
    "precip_ca = cal_climatology_and_anomaly(precip.dat,precip.ymdhms,\n",
    "                                        precip.last_valid_date, precip.last_valid_index,\n",
    "                                        precip_latitudes,\n",
    "                                        precip_longitudes,1979,2016)\n",
    "\n",
    "uwnd_ca = cal_climatology_and_anomaly(uwnd.dat,uwnd.ymdhms,\n",
    "                                      uwnd.last_valid_date,uwnd.last_valid_index,\n",
    "                                      ncep_latitudes,\n",
    "                                      ncep_longitudes,1965,2016)\n",
    "\n",
    "vwnd_ca = cal_climatology_and_anomaly(vwnd.dat,vwnd.ymdhms,\n",
    "                                      vwnd.last_valid_date,vwnd.last_valid_index,\n",
    "                                      ncep_latitudes,\n",
    "                                      ncep_longitudes,1965,2016)\n",
    "\n",
    "hgt_ca = cal_climatology_and_anomaly(hgt.dat,hgt.ymdhms,\n",
    "                                     hgt.last_valid_date,hgt.last_valid_index,\n",
    "                                     ncep_latitudes,\n",
    "                                     ncep_longitudes,1965,2016)\n",
    "\n",
    "sst_ca = cal_climatology_and_anomaly(sst.dat,sst.ymdhms,\n",
    "                                     sst.last_valid_date,sst.last_valid_index,\n",
    "                                     ersst_latitudes,\n",
    "                                     ersst_longitudes,1965,2016)\n",
    "\n",
    "precip_seasonal_anom = running_sum_2(precip_ca.anomaly,6, nmin=4)\n",
    "\n",
    "uwnd_seasonal_anom = running_sum_2(uwnd_ca.anomaly,3, nmin=2)\n",
    "vwnd_seasonal_anom = running_sum_2(vwnd_ca.anomaly,3, nmin=2)\n",
    "hgt_seasonal_anom = running_sum_2(hgt_ca.anomaly,3, nmin=2)\n",
    "\n",
    "sst_seasonal_anom = running_sum_2(sst_ca.anomaly,3, nmin=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Guam', 'Saipan', 'Koror', 'Yap', 'Chuuk', 'Pohnpei', 'Majuro', 'Kwajalein', 'PagoPago', 'Saipan_old', 'JFM', 'FMA', 'MAM', 'AMJ', 'MJJ', 'JJA', 'JAS', 'ASO', 'SON', 'OND', 'NDJ', 'DJF']\n",
      "['ONI']\n"
     ]
    }
   ],
   "source": [
    "peac_station_rain = pd.ExcelFile(exceldatadir +'PEAC_station_rainfall_database.xlsx')\n",
    "print(peac_station_rain.sheet_names)\n",
    "\n",
    "ONI_file = pd.ExcelFile(exceldatadir +'CPC_ONI.xlsx')\n",
    "print(ONI_file.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_USAPI_data(file_name, island_name):\n",
    "    \n",
    "    #Here we read the data into python from the excel file\n",
    "    raw_data= pd.read_excel(file_name, sheetname = island_name, \n",
    "                            skiprows = 1, parse_cols = \"A:O\")\n",
    "    \n",
    "    #\n",
    "    raw_matrix = raw_data.as_matrix(columns=None)\n",
    "    print(type(raw_matrix), raw_matrix.dtype)\n",
    "    \n",
    "    years =  raw_matrix[:51,1]\n",
    "    station_id = raw_matrix[0,0]\n",
    "\n",
    "    rainfall = raw_matrix[:51,3:]\n",
    "\n",
    "    rainfall[rainfall == '9999'] = np.nan\n",
    "    rainfall[rainfall == \"nan\"] = np.nan\n",
    "\n",
    "    rainfall = rainfall * 0.1\n",
    "    \n",
    "    # We have to convert from an object array to floating point.\n",
    "    rainfall = np.ma.masked_invalid(rainfall.astype(float))\n",
    "    print('rainfall: ', rainfall.dtype, rainfall[5, 5])\n",
    " \n",
    "    #Initiate the ymdhms array as an array of int values filler with zeros\n",
    "    #that has the length og the total number of time steps years*months\n",
    "    ymdhms = np.zeros([51*12,6],dtype = np.int)\n",
    "\n",
    "    #here we will the first column with the year values from the excel sheet. \n",
    "    #repeated each one 12 consecutive times\n",
    "    ymdhms[:,0] = np.repeat(years,12)\n",
    "\n",
    "    #Here we create a list of the month indices\n",
    "    m = np.arange(1,13)\n",
    "    #we tile the list so that the entire list 1..12 repeats as many times as the number of years\n",
    "    mm = np.tile(m,51)\n",
    "\n",
    "    ymdhms[:,1] = mm\n",
    "\n",
    "    #the day column is filled with 15\n",
    "    ymdhms[:,2] = 15\n",
    "    \n",
    "    dday = to_day(1800, ymdhms)\n",
    "    mpldays = dday_to_mpl(1800, dday)\n",
    "    mpldaysformated = mpl.dates.num2date(mpldays)\n",
    "\n",
    "    out = Bunch(island_name = island_name, station_id = station_id, rainfall=rainfall,\n",
    "               ymdhms = ymdhms, mpldaysformated = mpldaysformated)\n",
    "    \n",
    "    return out\n",
    "    #return station_id, rainfall, ymdhms, mpldaysformated\n",
    "    \n",
    "def read_index_data(file_name, index_name):\n",
    "    \n",
    "    #Here we read the data into python from the excel file\n",
    "    raw_data= pd.read_excel(file_name, sheetname = index_name, skiprows = 1, parse_cols = \"A:M\")\n",
    "    \n",
    "    #\n",
    "    raw_matrix = raw_data.as_matrix(columns=None)\n",
    "    print(type(raw_matrix), raw_matrix.dtype)\n",
    "    \n",
    "    years =  raw_matrix[:,0]\n",
    "    index = raw_matrix[:,1:]\n",
    "    \n",
    "\n",
    "    # We have to convert from an object array to floating point.\n",
    "    index = np.ma.masked_invalid(index.astype(float))\n",
    "    print('index: ', index.dtype, index[5, 5])\n",
    " \n",
    "    #Initiate the ymdhms array as an array of int values filler with zeros\n",
    "    #that has the length og the total number of time steps years*months\n",
    "    ymdhms = np.zeros([67*12,6],dtype = np.int)\n",
    "\n",
    "    #here we will the first column with the year values from the excel sheet. \n",
    "    #repeated each one 12 consecutive times\n",
    "    ymdhms[:,0] = np.repeat(years,12)\n",
    "\n",
    "    #Here we create a list of the month indices\n",
    "    m = np.arange(1,13)\n",
    "    #we tile the list so that the entire list 1..12 repeats as many times as the number of years\n",
    "    mm = np.tile(m,67)\n",
    "\n",
    "    ymdhms[:,1] = mm\n",
    "\n",
    "    #the day column is filled with 15\n",
    "    ymdhms[:,2] = 15\n",
    "    \n",
    "    dday = to_day(1800, ymdhms)\n",
    "    mpldays = dday_to_mpl(1800, dday)\n",
    "    mpldaysformated = mpl.dates.num2date(mpldays)\n",
    "\n",
    "    out = Bunch(index_name = index_name, index = index,\n",
    "               ymdhms = ymdhms, mpldaysformated = mpldaysformated)\n",
    "    \n",
    "    return out\n",
    "    #return station_id, rainfall, ymdhms, mpldaysformated\n",
    "\n",
    "def seasonal_anomaly_old(m_anom):\n",
    "    s_anom = np.ma.zeros(m_anom.shape, float)\n",
    "    s_anom[1:-1] = (m_anom[:-2] + m_anom[1:-1] + m_anom[2:]) / 3\n",
    "    s_anom[0] = (m_anom[0] + m_anom[1]) / 2\n",
    "    s_anom[-1] = (m_anom[-2] + m_anom[-1]) / 2\n",
    "    return s_anom\n",
    "\n",
    "def seasonal_sum(m_anom):\n",
    "    s_anom = np.ma.zeros(m_anom.shape, float)\n",
    "    s_anom[1:-1] = (m_anom[:-2] + m_anom[1:-1] + m_anom[2:])\n",
    "    s_anom[0] = (m_anom[0] + m_anom[1])\n",
    "    s_anom[-1] = (m_anom[-2] + m_anom[-1])\n",
    "    return s_anom\n",
    "\n",
    "def running_sum(m_anom, window, nmin = 2):\n",
    "    \n",
    "    if window % 2 == 0:\n",
    "        nmin = (window)/2\n",
    "        #nmin = 1\n",
    "        chop = (window-1)\n",
    "\n",
    "        newshape = [window] + list(m_anom.shape)\n",
    "        newshape[1] += window -1\n",
    "        accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "        accum[:] = np.ma.masked\n",
    "        for i in range(window):\n",
    "            end = -window+i+1\n",
    "            if end == 0:\n",
    "                accum[i, i:] = m_anom[:]\n",
    "            else:\n",
    "                accum[i, i:end] = m_anom[:]\n",
    "        if window != 2:\n",
    "            start=window/2\n",
    "            stop = (window/2) -1\n",
    "            out = accum.mean(axis=0)[start:-stop]\n",
    "            out = np.ma.masked_where(accum[:,start:-stop].count(axis=0) < nmin, out, copy=False)\n",
    "        else:\n",
    "            out = accum.mean(axis=0)[1:]\n",
    "            out = np.ma.masked_where(accum[:,1:].count(axis=0) < nmin, out, copy=False)\n",
    "        \n",
    "    else:\n",
    "        nmin = (window+1)/2\n",
    "        chop = (window -1)/2\n",
    "\n",
    "        newshape = [window] + list(m_anom.shape)\n",
    "        newshape[1] += window -1\n",
    "        accum = np.ma.zeros(newshape, dtype=m_anom.dtype)\n",
    "        accum[:] = np.ma.masked\n",
    "        for i in range(window):\n",
    "            end = -window+i+1\n",
    "            if end == 0:\n",
    "                accum[i, i:] = m_anom[:]\n",
    "            else:\n",
    "                accum[i, i:end] = m_anom[:]\n",
    "        out = accum.mean(axis=0)[chop:-chop]\n",
    "        out = np.ma.masked_where(accum[:, chop:-chop].count(axis=0) < nmin, out, copy=False)\n",
    "        \n",
    "    return Bunch(s_anom = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 498.1\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 354.3\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 128.6\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 353.9\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 599.9\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 292.9\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 340.5\n",
      "<class 'numpy.ndarray'> object\n",
      "rainfall:  float64 --\n"
     ]
    }
   ],
   "source": [
    "station_name_list = [\"Koror\", \"Yap\", \"Guam\", \"Chuuk\", \"Pohnpei\", \"Kwajalein\", \"Majuro\", \"Saipan\"]\n",
    "#variable_name_list = [koror, yap, guam, chuuk, phonpei, kwajalein, majuro]\n",
    "\n",
    "stations = Bunch()\n",
    "for name in station_name_list:\n",
    "    stations[name] = read_USAPI_data(peac_station_rain, name)\n",
    "    \n",
    "for raindata in stations.values():\n",
    "    #print(raindata.island_name)\n",
    "    #print(np.shape(raindata.rainfall))\n",
    "    raindata.monmean = raindata.rainfall.mean(axis=0)\n",
    "    raindata.monstd = raindata.rainfall.std(axis=0)\n",
    "    raindata.monanom = raindata.rainfall - raindata.monmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erin/anaconda3/lib/python3.5/site-packages/numpy/ma/core.py:3117: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  dout = self.data[indx]\n",
      "/home/erin/anaconda3/lib/python3.5/site-packages/numpy/ma/core.py:3172: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  dout._mask = _mask[indx]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(stations.Kwajalein.monanom))\n",
    "kwajalein_seasonal_anom = seasonal_anomaly_old(stations.Kwajalein.monanom.ravel())\n",
    "kwajalein_seasonal_total = seasonal_sum(stations.Kwajalein.rainfall.ravel())\n",
    "kwajalein_dry_season_total = running_sum(stations.Kwajalein.rainfall.ravel(),6)\n",
    "kwajalein_dry_season_anom = running_sum(stations.Kwajalein.monanom.ravel(),6)\n",
    "\n",
    "guam_seasonal_anom = seasonal_anomaly_old(stations.Guam.monanom.ravel())\n",
    "guam_seasonal_total = seasonal_sum(stations.Guam.rainfall.ravel())\n",
    "guam_dry_season_total = running_sum(stations.Guam.rainfall.ravel(),6)\n",
    "guam_dry_season_anom = running_sum(stations.Guam.monanom.ravel(),6)\n",
    "\n",
    "\n",
    "yap_seasonal_anom = seasonal_anomaly_old(stations.Yap.monanom.ravel())\n",
    "yap_seasonal_total = seasonal_sum(stations.Yap.rainfall.ravel())\n",
    "yap_dry_season_total = running_sum(stations.Yap.rainfall.ravel(),6)\n",
    "yap_dry_season_anom = running_sum(stations.Yap.monanom.ravel(),6)\n",
    "\n",
    "majuro_seasonal_anom = seasonal_anomaly_old(stations.Majuro.monanom.ravel())\n",
    "majuro_seasonal_total = seasonal_sum(stations.Majuro.rainfall.ravel())\n",
    "majuro_dry_season_total = running_sum(stations.Majuro.rainfall.ravel(),6)\n",
    "majuro_dry_season_anom = running_sum(stations.Majuro.monanom.ravel(),6)\n",
    "\n",
    "chuuk_seasonal_anom = seasonal_anomaly_old(stations.Chuuk.monanom.ravel())\n",
    "chuuk_seasonal_total = seasonal_sum(stations.Chuuk.rainfall.ravel())\n",
    "chuuk_dry_season_total = running_sum(stations.Chuuk.rainfall.ravel(),6)\n",
    "chuuk_dry_season_anom = running_sum(stations.Chuuk.monanom.ravel(),6)\n",
    "\n",
    "koror_seasonal_anom = seasonal_anomaly_old(stations.Koror.monanom.ravel())\n",
    "koror_seasonal_total = seasonal_sum(stations.Koror.rainfall.ravel())\n",
    "koror_dry_season_total = running_sum(stations.Koror.rainfall.ravel(),6)\n",
    "koror_dry_season_anom = running_sum(stations.Koror.monanom.ravel(),6)\n",
    "\n",
    "pohnpei_seasonal_anom = seasonal_anomaly_old(stations.Pohnpei.monanom.ravel())\n",
    "pohnpei_seasonal_total = seasonal_sum(stations.Pohnpei.rainfall.ravel())\n",
    "pohnpei_dry_season_total = running_sum(stations.Pohnpei.rainfall.ravel(),6)\n",
    "pohnpei_dry_season_anom = running_sum(stations.Pohnpei.monanom.ravel(),6)\n",
    "\n",
    "saipan_seasonal_anom = seasonal_anomaly_old(stations.Saipan.monanom.ravel())\n",
    "saipan_seasonal_total = seasonal_sum(stations.Saipan.rainfall.ravel())\n",
    "saipan_dry_season_total = running_sum(stations.Saipan.rainfall.ravel(),6)\n",
    "saipan_dry_season_anom = running_sum(stations.Saipan.monanom.ravel(),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spi_calculation(data, climatology_series):\n",
    "    \n",
    "    good_data = data[~data.mask]\n",
    "    good_climatology = climatology_series[~climatology_series.mask]\n",
    "    \n",
    "    spi = np.empty_like(good_data)\n",
    "    spi[:] = np.NAN\n",
    "\n",
    "    #fit gamma distribution to climatology series\n",
    "    fit_alpha, fit_loc, fit_beta = stats.gamma.fit(good_climatology)\n",
    "\n",
    "    #find cumulative probabilities of data from fitted distribution\n",
    "    data_cdf = stats.gamma.cdf(good_data, fit_alpha, loc=fit_loc, scale=fit_beta)\n",
    "\n",
    "    # find the percent points from the random normal dist\n",
    "\n",
    "    spi[:] = stats.norm.ppf(data_cdf, loc=0, scale=1)\n",
    "    \n",
    "    return spi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kwajalein_dry_season_total_matrix = kwajalein_dry_season_total.s_anom.reshape(51, 12)\n",
    "kwajalein_dry_season_anom_matrix = kwajalein_dry_season_anom.s_anom.reshape(51, 12)\n",
    "majuro_dry_season_anom_matrix = majuro_dry_season_anom.s_anom.reshape(51, 12)\n",
    "guam_dry_season_anom_matrix = guam_dry_season_anom.s_anom.reshape(51, 12)\n",
    "yap_dry_season_anom_matrix = yap_dry_season_anom.s_anom.reshape(51, 12)\n",
    "koror_dry_season_anom_matrix = koror_dry_season_anom.s_anom.reshape(51, 12)\n",
    "chuuk_dry_season_anom_matrix = chuuk_dry_season_anom.s_anom.reshape(51, 12)\n",
    "pohnpei_dry_season_anom_matrix = pohnpei_dry_season_anom.s_anom.reshape(51, 12)\n",
    "saipan_dry_season_anom_matrix = saipan_dry_season_anom.s_anom.reshape(51, 12)\n",
    "\n",
    "#Put ONI data into a pandas table\n",
    "season_list = ['DJF' , 'JFM' , 'FMA',\n",
    "               'MAM' , 'AMJ' , 'MJJ',\n",
    "               'JJA' , 'JAS' , 'ASO',\n",
    "               'SON' , 'OND' , 'NDJ']\n",
    "\n",
    "seven_month_list = ['Oct-Jan-Apr' , 'Nov-Feb-May' , 'Dec-Mar-Jun',\n",
    "                    'Jan-Apr-Jul' , 'Feb-May-Aug' , 'Mar-Jun-Sep',\n",
    "                    'Apr-Jul-Oct' , 'May-Aug-Nov' , 'Jun-Sep-Dec',\n",
    "                    'Jul-Oct-Jan' , 'Aug-Nov-Feb' , 'Sep-Dec-Mar']   \n",
    "\n",
    "six_month_list = ['Nov-Apr' , 'Dec-May' , 'Jan-Jun',\n",
    "                  'Feb-Jul' , 'Mar-Aug' , 'Apr-Sep',\n",
    "                  'May-Oct' , 'Jun-Nov' , 'Jul-Dec',\n",
    "                  'Aug-Jan' , 'Sep-Feb' , 'Oct-Mar']    \n",
    "\n",
    "kawj_dry_season_total_df = pd.DataFrame(kwajalein_dry_season_total_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "kawj_dry_season_anom_df = pd.DataFrame(kwajalein_dry_season_anom_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "#kawj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kwajalein_dry_season_total_matrix = kwajalein_dry_season_total.s_anom.reshape(51, 12)\n",
    "\n",
    "kwajalein_spi_matrix = np.empty_like(kwajalein_dry_season_total_matrix)\n",
    "kwajalein_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(kwajalein_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(kwajalein_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(kwajalein_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    kwajalein_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "\n",
    "    \n",
    "guam_dry_season_total_matrix = guam_dry_season_total.s_anom.reshape(51, 12)\n",
    "guam_spi_matrix = np.empty_like(guam_dry_season_total_matrix)\n",
    "guam_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(guam_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(guam_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(guam_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    guam_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "    \n",
    "yap_dry_season_total_matrix = yap_dry_season_total.s_anom.reshape(51, 12)\n",
    "yap_spi_matrix = np.empty_like(yap_dry_season_total_matrix)\n",
    "yap_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(yap_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(yap_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(yap_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    yap_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "majuro_dry_season_total_matrix = majuro_dry_season_total.s_anom.reshape(51, 12)\n",
    "majuro_spi_matrix = np.empty_like(majuro_dry_season_total_matrix)\n",
    "majuro_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(majuro_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(majuro_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(majuro_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    majuro_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "    \n",
    "koror_dry_season_total_matrix = koror_dry_season_total.s_anom.reshape(51, 12)\n",
    "koror_spi_matrix = np.empty_like(koror_dry_season_total_matrix)\n",
    "koror_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(koror_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(koror_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(koror_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    koror_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "    \n",
    "chuuk_dry_season_total_matrix = chuuk_dry_season_total.s_anom.reshape(51, 12)\n",
    "chuuk_spi_matrix = np.empty_like(chuuk_dry_season_total_matrix)\n",
    "chuuk_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(chuuk_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(chuuk_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(chuuk_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    chuuk_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "    \n",
    "pohnpei_dry_season_total_matrix = pohnpei_dry_season_total.s_anom.reshape(51, 12)\n",
    "pohnpei_spi_matrix = np.empty_like(pohnpei_dry_season_total_matrix)\n",
    "pohnpei_spi_matrix[:] = np.NAN\n",
    "\n",
    "for col in range(pohnpei_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(pohnpei_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(pohnpei_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    pohnpei_spi_matrix[:len(spi),col] = spi\n",
    "    \n",
    "saipan_dry_season_total_matrix = saipan_dry_season_total.s_anom.reshape(51, 12)\n",
    "saipan_spi_matrix = np.empty_like(saipan_dry_season_total_matrix)\n",
    "saipan_spi_matrix[:] = np.NAN\n",
    "saipan_spi_matrix_temp = np.empty_like(saipan_dry_season_total_matrix)\n",
    "saipan_spi_matrix_temp[:] = np.NAN\n",
    "\n",
    "for col in range(saipan_dry_season_total_matrix.shape[1]):\n",
    "    #print(kwajalein_seasonal_total_matrix[:,col])\n",
    "    spi = spi_calculation(np.squeeze(saipan_dry_season_total_matrix[:,col]),\n",
    "                          np.squeeze(saipan_dry_season_total_matrix[:,col]))\n",
    "\n",
    "    saipan_spi_matrix_temp[-len(spi):,col] = spi\n",
    "\n",
    "saipan_spi_matrix_temp[saipan_spi_matrix_temp == np.inf] = 7\n",
    "saipan_mask = np.ma.getmask(saipan_dry_season_total_matrix)\n",
    "saipan_spi_matrix = np.ma.masked_invalid(np.ma.array(saipan_spi_matrix_temp,mask = saipan_mask))\n",
    "    \n",
    "#Put ONI data into a pandas table\n",
    "seven_month_list = ['Oct-Jan-Apr' , 'Nov-Feb-May' , 'Dec-Mar-Jun',\n",
    "                    'Jan-Apr-Jul' , 'Feb-May-Aug' , 'Mar-Jun-Sep',\n",
    "                    'Apr-Jul-Oct' , 'May-Aug-Nov' , 'Jun-Sep-Dec',\n",
    "                    'Jul-Oct-Jan' , 'Aug-Nov-Feb' , 'Sep-Dec-Mar']    \n",
    "    \n",
    "six_month_list = ['Nov-Apr' , 'Dec-May' , 'Jan-Jun',\n",
    "                  'Feb-Jul' , 'Mar-Aug' , 'Apr-Sep',\n",
    "                  'May-Oct' , 'Jun-Nov' , 'Jul-Dec',\n",
    "                  'Aug-Jan' , 'Sep-Feb' , 'Oct-Mar']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> float64\n",
      "index:  float64 -0.6\n"
     ]
    }
   ],
   "source": [
    "average_all_station_spi_matrix = np.mean(np.array([kwajalein_spi_matrix, \n",
    "                                                   majuro_spi_matrix, \n",
    "                                                   guam_spi_matrix, \n",
    "                                                   yap_spi_matrix,\n",
    "                                                   chuuk_spi_matrix,\n",
    "                                                   koror_spi_matrix,\n",
    "                                                   pohnpei_spi_matrix]), axis=0)\n",
    "\n",
    "\n",
    "average_all_station_dry_season_anom_matrix = np.mean(np.array([kwajalein_dry_season_anom_matrix, \n",
    "                                                               majuro_dry_season_anom_matrix, \n",
    "                                                               guam_dry_season_anom_matrix, \n",
    "                                                               yap_dry_season_anom_matrix,\n",
    "                                                               chuuk_dry_season_anom_matrix, \n",
    "                                                               koror_dry_season_anom_matrix, \n",
    "                                                               pohnpei_dry_season_anom_matrix,]), axis=0)\n",
    "\n",
    "southern_station_spi_matrix = np.mean(np.array([majuro_spi_matrix, \n",
    "                                                   chuuk_spi_matrix,\n",
    "                                                   koror_spi_matrix,\n",
    "                                                   pohnpei_spi_matrix]), axis=0)\n",
    "\n",
    "\n",
    "southern_station_dry_season_anom_matrix = np.mean(np.array([majuro_dry_season_anom_matrix, \n",
    "                                                               chuuk_dry_season_anom_matrix, \n",
    "                                                               koror_dry_season_anom_matrix, \n",
    "                                                               pohnpei_dry_season_anom_matrix,]), axis=0)\n",
    "\n",
    "average_2_station_spi_matrix = np.mean(np.array([kwajalein_spi_matrix, \n",
    "                                                   guam_spi_matrix]), axis=0)\n",
    "\n",
    "\n",
    "average_2_station_dry_season_anom_matrix = np.mean(np.array([kwajalein_dry_season_anom_matrix, \n",
    "                                                               guam_dry_season_anom_matrix]), axis=0)\n",
    "\n",
    "average_all_station_SPI_df = pd.DataFrame(average_all_station_spi_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "average_all_station_dry_season_anom_df = pd.DataFrame(average_all_station_dry_season_anom_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "\n",
    "average_2_station_SPI_df = pd.DataFrame(average_2_station_spi_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "average_2_station_dry_season_anom_df = pd.DataFrame(average_2_station_dry_season_anom_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "\n",
    "southern_station_SPI_df = pd.DataFrame(southern_station_spi_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "southern_station_dry_season_anom_df = pd.DataFrame(southern_station_dry_season_anom_matrix, columns = six_month_list, index = range(1966,2017))\n",
    "\n",
    "\n",
    "\n",
    "oni = read_index_data(ONI_file, \"ONI\")\n",
    "oni_selection = ((oni.ymdhms[:, 0] >= 1965))\n",
    "oni_time_series = oni.index.ravel()\n",
    "oni_period = oni_time_series[oni_selection]\n",
    "oni_period_matrix = oni_period.reshape(52,12)\n",
    "ONI_df = pd.DataFrame(oni_period_matrix, columns = season_list, index = range(1965,2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgt_2_lat_sl = rangeslice(ncep_latitudes, (10, 20.001))\n",
    "hgt_2_lon_sl = rangeslice(ncep_longitudes, (140, 180.001))\n",
    "\n",
    "hgt_2_avg = np.nanmean(hgt_seasonal_anom.s_anom[:,:,hgt_2_lon_sl], axis=2)\n",
    "hgt_2_avg = np.nanmean(hgt_2_avg[:,hgt_2_lat_sl], axis=1)\n",
    "\n",
    "hgt_2_avg_matrix = hgt_2_avg.reshape(52, 12)\n",
    "\n",
    "hgt_2_avg_matrix_df = pd.DataFrame(hgt_2_avg_matrix, columns = season_list, index = range(1965,2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uwnd_2_avg = np.nanmean(uwnd_seasonal_anom.s_anom[:,:,hgt_2_lon_sl], axis=2)\n",
    "uwnd_2_avg = np.nanmean(uwnd_2_avg[:,hgt_2_lat_sl], axis=1)\n",
    "\n",
    "uwnd_2_avg_matrix = uwnd_2_avg.reshape(52, 12)\n",
    "\n",
    "uwnd_2_avg_matrix_df = pd.DataFrame(uwnd_2_avg_matrix, columns = season_list, index = range(1965,2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uwnd_3_avg = np.nanmean(uwnd_seasonal_anom.s_anom[:,:,hgt_2_lon_sl], axis=2)\n",
    "uwnd_3_avg = np.nanmean(uwnd_3_avg[:,hgt_2_lat_sl], axis=1)\n",
    "\n",
    "uwnd_3_avg_matrix = uwnd_3_avg.reshape(52, 12)\n",
    "\n",
    "new_wind_index = uwnd_2_avg_matrix - uwnd_3_avg_matrix\n",
    "\n",
    "new_wind_index_df = pd.DataFrame(new_wind_index, columns = season_list, index = range(1965,2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor_train_df = pd.concat([ONI_df.loc[1965:2004]['SON'],ONI_df.loc[1965:2004]['JJA'],\n",
    "                         uwnd_2_avg_matrix_df.loc[1965:2004]['SON'], uwnd_2_avg_matrix_df.loc[1965:2004]['JJA']],\n",
    "                         axis = 1, keys = ['SON ONI', 'JJA ONI', 'SON uwnd', 'JJA uwnd'])\n",
    "\n",
    "predictor_test_df = pd.concat([ONI_df.loc[2005:2015]['SON'],ONI_df.loc[2005:2015]['JJA'],\n",
    "                         uwnd_2_avg_matrix_df.loc[2005:2015]['SON'], uwnd_2_avg_matrix_df.loc[2005:2015]['JJA']],\n",
    "                         axis = 1, keys = ['SON ONI', 'JJA ONI', 'SON uwnd', 'JJA uwnd'])\n",
    "\n",
    "predictand_train_df = average_2_station_SPI_df.loc[1966:2005]['Dec-May']\n",
    "predictand_test_df = average_2_station_SPI_df.loc[2006:2016]['Dec-May']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Large data (small grid) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 33, 77)\n",
      "40\n",
      "(40, 5517)\n",
      "11\n",
      "(11, 5517)\n"
     ]
    }
   ],
   "source": [
    "sst_area_lats = rangeslice(ersst_latitudes, (-30, 30.001))\n",
    "sst_area_lons = rangeslice(ersst_longitudes, (110, 300.001))\n",
    "\n",
    "sst_test = sst_seasonal_anom.s_anom[:,sst_area_lats,sst_area_lons]\n",
    "\n",
    "sst_selection = (sst_ca.ymdhms[:, 1] == 10)\n",
    "sst_selection = np.nonzero(sst_selection)[0]\n",
    "sst_test_region_son = sst_test[sst_selection]\n",
    "sst_flat = np.reshape(sst_test_region_son,(52,2976))\n",
    "\n",
    "sst_predictor_train = sst_flat[:40,:]\n",
    "sst_predictor_test = sst_flat[40:-1,:]\n",
    "\n",
    "uwnd_area_lats = rangeslice(ncep_latitudes, (-30, 50.001))\n",
    "uwnd_area_lons = rangeslice(ncep_longitudes, (110, 300.001))\n",
    "\n",
    "uwnd_test = uwnd_seasonal_anom.s_anom[:,uwnd_area_lats, uwnd_area_lons]\n",
    "\n",
    "uwnd_selection = (uwnd_ca.ymdhms[:, 1] == 10)\n",
    "uwnd_selection = np.nonzero(uwnd_selection)[0]\n",
    "uwnd_test_region_son = uwnd_test[uwnd_selection]\n",
    "print(np.shape(uwnd_test_region_son))\n",
    "uwnd_flat = np.reshape(uwnd_test_region_son,(52,2541))\n",
    "\n",
    "uwnd_predictor_train = uwnd_flat[:40,:]\n",
    "uwnd_predictor_test = uwnd_flat[40:-1,:]\n",
    "\n",
    "predictor_train_matrix = np.hstack((sst_predictor_train, uwnd_predictor_train))\n",
    "predictor_test_matrix = np.hstack((sst_predictor_test, uwnd_predictor_test))\n",
    "\n",
    "print(len(predictand_train_df))\n",
    "print(np.shape(predictor_train_matrix))\n",
    "\n",
    "print(len(predictand_test_df))\n",
    "print(np.shape(predictor_test_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_forest = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_forest.fit(uwnd_predictor_train, predictand_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_prediction_result = big_forest.predict(uwnd_predictor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe5e63d2550>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = np.arange(2006,2017)\n",
    "plt.plot(dummy,predictand_test_df,dummy, big_prediction_result)\n",
    "plt.legend(['data', 'prediction'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
